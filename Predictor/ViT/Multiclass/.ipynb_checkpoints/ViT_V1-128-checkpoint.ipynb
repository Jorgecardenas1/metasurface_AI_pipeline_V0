{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b4d5ee-7caa-415a-9547-ceeb4a9fe138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --quiet  \"urllib3\" \"seaborn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c70d08-f24b-46ee-8b35-3904aff3c267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, r\"C:\\\\Users\\jorge\\\\Documents\\\\Projects Jorge C\\\\DRUIDA PROJECT\\\\POC\\\\druida_V01\\\\src\\\\\")\n",
    "\n",
    "import os\n",
    "\n",
    "from __future__ import print_function\n",
    "#from Utilities.SaveAnimation import Video\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from druida import Stack\n",
    "from druida import setup\n",
    "\n",
    "from druida.DataManager import datamanager\n",
    "from druidaHFSS.modules import tools\n",
    "from druida.tools import utils\n",
    "\n",
    "\n",
    "#import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/VisionTransformers/\")\n",
    "\n",
    "# Setting the seed\n",
    "#L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8162b48b-37e8-4cf5-8100-e6a5018cf2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boxImagesPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images Jorge Cardenas\\\\\"\n",
    "DataPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Exports\\\\output\\\\\"\n",
    "simulationData=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\DBfiles\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdd185a-8600-4f1a-9540-62660ea72349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"run_name\",type=str)\n",
    "parser.add_argument(\"epochs\",type=int)\n",
    "parser.add_argument(\"batch_size\",type=int)\n",
    "parser.add_argument(\"workers\",type=int)\n",
    "parser.add_argument(\"gpu_number\",type=int)\n",
    "parser.add_argument(\"device\",type=str)\n",
    "parser.add_argument(\"learning_rate\",type=float)\n",
    "parser.add_argument(\"condition_len\",type=float) #This defines the length of our conditioning vector\n",
    "parser.add_argument(\"metricType\",type=float) #This defines the length of our conditioning vector\n",
    "parser.add_argument(\"patch_size\",type=int)\n",
    "\n",
    "parser.run_name = \"Predictor Training\"\n",
    "parser.epochs = 10\n",
    "parser.batch_size = 10\n",
    "parser.workers=0\n",
    "parser.gpu_number=0\n",
    "parser.image_size = 128\n",
    "parser.dataset_path = os.path.normpath('/content/drive/MyDrive/Training_Data/Training_lite/')\n",
    "parser.device = \"cpu\"\n",
    "parser.learning_rate = 1e-4\n",
    "parser.condition_len = 10\n",
    "parser.metricType='AbsorbanceTM' #this is to be modified when training for different metrics.\n",
    "parser.patch_size=16\n",
    "\n",
    "metricType=['AbsorbanceTM','AbsorbanceTE' ]\n",
    "\n",
    "categories=[\"box\", \"circle\", \"cross\"]\n",
    "\n",
    "model_kwargs={\n",
    "        \"batch_size\":parser.batch_size,\n",
    "        \"embed_dim\":  3 * (parser.patch_size)**2 ,\n",
    "        \"hidden_dim\":  2*(3 * (parser.patch_size)**2),\n",
    "        \"num_heads\": 16,\n",
    "        \"num_layers\": 8,\n",
    "        \"patch_size\": parser.patch_size,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_patches\": (parser.image_size//parser.patch_size)**2,\n",
    "        \"num_classes\": 601,\n",
    "        \"dropout\": 0.1,\n",
    "        \"image_size\":parser.image_size,\n",
    "        \"conditionalIn\":True,\n",
    "        \"conditionalLen\":10\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1611c48f-27c9-47d9-885c-35894ec3cb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = utils.get_data_with_labels(parser.image_size, parser.image_size,0.9, boxImagesPath,parser.batch_size, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8db130-c764-4383-acba-bcef364cd3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa19882-3adf-4635-a6e0-909e284d8164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf093e5-37b4-44d2-8c2a-27c3afaf64e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagesPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6069e9d9-707f-4876-9626-64ce2370fa79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\09a2f2c2-ad7f-11ee-bb2a-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\0ed3a0e8-a653-11ee-9db6-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\3be7fd33-bd16-11ee-be43-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\55628454-b316-11ee-82a4-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\56dee422-b317-11ee-bb58-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\7262ded2-a81d-11ee-8e0e-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\89962313-bbc8-11ee-a7fe-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\a03ae6f6-bc71-11ee-aef0-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\b34769e8-bcbb-11ee-bce8-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\b6b02fee-a8bb-11ee-8e20-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\b7692f2a-b9ff-11ee-8bd2-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\bc37ca3a-c608-11ee-97a2-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\c49e201f-ae96-11ee-88f3-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\d31e1e9f-a766-11ee-8b8e-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\e3744294-aba9-11ee-b4d0-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\e9503ad7-c681-11ee-809f-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\processed\\\\']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folders=glob.glob(imagesPath+\"/*/\", recursive = True)\n",
    "files=[]\n",
    "\n",
    "print(folders)\n",
    "for folder in folders:\n",
    "    \n",
    "    if folder != imagesPath+\"\\\\\"+ \"processed128\\\\\":\n",
    "        files=(files+glob.glob(folder+\"/*\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f86636-ce80-4fd3-ab4f-796f29561cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in files:\n",
    "    fileName_absolute = os.path.basename(file) \n",
    "    path=os.path.dirname(file)\n",
    "\n",
    "    #ROI is \n",
    "    image_rgb=tools.cropImage( file,image_path=path,\n",
    "                              image_name=fileName_absolute,\n",
    "                              output_path=imagesPath, \n",
    "                             resize_dim=(128,128))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa8231-51c5-460b-bfa6-a4c4c2a63e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a588ab-ef7a-4b97-8f53-d17ed095ebf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vision_transformer = Stack.VisionTransformer(**model_kwargs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f833f03-1e31-43c5-b8b1-2f3de96eca70",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abbfc66-fe91-40d8-a06a-23db63ac5355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.optim as optimizer\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f24715-4b69-4f09-90c8-9f2b3b02ce78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_simulationData():\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in glob.glob(simulationData+\"*.csv\"): \n",
    "        df2 = pd.read_csv(file)\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    \n",
    "    df.to_csv('out.csv',index=False)\n",
    "    \n",
    "join_simulationData()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be6a2a6-b445-49a5-a3f0-edbe24f283fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "Substrates={\"Rogers RT/duroid 5880 (tm)\":0}\n",
    "Materials={\"copper\":0,\"pec\":1}\n",
    "Surfacetypes={\"Reflective\":0,\"Transmissive\":1}\n",
    "TargetGeometries={\"circ\":0,\"box\":1, \"cross\":2}\n",
    "           \n",
    "def set_conditioning(target,path,categories):\n",
    "    df = pd.read_csv(\"out.csv\")\n",
    "    arr=[]\n",
    "\n",
    "    for idx,name in enumerate(path):\n",
    "        series=name.split('_')[-1].split('.')[0]\n",
    "        batch=name.split('_')[4]\n",
    "        iteration=series.split('-')[-1]\n",
    "        row=df[(df['sim_id']==batch) & (df['iteration']==int(iteration))  ]\n",
    "\n",
    "        \n",
    "        target_val=target[idx]\n",
    "        category=categories[idx]\n",
    "        geometry=TargetGeometries[category]\n",
    "        \n",
    "        \"\"\"\"\n",
    "        surface type: reflective, transmissive\n",
    "        layers: conductor and conductor material / Substrate information\n",
    "        \"\"\"\n",
    "        surfacetype=row[\"type\"].values[0]\n",
    "        surfacetype=Surfacetypes[surfacetype]\n",
    "        \n",
    "        layers=row[\"layers\"].values[0]\n",
    "        layers= layers.replace(\"'\", '\"')\n",
    "        layer=json.loads(layers)\n",
    "        \n",
    "        materialconductor=Materials[layer['conductor']['material']]\n",
    "        materialsustrato=Substrates[layer['substrate']['material']]\n",
    "        \n",
    "        \n",
    "        if (target_val==2): #is cross. Because an added variable to the desing \n",
    "            \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-2]\n",
    "        else:\n",
    "        \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-1]\n",
    "        \n",
    "        arr.append([geometry,surfacetype,materialconductor,materialsustrato,sustratoHeight,1,1,1,1,1])\n",
    "    \n",
    "    return arr\n",
    "\n",
    "#\n",
    "#conditions=set_conditioning(targets, path, categories)\n",
    "#conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c79871d5-7017-4c47-abf2-8e78c3c03593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 601])\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "images, targets,path,categories= next(iter(dataloader))\n",
    "conditions=set_conditioning(targets, path, categories)\n",
    "conditioningArray=torch.FloatTensor(conditions)\n",
    "# compute vision transformer output\n",
    "vit_output = vision_transformer(images,condition=conditioningArray)\n",
    "\n",
    "assert vit_output.size(dim=1) == 601\n",
    "print(vit_output.shape)\n",
    "\n",
    "# get class probabilities\n",
    "probabilities = F.softmax(vit_output[0], dim=0)\n",
    "# probabilities should sum up to 1\n",
    "print(torch.sum(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6536c6f7-2ddb-4c2d-a6cb-a3185b8d709c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### #File reading conf\n",
    "a = []\n",
    "idx=0\n",
    "iters=0\n",
    "\n",
    "\n",
    "\"\"\"using weigth decay regularization\"\"\"\n",
    "opt = optimizer.Adam(vision_transformer.parameters(), lr=parser.learning_rate, betas=(0.5, 0.999),weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "vision_transformer.train()\n",
    "\n",
    "loss_values, valid_loss_list = [], []\n",
    "acc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bbd6ced-adf5-4d3a-92f3-688d8943f168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3326d1befbce41049a323f69e24ace75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     6] loss: 5.163 running loss:  32.470\n",
      "accuracy: 0.000 \n",
      "[1,    16] loss: 7.535 running loss:  99.910\n",
      "accuracy: 0.000 \n",
      "[1,    26] loss: 6.033 running loss:  159.855\n",
      "accuracy: 0.200 \n",
      "[1,    36] loss: 7.481 running loss:  229.121\n",
      "accuracy: 0.000 \n",
      "[1,    46] loss: 6.570 running loss:  292.418\n",
      "accuracy: 0.100 \n",
      "[1,    56] loss: 5.022 running loss:  357.831\n",
      "accuracy: 0.000 \n",
      "[1,    66] loss: 5.693 running loss:  417.293\n",
      "accuracy: 0.100 \n",
      "[1,    76] loss: 5.610 running loss:  482.823\n",
      "accuracy: 0.000 \n",
      "[1,    86] loss: 8.500 running loss:  549.358\n",
      "accuracy: 0.100 \n",
      "[1,    96] loss: 7.300 running loss:  614.134\n",
      "accuracy: 0.100 \n",
      "[1,   106] loss: 7.376 running loss:  680.728\n",
      "accuracy: 0.200 \n",
      "[1,   116] loss: 5.211 running loss:  735.464\n",
      "accuracy: 0.100 \n",
      "[1,   126] loss: 6.805 running loss:  800.317\n",
      "accuracy: 0.100 \n",
      "[1,   136] loss: 8.529 running loss:  863.361\n",
      "accuracy: 0.000 \n",
      "[1,   146] loss: 6.931 running loss:  925.712\n",
      "accuracy: 0.000 \n",
      "[1,   156] loss: 6.940 running loss:  992.750\n",
      "accuracy: 0.000 \n",
      "[1,   166] loss: 5.528 running loss:  1057.664\n",
      "accuracy: 0.100 \n",
      "[1,   176] loss: 6.324 running loss:  1119.356\n",
      "accuracy: 0.000 \n",
      "[1,   186] loss: 6.110 running loss:  1185.495\n",
      "accuracy: 0.000 \n",
      "[1,   196] loss: 5.424 running loss:  1252.047\n",
      "accuracy: 0.100 \n",
      "[1,   206] loss: 5.681 running loss:  1319.378\n",
      "accuracy: 0.000 \n",
      "[1,   216] loss: 6.420 running loss:  1388.652\n",
      "accuracy: 0.000 \n",
      "[1,   226] loss: 6.996 running loss:  1456.516\n",
      "accuracy: 0.000 \n",
      "[1,   236] loss: 5.176 running loss:  1520.579\n",
      "accuracy: 0.000 \n",
      "[1,   246] loss: 5.214 running loss:  1592.646\n",
      "accuracy: 0.100 \n",
      "[1,   256] loss: 7.196 running loss:  1651.479\n",
      "accuracy: 0.000 \n",
      "[1,   266] loss: 6.800 running loss:  1717.087\n",
      "accuracy: 0.000 \n",
      "[1,   276] loss: 5.991 running loss:  1780.052\n",
      "accuracy: 0.200 \n",
      "[1,   286] loss: 8.302 running loss:  1837.512\n",
      "accuracy: 0.100 \n",
      "[1,   296] loss: 8.205 running loss:  1900.580\n",
      "accuracy: 0.000 \n",
      "[1,   306] loss: 6.617 running loss:  1963.273\n",
      "accuracy: 0.000 \n",
      "[1,   316] loss: 6.494 running loss:  2025.221\n",
      "accuracy: 0.000 \n",
      "[1,   326] loss: 5.661 running loss:  2089.028\n",
      "accuracy: 0.100 \n",
      "[1,   336] loss: 6.234 running loss:  2148.581\n",
      "accuracy: 0.000 \n",
      "[1,   346] loss: 6.624 running loss:  2215.679\n",
      "accuracy: 0.200 \n",
      "[1,   356] loss: 7.624 running loss:  2280.922\n",
      "accuracy: 0.100 \n",
      "[1,   366] loss: 7.439 running loss:  2348.046\n",
      "accuracy: 0.200 \n",
      "[1,   376] loss: 6.265 running loss:  2412.067\n",
      "accuracy: 0.100 \n",
      "[1,   386] loss: 6.197 running loss:  2474.724\n",
      "accuracy: 0.100 \n",
      "[1,   396] loss: 6.445 running loss:  2539.039\n",
      "accuracy: 0.000 \n",
      "[1,   406] loss: 5.650 running loss:  2603.888\n",
      "accuracy: 0.100 \n",
      "[1,   416] loss: 7.667 running loss:  2665.481\n",
      "accuracy: 0.000 \n",
      "[1,   426] loss: 4.399 running loss:  2728.454\n",
      "accuracy: 0.000 \n",
      "[1,   436] loss: 5.003 running loss:  2791.287\n",
      "accuracy: 0.100 \n",
      "[1,   446] loss: 7.248 running loss:  2856.223\n",
      "accuracy: 0.000 \n",
      "[1,   456] loss: 8.252 running loss:  2920.113\n",
      "accuracy: 0.000 \n",
      "[1,   466] loss: 7.772 running loss:  2988.945\n",
      "accuracy: 0.100 \n",
      "[1,   476] loss: 6.708 running loss:  3051.997\n",
      "accuracy: 0.100 \n",
      "[1,   486] loss: 6.030 running loss:  3110.737\n",
      "accuracy: 0.000 \n",
      "[1,   496] loss: 6.483 running loss:  3177.623\n",
      "accuracy: 0.000 \n",
      "[1,   506] loss: 8.215 running loss:  3247.763\n",
      "accuracy: 0.000 \n",
      "[1,   516] loss: 6.337 running loss:  3311.489\n",
      "accuracy: 0.000 \n",
      "[1,   526] loss: 6.267 running loss:  3372.536\n",
      "accuracy: 0.000 \n",
      "[1,   536] loss: 6.015 running loss:  3434.865\n",
      "accuracy: 0.000 \n",
      "[1,   546] loss: 6.260 running loss:  3504.638\n",
      "accuracy: 0.000 \n",
      "[1,   556] loss: 7.458 running loss:  3567.697\n",
      "accuracy: 0.000 \n",
      "[1,   566] loss: 5.768 running loss:  3632.462\n",
      "accuracy: 0.000 \n",
      "[1,   576] loss: 6.262 running loss:  3692.769\n",
      "accuracy: 0.100 \n",
      "[1,   586] loss: 5.215 running loss:  3754.832\n",
      "accuracy: 0.000 \n",
      "[1,   596] loss: 6.548 running loss:  3817.477\n",
      "accuracy: 0.100 \n",
      "[1,   606] loss: 3.934 running loss:  3877.624\n",
      "accuracy: 0.000 \n",
      "[1,   616] loss: 7.497 running loss:  3945.833\n",
      "accuracy: 0.000 \n",
      "[1,   626] loss: 5.994 running loss:  4008.496\n",
      "accuracy: 0.000 \n",
      "[1,   636] loss: 6.509 running loss:  4066.311\n",
      "accuracy: 0.000 \n",
      "[1,   646] loss: 5.829 running loss:  4128.228\n",
      "accuracy: 0.200 \n",
      "[1,   656] loss: 6.219 running loss:  4190.412\n",
      "accuracy: 0.000 \n",
      "[1,   666] loss: 6.336 running loss:  4252.863\n",
      "accuracy: 0.100 \n",
      "[1,   676] loss: 5.067 running loss:  4314.832\n",
      "accuracy: 0.200 \n",
      "[1,   686] loss: 5.711 running loss:  4378.745\n",
      "accuracy: 0.200 \n",
      "[1,   696] loss: 8.527 running loss:  4441.691\n",
      "accuracy: 0.100 \n",
      "[1,   706] loss: 6.847 running loss:  4502.513\n",
      "accuracy: 0.000 \n",
      "[1,   716] loss: 5.131 running loss:  4563.203\n",
      "accuracy: 0.000 \n",
      "[1,   726] loss: 5.280 running loss:  4622.649\n",
      "accuracy: 0.000 \n",
      "[1,   736] loss: 6.954 running loss:  4683.137\n",
      "accuracy: 0.000 \n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445efe3629d44d898f0d100de79e8196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     6] loss: 6.419 running loss:  29.522\n",
      "accuracy: 0.100 \n",
      "[2,    16] loss: 6.225 running loss:  95.362\n",
      "accuracy: 0.000 \n",
      "[2,    26] loss: 4.244 running loss:  155.775\n",
      "accuracy: 0.100 \n",
      "[2,    36] loss: 4.547 running loss:  217.568\n",
      "accuracy: 0.100 \n",
      "[2,    46] loss: 5.965 running loss:  283.041\n",
      "accuracy: 0.100 \n",
      "[2,    56] loss: 7.546 running loss:  344.954\n",
      "accuracy: 0.100 \n",
      "[2,    66] loss: 7.130 running loss:  407.140\n",
      "accuracy: 0.000 \n",
      "[2,    76] loss: 6.509 running loss:  469.991\n",
      "accuracy: 0.000 \n",
      "[2,    86] loss: 7.305 running loss:  532.273\n",
      "accuracy: 0.300 \n",
      "[2,    96] loss: 4.905 running loss:  589.883\n",
      "accuracy: 0.000 \n",
      "[2,   106] loss: 4.957 running loss:  648.871\n",
      "accuracy: 0.000 \n",
      "[2,   116] loss: 6.975 running loss:  714.341\n",
      "accuracy: 0.000 \n",
      "[2,   126] loss: 7.966 running loss:  781.924\n",
      "accuracy: 0.000 \n",
      "[2,   136] loss: 6.271 running loss:  848.460\n",
      "accuracy: 0.000 \n",
      "[2,   146] loss: 5.879 running loss:  913.900\n",
      "accuracy: 0.000 \n",
      "[2,   156] loss: 5.986 running loss:  974.092\n",
      "accuracy: 0.000 \n",
      "[2,   166] loss: 5.683 running loss:  1030.390\n",
      "accuracy: 0.000 \n",
      "[2,   176] loss: 6.733 running loss:  1092.695\n",
      "accuracy: 0.000 \n",
      "[2,   186] loss: 6.256 running loss:  1155.987\n",
      "accuracy: 0.000 \n",
      "[2,   196] loss: 5.975 running loss:  1217.241\n",
      "accuracy: 0.000 \n",
      "[2,   206] loss: 7.706 running loss:  1282.773\n",
      "accuracy: 0.100 \n",
      "[2,   216] loss: 4.936 running loss:  1342.181\n",
      "accuracy: 0.000 \n",
      "[2,   226] loss: 5.902 running loss:  1407.317\n",
      "accuracy: 0.000 \n",
      "[2,   236] loss: 4.534 running loss:  1461.512\n",
      "accuracy: 0.100 \n",
      "[2,   246] loss: 5.377 running loss:  1526.026\n",
      "accuracy: 0.000 \n",
      "[2,   256] loss: 6.042 running loss:  1585.755\n",
      "accuracy: 0.300 \n",
      "[2,   266] loss: 5.955 running loss:  1654.732\n",
      "accuracy: 0.200 \n",
      "[2,   276] loss: 8.088 running loss:  1718.711\n",
      "accuracy: 0.000 \n",
      "[2,   286] loss: 5.587 running loss:  1774.829\n",
      "accuracy: 0.100 \n",
      "[2,   296] loss: 6.051 running loss:  1834.804\n",
      "accuracy: 0.200 \n",
      "[2,   306] loss: 7.497 running loss:  1901.304\n",
      "accuracy: 0.000 \n",
      "[2,   316] loss: 5.900 running loss:  1967.566\n",
      "accuracy: 0.000 \n",
      "[2,   326] loss: 6.585 running loss:  2034.939\n",
      "accuracy: 0.000 \n",
      "[2,   336] loss: 5.735 running loss:  2097.044\n",
      "accuracy: 0.300 \n",
      "[2,   346] loss: 4.979 running loss:  2168.251\n",
      "accuracy: 0.000 \n",
      "[2,   356] loss: 6.810 running loss:  2232.243\n",
      "accuracy: 0.000 \n",
      "[2,   366] loss: 6.671 running loss:  2297.091\n",
      "accuracy: 0.100 \n",
      "[2,   376] loss: 7.916 running loss:  2364.280\n",
      "accuracy: 0.000 \n",
      "[2,   386] loss: 7.159 running loss:  2435.804\n",
      "accuracy: 0.000 \n",
      "[2,   396] loss: 5.362 running loss:  2505.519\n",
      "accuracy: 0.000 \n",
      "[2,   406] loss: 4.993 running loss:  2566.828\n",
      "accuracy: 0.000 \n",
      "[2,   416] loss: 6.286 running loss:  2629.142\n",
      "accuracy: 0.000 \n",
      "[2,   426] loss: 7.296 running loss:  2696.027\n",
      "accuracy: 0.200 \n",
      "[2,   436] loss: 5.636 running loss:  2758.880\n",
      "accuracy: 0.000 \n",
      "[2,   446] loss: 4.747 running loss:  2818.800\n",
      "accuracy: 0.100 \n",
      "[2,   456] loss: 5.776 running loss:  2880.505\n",
      "accuracy: 0.100 \n",
      "[2,   466] loss: 6.879 running loss:  2945.034\n",
      "accuracy: 0.000 \n",
      "[2,   476] loss: 9.973 running loss:  3016.161\n",
      "accuracy: 0.000 \n",
      "[2,   486] loss: 6.174 running loss:  3082.813\n",
      "accuracy: 0.100 \n",
      "[2,   496] loss: 6.517 running loss:  3147.614\n",
      "accuracy: 0.100 \n",
      "[2,   506] loss: 7.525 running loss:  3216.986\n",
      "accuracy: 0.000 \n",
      "[2,   516] loss: 7.075 running loss:  3283.344\n",
      "accuracy: 0.000 \n",
      "[2,   526] loss: 6.383 running loss:  3343.443\n",
      "accuracy: 0.000 \n",
      "[2,   536] loss: 6.616 running loss:  3409.123\n",
      "accuracy: 0.000 \n",
      "[2,   546] loss: 7.149 running loss:  3472.372\n",
      "accuracy: 0.200 \n",
      "[2,   556] loss: 6.705 running loss:  3529.053\n",
      "accuracy: 0.000 \n",
      "[2,   566] loss: 7.216 running loss:  3593.791\n",
      "accuracy: 0.000 \n",
      "[2,   576] loss: 8.446 running loss:  3661.745\n",
      "accuracy: 0.100 \n",
      "[2,   586] loss: 5.699 running loss:  3723.669\n",
      "accuracy: 0.000 \n",
      "[2,   596] loss: 5.518 running loss:  3781.843\n",
      "accuracy: 0.000 \n",
      "[2,   606] loss: 5.467 running loss:  3842.907\n",
      "accuracy: 0.000 \n",
      "[2,   616] loss: 8.623 running loss:  3909.216\n",
      "accuracy: 0.000 \n",
      "[2,   626] loss: 8.294 running loss:  3975.963\n",
      "accuracy: 0.000 \n",
      "[2,   636] loss: 6.296 running loss:  4040.543\n",
      "accuracy: 0.100 \n",
      "[2,   646] loss: 6.005 running loss:  4099.161\n",
      "accuracy: 0.200 \n",
      "[2,   656] loss: 6.155 running loss:  4166.806\n",
      "accuracy: 0.200 \n",
      "[2,   666] loss: 5.541 running loss:  4224.994\n",
      "accuracy: 0.300 \n",
      "[2,   676] loss: 6.692 running loss:  4295.284\n",
      "accuracy: 0.000 \n",
      "[2,   686] loss: 8.031 running loss:  4361.054\n",
      "accuracy: 0.100 \n",
      "[2,   696] loss: 4.317 running loss:  4421.513\n",
      "accuracy: 0.100 \n",
      "[2,   706] loss: 4.684 running loss:  4483.367\n",
      "accuracy: 0.100 \n",
      "[2,   716] loss: 7.331 running loss:  4546.239\n",
      "accuracy: 0.000 \n",
      "[2,   726] loss: 4.895 running loss:  4607.510\n",
      "accuracy: 0.000 \n",
      "[2,   736] loss: 5.742 running loss:  4663.732\n",
      "accuracy: 0.000 \n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89af0b5031154621a7ab1c1a73ab3793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     6] loss: 5.409 running loss:  27.704\n",
      "accuracy: 0.100 \n",
      "[3,    16] loss: 6.218 running loss:  89.715\n",
      "accuracy: 0.000 \n",
      "[3,    26] loss: 7.054 running loss:  149.602\n",
      "accuracy: 0.000 \n",
      "[3,    36] loss: 6.139 running loss:  208.387\n",
      "accuracy: 0.100 \n",
      "[3,    46] loss: 5.855 running loss:  274.624\n",
      "accuracy: 0.000 \n",
      "[3,    56] loss: 6.203 running loss:  337.760\n",
      "accuracy: 0.100 \n",
      "[3,    66] loss: 6.477 running loss:  403.247\n",
      "accuracy: 0.200 \n",
      "[3,    76] loss: 6.041 running loss:  472.476\n",
      "accuracy: 0.200 \n",
      "[3,    86] loss: 6.615 running loss:  532.627\n",
      "accuracy: 0.000 \n",
      "[3,    96] loss: 6.911 running loss:  602.167\n",
      "accuracy: 0.000 \n",
      "[3,   106] loss: 5.301 running loss:  658.432\n",
      "accuracy: 0.000 \n",
      "[3,   116] loss: 6.117 running loss:  722.061\n",
      "accuracy: 0.100 \n",
      "[3,   126] loss: 6.278 running loss:  783.498\n",
      "accuracy: 0.100 \n",
      "[3,   136] loss: 6.031 running loss:  849.337\n",
      "accuracy: 0.100 \n",
      "[3,   146] loss: 6.684 running loss:  910.668\n",
      "accuracy: 0.000 \n",
      "[3,   156] loss: 4.722 running loss:  971.205\n",
      "accuracy: 0.000 \n",
      "[3,   166] loss: 5.678 running loss:  1039.262\n",
      "accuracy: 0.000 \n",
      "[3,   176] loss: 6.891 running loss:  1099.671\n",
      "accuracy: 0.000 \n",
      "[3,   186] loss: 6.799 running loss:  1161.104\n",
      "accuracy: 0.100 \n",
      "[3,   196] loss: 6.310 running loss:  1224.135\n",
      "accuracy: 0.000 \n",
      "[3,   206] loss: 5.041 running loss:  1287.350\n",
      "accuracy: 0.100 \n",
      "[3,   216] loss: 6.178 running loss:  1352.907\n",
      "accuracy: 0.100 \n",
      "[3,   226] loss: 6.549 running loss:  1418.003\n",
      "accuracy: 0.000 \n",
      "[3,   236] loss: 7.353 running loss:  1481.748\n",
      "accuracy: 0.100 \n",
      "[3,   246] loss: 5.969 running loss:  1542.298\n",
      "accuracy: 0.000 \n",
      "[3,   256] loss: 8.007 running loss:  1609.031\n",
      "accuracy: 0.200 \n",
      "[3,   266] loss: 5.578 running loss:  1671.399\n",
      "accuracy: 0.000 \n",
      "[3,   276] loss: 5.275 running loss:  1738.635\n",
      "accuracy: 0.000 \n",
      "[3,   286] loss: 5.304 running loss:  1800.447\n",
      "accuracy: 0.000 \n",
      "[3,   296] loss: 6.863 running loss:  1858.409\n",
      "accuracy: 0.000 \n",
      "[3,   306] loss: 7.377 running loss:  1927.680\n",
      "accuracy: 0.000 \n",
      "[3,   316] loss: 6.758 running loss:  1996.947\n",
      "accuracy: 0.000 \n",
      "[3,   326] loss: 7.105 running loss:  2064.574\n",
      "accuracy: 0.000 \n",
      "[3,   336] loss: 5.487 running loss:  2122.099\n",
      "accuracy: 0.000 \n",
      "[3,   346] loss: 8.282 running loss:  2184.317\n",
      "accuracy: 0.000 \n",
      "[3,   356] loss: 6.445 running loss:  2250.134\n",
      "accuracy: 0.000 \n",
      "[3,   366] loss: 5.241 running loss:  2308.350\n",
      "accuracy: 0.100 \n",
      "[3,   376] loss: 7.497 running loss:  2376.496\n",
      "accuracy: 0.000 \n",
      "[3,   386] loss: 6.994 running loss:  2440.646\n",
      "accuracy: 0.000 \n",
      "[3,   396] loss: 6.722 running loss:  2505.378\n",
      "accuracy: 0.300 \n",
      "[3,   406] loss: 5.199 running loss:  2571.897\n",
      "accuracy: 0.000 \n",
      "[3,   416] loss: 7.667 running loss:  2632.250\n",
      "accuracy: 0.000 \n",
      "[3,   426] loss: 6.767 running loss:  2695.786\n",
      "accuracy: 0.000 \n",
      "[3,   436] loss: 5.613 running loss:  2755.179\n",
      "accuracy: 0.000 \n",
      "[3,   446] loss: 6.382 running loss:  2816.065\n",
      "accuracy: 0.000 \n",
      "[3,   456] loss: 5.703 running loss:  2875.293\n",
      "accuracy: 0.100 \n",
      "[3,   466] loss: 5.998 running loss:  2933.197\n",
      "accuracy: 0.100 \n",
      "[3,   476] loss: 6.264 running loss:  3000.651\n",
      "accuracy: 0.000 \n",
      "[3,   486] loss: 4.914 running loss:  3061.566\n",
      "accuracy: 0.100 \n",
      "[3,   496] loss: 6.005 running loss:  3129.651\n",
      "accuracy: 0.000 \n",
      "[3,   506] loss: 5.909 running loss:  3195.478\n",
      "accuracy: 0.000 \n",
      "[3,   516] loss: 6.906 running loss:  3254.132\n",
      "accuracy: 0.000 \n",
      "[3,   526] loss: 7.403 running loss:  3319.082\n",
      "accuracy: 0.000 \n",
      "[3,   536] loss: 7.722 running loss:  3381.145\n",
      "accuracy: 0.100 \n",
      "[3,   546] loss: 9.972 running loss:  3447.435\n",
      "accuracy: 0.000 \n",
      "[3,   556] loss: 4.300 running loss:  3504.192\n",
      "accuracy: 0.000 \n",
      "[3,   566] loss: 6.125 running loss:  3566.593\n",
      "accuracy: 0.000 \n",
      "[3,   576] loss: 4.387 running loss:  3631.737\n",
      "accuracy: 0.000 \n",
      "[3,   586] loss: 6.673 running loss:  3697.111\n",
      "accuracy: 0.000 \n",
      "[3,   596] loss: 5.907 running loss:  3763.522\n",
      "accuracy: 0.100 \n",
      "[3,   606] loss: 5.129 running loss:  3830.237\n",
      "accuracy: 0.200 \n",
      "[3,   616] loss: 7.120 running loss:  3896.915\n",
      "accuracy: 0.000 \n",
      "[3,   626] loss: 6.882 running loss:  3959.619\n",
      "accuracy: 0.200 \n",
      "[3,   636] loss: 4.769 running loss:  4022.636\n",
      "accuracy: 0.000 \n",
      "[3,   646] loss: 6.650 running loss:  4087.355\n",
      "accuracy: 0.000 \n",
      "[3,   656] loss: 5.753 running loss:  4147.050\n",
      "accuracy: 0.100 \n",
      "[3,   666] loss: 6.130 running loss:  4213.641\n",
      "accuracy: 0.000 \n",
      "[3,   676] loss: 5.838 running loss:  4272.799\n",
      "accuracy: 0.100 \n",
      "[3,   686] loss: 8.252 running loss:  4340.776\n",
      "accuracy: 0.300 \n",
      "[3,   696] loss: 5.989 running loss:  4400.872\n",
      "accuracy: 0.000 \n",
      "[3,   706] loss: 6.232 running loss:  4466.918\n",
      "accuracy: 0.000 \n",
      "[3,   716] loss: 7.951 running loss:  4536.516\n",
      "accuracy: 0.000 \n",
      "[3,   726] loss: 6.006 running loss:  4599.602\n",
      "accuracy: 0.000 \n",
      "[3,   736] loss: 7.508 running loss:  4666.575\n",
      "accuracy: 0.000 \n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a53d55b434ba7ad84015d65ee08d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,     6] loss: 6.358 running loss:  30.639\n",
      "accuracy: 0.100 \n",
      "[4,    16] loss: 6.419 running loss:  95.279\n",
      "accuracy: 0.000 \n",
      "[4,    26] loss: 6.081 running loss:  155.374\n",
      "accuracy: 0.000 \n",
      "[4,    36] loss: 5.859 running loss:  218.859\n",
      "accuracy: 0.000 \n",
      "[4,    46] loss: 6.121 running loss:  281.006\n",
      "accuracy: 0.000 \n",
      "[4,    56] loss: 5.464 running loss:  343.038\n",
      "accuracy: 0.000 \n",
      "[4,    66] loss: 6.300 running loss:  407.684\n",
      "accuracy: 0.000 \n",
      "[4,    76] loss: 8.376 running loss:  471.927\n",
      "accuracy: 0.000 \n",
      "[4,    86] loss: 5.547 running loss:  536.286\n",
      "accuracy: 0.000 \n",
      "[4,    96] loss: 5.181 running loss:  597.090\n",
      "accuracy: 0.000 \n",
      "[4,   106] loss: 8.380 running loss:  666.439\n",
      "accuracy: 0.000 \n",
      "[4,   116] loss: 5.409 running loss:  729.073\n",
      "accuracy: 0.100 \n",
      "[4,   126] loss: 7.774 running loss:  790.069\n",
      "accuracy: 0.000 \n",
      "[4,   136] loss: 7.334 running loss:  850.521\n",
      "accuracy: 0.100 \n",
      "[4,   146] loss: 7.072 running loss:  910.066\n",
      "accuracy: 0.000 \n",
      "[4,   156] loss: 7.626 running loss:  971.374\n",
      "accuracy: 0.100 \n",
      "[4,   166] loss: 8.009 running loss:  1042.681\n",
      "accuracy: 0.200 \n",
      "[4,   176] loss: 4.788 running loss:  1104.324\n",
      "accuracy: 0.000 \n",
      "[4,   186] loss: 6.755 running loss:  1163.025\n",
      "accuracy: 0.000 \n",
      "[4,   196] loss: 7.940 running loss:  1228.301\n",
      "accuracy: 0.000 \n",
      "[4,   206] loss: 4.136 running loss:  1290.211\n",
      "accuracy: 0.000 \n",
      "[4,   216] loss: 8.501 running loss:  1359.399\n",
      "accuracy: 0.100 \n",
      "[4,   226] loss: 6.941 running loss:  1421.761\n",
      "accuracy: 0.100 \n",
      "[4,   236] loss: 7.951 running loss:  1485.082\n",
      "accuracy: 0.100 \n",
      "[4,   246] loss: 6.875 running loss:  1558.442\n",
      "accuracy: 0.200 \n",
      "[4,   256] loss: 6.360 running loss:  1615.348\n",
      "accuracy: 0.000 \n",
      "[4,   266] loss: 5.870 running loss:  1676.355\n",
      "accuracy: 0.100 \n",
      "[4,   276] loss: 6.262 running loss:  1736.946\n",
      "accuracy: 0.000 \n",
      "[4,   286] loss: 6.871 running loss:  1804.436\n",
      "accuracy: 0.000 \n",
      "[4,   296] loss: 6.837 running loss:  1863.724\n",
      "accuracy: 0.100 \n",
      "[4,   306] loss: 7.487 running loss:  1930.124\n",
      "accuracy: 0.400 \n",
      "[4,   316] loss: 4.077 running loss:  1994.742\n",
      "accuracy: 0.000 \n",
      "[4,   326] loss: 7.724 running loss:  2060.521\n",
      "accuracy: 0.300 \n",
      "[4,   336] loss: 5.741 running loss:  2125.203\n",
      "accuracy: 0.000 \n",
      "[4,   346] loss: 5.379 running loss:  2190.204\n",
      "accuracy: 0.000 \n",
      "[4,   356] loss: 10.487 running loss:  2258.544\n",
      "accuracy: 0.000 \n",
      "[4,   366] loss: 5.799 running loss:  2319.810\n",
      "accuracy: 0.000 \n",
      "[4,   376] loss: 6.020 running loss:  2385.436\n",
      "accuracy: 0.000 \n",
      "[4,   386] loss: 7.543 running loss:  2448.516\n",
      "accuracy: 0.100 \n",
      "[4,   396] loss: 4.937 running loss:  2508.790\n",
      "accuracy: 0.200 \n",
      "[4,   406] loss: 4.719 running loss:  2571.700\n",
      "accuracy: 0.200 \n",
      "[4,   416] loss: 5.134 running loss:  2633.040\n",
      "accuracy: 0.100 \n",
      "[4,   426] loss: 6.551 running loss:  2702.440\n",
      "accuracy: 0.000 \n",
      "[4,   436] loss: 6.216 running loss:  2768.852\n",
      "accuracy: 0.000 \n",
      "[4,   446] loss: 5.718 running loss:  2828.603\n",
      "accuracy: 0.100 \n",
      "[4,   456] loss: 7.969 running loss:  2898.485\n",
      "accuracy: 0.100 \n",
      "[4,   466] loss: 6.933 running loss:  2963.144\n",
      "accuracy: 0.100 \n",
      "[4,   476] loss: 7.230 running loss:  3026.272\n",
      "accuracy: 0.000 \n",
      "[4,   486] loss: 5.891 running loss:  3089.845\n",
      "accuracy: 0.100 \n",
      "[4,   496] loss: 5.890 running loss:  3154.552\n",
      "accuracy: 0.000 \n",
      "[4,   506] loss: 7.064 running loss:  3223.927\n",
      "accuracy: 0.000 \n",
      "[4,   516] loss: 6.477 running loss:  3283.724\n",
      "accuracy: 0.000 \n",
      "[4,   526] loss: 7.329 running loss:  3344.012\n",
      "accuracy: 0.100 \n",
      "[4,   536] loss: 6.989 running loss:  3406.344\n",
      "accuracy: 0.000 \n",
      "[4,   546] loss: 7.344 running loss:  3472.634\n",
      "accuracy: 0.000 \n",
      "[4,   556] loss: 5.855 running loss:  3529.969\n",
      "accuracy: 0.100 \n",
      "[4,   566] loss: 6.672 running loss:  3589.085\n",
      "accuracy: 0.100 \n",
      "[4,   576] loss: 4.883 running loss:  3647.380\n",
      "accuracy: 0.200 \n",
      "[4,   586] loss: 5.166 running loss:  3703.851\n",
      "accuracy: 0.200 \n",
      "[4,   596] loss: 5.956 running loss:  3769.269\n",
      "accuracy: 0.000 \n",
      "[4,   606] loss: 7.424 running loss:  3834.806\n",
      "accuracy: 0.000 \n",
      "[4,   616] loss: 7.124 running loss:  3900.111\n",
      "accuracy: 0.000 \n",
      "[4,   626] loss: 6.685 running loss:  3962.752\n",
      "accuracy: 0.000 \n",
      "[4,   636] loss: 6.840 running loss:  4023.008\n",
      "accuracy: 0.100 \n",
      "[4,   646] loss: 7.944 running loss:  4085.086\n",
      "accuracy: 0.200 \n",
      "[4,   656] loss: 6.410 running loss:  4156.133\n",
      "accuracy: 0.000 \n",
      "[4,   666] loss: 8.025 running loss:  4218.692\n",
      "accuracy: 0.000 \n",
      "[4,   676] loss: 5.662 running loss:  4281.069\n",
      "accuracy: 0.000 \n",
      "[4,   686] loss: 8.132 running loss:  4350.371\n",
      "accuracy: 0.000 \n",
      "[4,   696] loss: 5.105 running loss:  4409.404\n",
      "accuracy: 0.100 \n",
      "[4,   706] loss: 7.858 running loss:  4474.654\n",
      "accuracy: 0.200 \n",
      "[4,   716] loss: 5.620 running loss:  4539.763\n",
      "accuracy: 0.100 \n",
      "[4,   726] loss: 7.247 running loss:  4601.114\n",
      "accuracy: 0.100 \n",
      "[4,   736] loss: 5.387 running loss:  4661.897\n",
      "accuracy: 0.000 \n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd7966511dc44569749e1d151206ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,     6] loss: 5.737 running loss:  35.754\n",
      "accuracy: 0.000 \n",
      "[5,    16] loss: 6.431 running loss:  102.049\n",
      "accuracy: 0.100 \n",
      "[5,    26] loss: 7.787 running loss:  165.115\n",
      "accuracy: 0.000 \n",
      "[5,    36] loss: 6.421 running loss:  228.262\n",
      "accuracy: 0.000 \n",
      "[5,    46] loss: 5.380 running loss:  291.866\n",
      "accuracy: 0.000 \n",
      "[5,    56] loss: 5.300 running loss:  355.715\n",
      "accuracy: 0.100 \n",
      "[5,    66] loss: 4.825 running loss:  422.890\n",
      "accuracy: 0.000 \n",
      "[5,    76] loss: 7.258 running loss:  487.212\n",
      "accuracy: 0.000 \n",
      "[5,    86] loss: 5.474 running loss:  551.525\n",
      "accuracy: 0.000 \n",
      "[5,    96] loss: 5.492 running loss:  606.212\n",
      "accuracy: 0.200 \n",
      "[5,   106] loss: 5.771 running loss:  665.709\n",
      "accuracy: 0.200 \n",
      "[5,   116] loss: 5.606 running loss:  722.510\n",
      "accuracy: 0.000 \n",
      "[5,   126] loss: 7.532 running loss:  786.323\n",
      "accuracy: 0.000 \n",
      "[5,   136] loss: 8.021 running loss:  854.158\n",
      "accuracy: 0.000 \n",
      "[5,   146] loss: 6.344 running loss:  914.711\n",
      "accuracy: 0.300 \n",
      "[5,   156] loss: 4.012 running loss:  977.671\n",
      "accuracy: 0.000 \n",
      "[5,   166] loss: 6.629 running loss:  1038.948\n",
      "accuracy: 0.100 \n",
      "[5,   176] loss: 7.450 running loss:  1104.788\n",
      "accuracy: 0.000 \n",
      "[5,   186] loss: 6.304 running loss:  1169.025\n",
      "accuracy: 0.000 \n",
      "[5,   196] loss: 4.299 running loss:  1235.229\n",
      "accuracy: 0.100 \n",
      "[5,   206] loss: 5.752 running loss:  1297.778\n",
      "accuracy: 0.000 \n",
      "[5,   216] loss: 5.873 running loss:  1358.998\n",
      "accuracy: 0.100 \n",
      "[5,   226] loss: 6.435 running loss:  1418.626\n",
      "accuracy: 0.100 \n",
      "[5,   236] loss: 6.794 running loss:  1478.782\n",
      "accuracy: 0.200 \n",
      "[5,   246] loss: 5.911 running loss:  1542.974\n",
      "accuracy: 0.000 \n",
      "[5,   256] loss: 6.141 running loss:  1611.519\n",
      "accuracy: 0.200 \n",
      "[5,   266] loss: 5.017 running loss:  1672.297\n",
      "accuracy: 0.100 \n",
      "[5,   276] loss: 6.277 running loss:  1735.204\n",
      "accuracy: 0.200 \n",
      "[5,   286] loss: 8.206 running loss:  1794.107\n",
      "accuracy: 0.000 \n",
      "[5,   296] loss: 5.433 running loss:  1856.880\n",
      "accuracy: 0.000 \n",
      "[5,   306] loss: 4.805 running loss:  1918.530\n",
      "accuracy: 0.000 \n",
      "[5,   316] loss: 6.376 running loss:  1979.851\n",
      "accuracy: 0.300 \n",
      "[5,   326] loss: 6.181 running loss:  2045.584\n",
      "accuracy: 0.000 \n",
      "[5,   336] loss: 7.002 running loss:  2103.651\n",
      "accuracy: 0.000 \n",
      "[5,   346] loss: 6.097 running loss:  2166.485\n",
      "accuracy: 0.100 \n",
      "[5,   356] loss: 5.845 running loss:  2227.540\n",
      "accuracy: 0.000 \n",
      "[5,   366] loss: 5.009 running loss:  2294.140\n",
      "accuracy: 0.000 \n",
      "[5,   376] loss: 8.330 running loss:  2364.953\n",
      "accuracy: 0.000 \n",
      "[5,   386] loss: 6.508 running loss:  2427.316\n",
      "accuracy: 0.200 \n",
      "[5,   396] loss: 6.565 running loss:  2495.029\n",
      "accuracy: 0.200 \n",
      "[5,   406] loss: 3.773 running loss:  2555.748\n",
      "accuracy: 0.100 \n",
      "[5,   416] loss: 7.636 running loss:  2621.622\n",
      "accuracy: 0.000 \n",
      "[5,   426] loss: 5.543 running loss:  2688.622\n",
      "accuracy: 0.100 \n",
      "[5,   436] loss: 5.942 running loss:  2751.210\n",
      "accuracy: 0.100 \n",
      "[5,   446] loss: 7.605 running loss:  2816.152\n",
      "accuracy: 0.000 \n",
      "[5,   456] loss: 5.484 running loss:  2878.153\n",
      "accuracy: 0.000 \n",
      "[5,   466] loss: 6.003 running loss:  2944.884\n",
      "accuracy: 0.200 \n",
      "[5,   476] loss: 7.343 running loss:  3007.292\n",
      "accuracy: 0.000 \n",
      "[5,   486] loss: 6.483 running loss:  3073.518\n",
      "accuracy: 0.000 \n",
      "[5,   496] loss: 7.386 running loss:  3139.717\n",
      "accuracy: 0.000 \n",
      "[5,   506] loss: 5.643 running loss:  3202.613\n",
      "accuracy: 0.000 \n",
      "[5,   516] loss: 5.627 running loss:  3263.548\n",
      "accuracy: 0.000 \n",
      "[5,   526] loss: 6.191 running loss:  3327.975\n",
      "accuracy: 0.000 \n",
      "[5,   536] loss: 6.317 running loss:  3399.125\n",
      "accuracy: 0.000 \n",
      "[5,   546] loss: 5.801 running loss:  3459.405\n",
      "accuracy: 0.200 \n",
      "[5,   556] loss: 6.308 running loss:  3515.636\n",
      "accuracy: 0.100 \n",
      "[5,   566] loss: 6.879 running loss:  3584.120\n",
      "accuracy: 0.100 \n",
      "[5,   576] loss: 6.691 running loss:  3646.010\n",
      "accuracy: 0.100 \n",
      "[5,   586] loss: 4.021 running loss:  3703.757\n",
      "accuracy: 0.100 \n",
      "[5,   596] loss: 6.452 running loss:  3769.709\n",
      "accuracy: 0.100 \n",
      "[5,   606] loss: 4.672 running loss:  3835.259\n",
      "accuracy: 0.000 \n",
      "[5,   616] loss: 6.899 running loss:  3898.899\n",
      "accuracy: 0.000 \n",
      "[5,   626] loss: 6.742 running loss:  3958.808\n",
      "accuracy: 0.000 \n",
      "[5,   636] loss: 6.119 running loss:  4021.654\n",
      "accuracy: 0.100 \n",
      "[5,   646] loss: 5.732 running loss:  4085.522\n",
      "accuracy: 0.000 \n",
      "[5,   656] loss: 4.338 running loss:  4145.361\n",
      "accuracy: 0.000 \n",
      "[5,   666] loss: 8.161 running loss:  4208.507\n",
      "accuracy: 0.000 \n",
      "[5,   676] loss: 5.120 running loss:  4267.866\n",
      "accuracy: 0.000 \n",
      "[5,   686] loss: 8.544 running loss:  4332.614\n",
      "accuracy: 0.000 \n",
      "[5,   696] loss: 4.941 running loss:  4394.641\n",
      "accuracy: 0.200 \n",
      "[5,   706] loss: 5.712 running loss:  4453.857\n",
      "accuracy: 0.200 \n",
      "[5,   716] loss: 6.107 running loss:  4521.763\n",
      "accuracy: 0.100 \n",
      "[5,   726] loss: 5.921 running loss:  4592.751\n",
      "accuracy: 0.100 \n",
      "[5,   736] loss: 7.650 running loss:  4656.499\n",
      "accuracy: 0.000 \n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02961e585db043108841448770894424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,     6] loss: 6.231 running loss:  31.749\n",
      "accuracy: 0.200 \n",
      "[6,    16] loss: 6.177 running loss:  91.805\n",
      "accuracy: 0.000 \n",
      "[6,    26] loss: 4.150 running loss:  154.805\n",
      "accuracy: 0.100 \n",
      "[6,    36] loss: 6.654 running loss:  216.939\n",
      "accuracy: 0.000 \n",
      "[6,    46] loss: 5.511 running loss:  286.431\n",
      "accuracy: 0.000 \n",
      "[6,    56] loss: 7.617 running loss:  349.568\n",
      "accuracy: 0.100 \n",
      "[6,    66] loss: 7.927 running loss:  413.320\n",
      "accuracy: 0.100 \n",
      "[6,    76] loss: 8.098 running loss:  481.124\n",
      "accuracy: 0.000 \n",
      "[6,    86] loss: 7.721 running loss:  543.449\n",
      "accuracy: 0.100 \n",
      "[6,    96] loss: 7.068 running loss:  608.869\n",
      "accuracy: 0.300 \n",
      "[6,   106] loss: 6.425 running loss:  675.494\n",
      "accuracy: 0.000 \n",
      "[6,   116] loss: 5.975 running loss:  734.939\n",
      "accuracy: 0.100 \n",
      "[6,   126] loss: 4.727 running loss:  792.760\n",
      "accuracy: 0.000 \n",
      "[6,   136] loss: 6.363 running loss:  857.086\n",
      "accuracy: 0.100 \n",
      "[6,   146] loss: 4.087 running loss:  918.213\n",
      "accuracy: 0.100 \n",
      "[6,   156] loss: 6.236 running loss:  978.966\n",
      "accuracy: 0.200 \n",
      "[6,   166] loss: 6.338 running loss:  1040.736\n",
      "accuracy: 0.000 \n",
      "[6,   176] loss: 4.806 running loss:  1108.645\n",
      "accuracy: 0.000 \n",
      "[6,   186] loss: 7.269 running loss:  1179.182\n",
      "accuracy: 0.000 \n",
      "[6,   196] loss: 5.866 running loss:  1242.958\n",
      "accuracy: 0.000 \n",
      "[6,   206] loss: 6.938 running loss:  1306.642\n",
      "accuracy: 0.000 \n",
      "[6,   216] loss: 6.813 running loss:  1377.559\n",
      "accuracy: 0.000 \n",
      "[6,   226] loss: 6.145 running loss:  1436.968\n",
      "accuracy: 0.200 \n",
      "[6,   236] loss: 7.348 running loss:  1501.869\n",
      "accuracy: 0.000 \n",
      "[6,   246] loss: 6.702 running loss:  1564.154\n",
      "accuracy: 0.300 \n",
      "[6,   256] loss: 9.266 running loss:  1630.876\n",
      "accuracy: 0.100 \n",
      "[6,   266] loss: 7.194 running loss:  1701.394\n",
      "accuracy: 0.200 \n",
      "[6,   276] loss: 6.357 running loss:  1759.021\n",
      "accuracy: 0.000 \n",
      "[6,   286] loss: 4.731 running loss:  1821.707\n",
      "accuracy: 0.000 \n",
      "[6,   296] loss: 6.246 running loss:  1879.895\n",
      "accuracy: 0.000 \n",
      "[6,   306] loss: 7.900 running loss:  1948.989\n",
      "accuracy: 0.000 \n",
      "[6,   316] loss: 5.442 running loss:  2014.271\n",
      "accuracy: 0.000 \n",
      "[6,   326] loss: 5.669 running loss:  2072.236\n",
      "accuracy: 0.000 \n",
      "[6,   336] loss: 7.455 running loss:  2134.823\n",
      "accuracy: 0.200 \n",
      "[6,   346] loss: 5.228 running loss:  2197.292\n",
      "accuracy: 0.200 \n",
      "[6,   356] loss: 4.858 running loss:  2256.940\n",
      "accuracy: 0.100 \n",
      "[6,   366] loss: 6.021 running loss:  2322.412\n",
      "accuracy: 0.200 \n",
      "[6,   376] loss: 6.107 running loss:  2387.972\n",
      "accuracy: 0.100 \n",
      "[6,   386] loss: 4.009 running loss:  2448.623\n",
      "accuracy: 0.100 \n",
      "[6,   396] loss: 7.124 running loss:  2509.741\n",
      "accuracy: 0.000 \n",
      "[6,   406] loss: 5.674 running loss:  2571.205\n",
      "accuracy: 0.100 \n",
      "[6,   416] loss: 6.374 running loss:  2627.807\n",
      "accuracy: 0.000 \n",
      "[6,   426] loss: 6.780 running loss:  2690.998\n",
      "accuracy: 0.100 \n",
      "[6,   436] loss: 8.502 running loss:  2762.150\n",
      "accuracy: 0.000 \n",
      "[6,   446] loss: 5.868 running loss:  2822.536\n",
      "accuracy: 0.000 \n",
      "[6,   456] loss: 7.457 running loss:  2888.995\n",
      "accuracy: 0.000 \n",
      "[6,   466] loss: 7.171 running loss:  2946.888\n",
      "accuracy: 0.200 \n",
      "[6,   476] loss: 6.940 running loss:  3006.345\n",
      "accuracy: 0.300 \n",
      "[6,   486] loss: 7.261 running loss:  3068.459\n",
      "accuracy: 0.000 \n",
      "[6,   496] loss: 8.785 running loss:  3132.960\n",
      "accuracy: 0.000 \n",
      "[6,   506] loss: 6.104 running loss:  3194.530\n",
      "accuracy: 0.000 \n",
      "[6,   516] loss: 7.958 running loss:  3252.653\n",
      "accuracy: 0.200 \n",
      "[6,   526] loss: 5.363 running loss:  3314.334\n",
      "accuracy: 0.000 \n",
      "[6,   536] loss: 5.198 running loss:  3379.621\n",
      "accuracy: 0.000 \n",
      "[6,   546] loss: 4.678 running loss:  3437.441\n",
      "accuracy: 0.100 \n",
      "[6,   556] loss: 8.048 running loss:  3501.517\n",
      "accuracy: 0.000 \n",
      "[6,   566] loss: 5.179 running loss:  3562.729\n",
      "accuracy: 0.000 \n",
      "[6,   576] loss: 5.819 running loss:  3625.433\n",
      "accuracy: 0.000 \n",
      "[6,   586] loss: 7.101 running loss:  3687.166\n",
      "accuracy: 0.000 \n",
      "[6,   596] loss: 5.845 running loss:  3754.728\n",
      "accuracy: 0.200 \n",
      "[6,   606] loss: 5.977 running loss:  3818.270\n",
      "accuracy: 0.100 \n",
      "[6,   616] loss: 8.198 running loss:  3876.590\n",
      "accuracy: 0.100 \n",
      "[6,   626] loss: 7.439 running loss:  3938.671\n",
      "accuracy: 0.000 \n",
      "[6,   636] loss: 4.939 running loss:  3993.498\n",
      "accuracy: 0.000 \n",
      "[6,   646] loss: 6.216 running loss:  4059.414\n",
      "accuracy: 0.100 \n",
      "[6,   656] loss: 7.647 running loss:  4130.651\n",
      "accuracy: 0.100 \n",
      "[6,   666] loss: 5.405 running loss:  4195.975\n",
      "accuracy: 0.000 \n",
      "[6,   676] loss: 6.381 running loss:  4263.414\n",
      "accuracy: 0.100 \n",
      "[6,   686] loss: 7.159 running loss:  4329.333\n",
      "accuracy: 0.000 \n",
      "[6,   696] loss: 6.421 running loss:  4388.243\n",
      "accuracy: 0.000 \n",
      "[6,   706] loss: 4.518 running loss:  4455.611\n",
      "accuracy: 0.000 \n",
      "[6,   716] loss: 6.090 running loss:  4521.401\n",
      "accuracy: 0.000 \n",
      "[6,   726] loss: 5.193 running loss:  4587.037\n",
      "accuracy: 0.000 \n",
      "[6,   736] loss: 6.585 running loss:  4650.514\n",
      "accuracy: 0.200 \n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45f6e2ec8c74ff7bf2f5483ed0469c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,     6] loss: 6.144 running loss:  32.341\n",
      "accuracy: 0.300 \n",
      "[7,    16] loss: 6.974 running loss:  97.978\n",
      "accuracy: 0.000 \n",
      "[7,    26] loss: 6.350 running loss:  157.128\n",
      "accuracy: 0.100 \n",
      "[7,    36] loss: 7.414 running loss:  223.996\n",
      "accuracy: 0.200 \n",
      "[7,    46] loss: 6.559 running loss:  294.158\n",
      "accuracy: 0.300 \n",
      "[7,    56] loss: 7.069 running loss:  362.175\n",
      "accuracy: 0.100 \n",
      "[7,    66] loss: 6.091 running loss:  424.396\n",
      "accuracy: 0.000 \n",
      "[7,    76] loss: 5.504 running loss:  488.206\n",
      "accuracy: 0.000 \n",
      "[7,    86] loss: 8.325 running loss:  555.729\n",
      "accuracy: 0.000 \n",
      "[7,    96] loss: 8.756 running loss:  617.295\n",
      "accuracy: 0.000 \n",
      "[7,   106] loss: 7.059 running loss:  682.646\n",
      "accuracy: 0.100 \n",
      "[7,   116] loss: 5.474 running loss:  742.536\n",
      "accuracy: 0.100 \n",
      "[7,   126] loss: 6.902 running loss:  813.524\n",
      "accuracy: 0.100 \n",
      "[7,   136] loss: 3.597 running loss:  873.643\n",
      "accuracy: 0.000 \n",
      "[7,   146] loss: 6.285 running loss:  939.490\n",
      "accuracy: 0.200 \n",
      "[7,   156] loss: 5.950 running loss:  999.323\n",
      "accuracy: 0.200 \n",
      "[7,   166] loss: 4.479 running loss:  1066.519\n",
      "accuracy: 0.100 \n",
      "[7,   176] loss: 6.450 running loss:  1140.275\n",
      "accuracy: 0.000 \n",
      "[7,   186] loss: 6.298 running loss:  1205.044\n",
      "accuracy: 0.100 \n",
      "[7,   196] loss: 7.083 running loss:  1278.439\n",
      "accuracy: 0.000 \n",
      "[7,   206] loss: 7.220 running loss:  1351.799\n",
      "accuracy: 0.000 \n",
      "[7,   216] loss: 6.433 running loss:  1419.278\n",
      "accuracy: 0.100 \n",
      "[7,   226] loss: 5.708 running loss:  1478.146\n",
      "accuracy: 0.100 \n",
      "[7,   236] loss: 5.055 running loss:  1536.483\n",
      "accuracy: 0.000 \n",
      "[7,   246] loss: 5.242 running loss:  1593.039\n",
      "accuracy: 0.000 \n",
      "[7,   256] loss: 6.551 running loss:  1658.223\n",
      "accuracy: 0.000 \n",
      "[7,   266] loss: 6.326 running loss:  1713.806\n",
      "accuracy: 0.200 \n",
      "[7,   276] loss: 5.170 running loss:  1775.140\n",
      "accuracy: 0.100 \n",
      "[7,   286] loss: 6.374 running loss:  1835.727\n",
      "accuracy: 0.000 \n",
      "[7,   296] loss: 4.709 running loss:  1898.873\n",
      "accuracy: 0.000 \n",
      "[7,   306] loss: 5.613 running loss:  1962.849\n",
      "accuracy: 0.100 \n",
      "[7,   316] loss: 5.751 running loss:  2020.876\n",
      "accuracy: 0.200 \n",
      "[7,   326] loss: 4.607 running loss:  2081.379\n",
      "accuracy: 0.000 \n",
      "[7,   336] loss: 4.378 running loss:  2139.945\n",
      "accuracy: 0.100 \n",
      "[7,   346] loss: 5.405 running loss:  2203.481\n",
      "accuracy: 0.000 \n",
      "[7,   356] loss: 4.727 running loss:  2271.189\n",
      "accuracy: 0.000 \n",
      "[7,   366] loss: 6.408 running loss:  2328.514\n",
      "accuracy: 0.100 \n",
      "[7,   376] loss: 4.977 running loss:  2391.133\n",
      "accuracy: 0.000 \n",
      "[7,   386] loss: 6.673 running loss:  2460.598\n",
      "accuracy: 0.200 \n",
      "[7,   396] loss: 9.428 running loss:  2526.464\n",
      "accuracy: 0.000 \n",
      "[7,   406] loss: 8.914 running loss:  2594.456\n",
      "accuracy: 0.000 \n",
      "[7,   416] loss: 5.910 running loss:  2659.312\n",
      "accuracy: 0.000 \n",
      "[7,   426] loss: 6.185 running loss:  2718.338\n",
      "accuracy: 0.000 \n",
      "[7,   436] loss: 5.286 running loss:  2778.343\n",
      "accuracy: 0.000 \n",
      "[7,   446] loss: 8.121 running loss:  2846.983\n",
      "accuracy: 0.000 \n",
      "[7,   456] loss: 5.233 running loss:  2914.587\n",
      "accuracy: 0.100 \n",
      "[7,   466] loss: 5.931 running loss:  2978.946\n",
      "accuracy: 0.100 \n",
      "[7,   476] loss: 5.466 running loss:  3041.291\n",
      "accuracy: 0.000 \n",
      "[7,   486] loss: 6.591 running loss:  3104.741\n",
      "accuracy: 0.100 \n",
      "[7,   496] loss: 7.217 running loss:  3167.881\n",
      "accuracy: 0.100 \n",
      "[7,   506] loss: 7.974 running loss:  3233.274\n",
      "accuracy: 0.200 \n",
      "[7,   516] loss: 7.396 running loss:  3299.271\n",
      "accuracy: 0.000 \n",
      "[7,   526] loss: 5.221 running loss:  3368.603\n",
      "accuracy: 0.000 \n",
      "[7,   536] loss: 6.857 running loss:  3429.725\n",
      "accuracy: 0.100 \n",
      "[7,   546] loss: 6.633 running loss:  3493.517\n",
      "accuracy: 0.000 \n",
      "[7,   556] loss: 7.169 running loss:  3554.716\n",
      "accuracy: 0.300 \n",
      "[7,   566] loss: 6.623 running loss:  3618.279\n",
      "accuracy: 0.400 \n",
      "[7,   576] loss: 6.043 running loss:  3679.127\n",
      "accuracy: 0.100 \n",
      "[7,   586] loss: 4.545 running loss:  3734.981\n",
      "accuracy: 0.000 \n",
      "[7,   596] loss: 5.345 running loss:  3799.031\n",
      "accuracy: 0.200 \n",
      "[7,   606] loss: 5.875 running loss:  3855.013\n",
      "accuracy: 0.300 \n",
      "[7,   616] loss: 6.801 running loss:  3921.101\n",
      "accuracy: 0.000 \n",
      "[7,   626] loss: 4.623 running loss:  3975.275\n",
      "accuracy: 0.100 \n",
      "[7,   636] loss: 6.546 running loss:  4034.234\n",
      "accuracy: 0.000 \n",
      "[7,   646] loss: 7.473 running loss:  4100.594\n",
      "accuracy: 0.100 \n",
      "[7,   656] loss: 5.866 running loss:  4164.573\n",
      "accuracy: 0.000 \n",
      "[7,   666] loss: 6.800 running loss:  4232.313\n",
      "accuracy: 0.000 \n",
      "[7,   676] loss: 7.173 running loss:  4296.054\n",
      "accuracy: 0.100 \n",
      "[7,   686] loss: 5.580 running loss:  4351.490\n",
      "accuracy: 0.100 \n",
      "[7,   696] loss: 5.192 running loss:  4411.958\n",
      "accuracy: 0.100 \n",
      "[7,   706] loss: 6.847 running loss:  4477.145\n",
      "accuracy: 0.100 \n",
      "[7,   716] loss: 5.154 running loss:  4539.431\n",
      "accuracy: 0.100 \n",
      "[7,   726] loss: 4.873 running loss:  4593.228\n",
      "accuracy: 0.400 \n",
      "[7,   736] loss: 3.559 running loss:  4652.863\n",
      "accuracy: 0.000 \n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1f0ebe46b64f4785ef240595d8af2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     6] loss: 5.498 running loss:  30.152\n",
      "accuracy: 0.100 \n",
      "[8,    16] loss: 4.474 running loss:  93.487\n",
      "accuracy: 0.100 \n",
      "[8,    26] loss: 5.525 running loss:  157.993\n",
      "accuracy: 0.000 \n",
      "[8,    36] loss: 7.004 running loss:  229.142\n",
      "accuracy: 0.000 \n",
      "[8,    46] loss: 6.481 running loss:  289.579\n",
      "accuracy: 0.000 \n",
      "[8,    56] loss: 6.567 running loss:  352.383\n",
      "accuracy: 0.100 \n",
      "[8,    66] loss: 5.676 running loss:  409.764\n",
      "accuracy: 0.200 \n",
      "[8,    76] loss: 5.255 running loss:  471.391\n",
      "accuracy: 0.100 \n",
      "[8,    86] loss: 8.666 running loss:  535.707\n",
      "accuracy: 0.100 \n",
      "[8,    96] loss: 6.451 running loss:  603.311\n",
      "accuracy: 0.000 \n",
      "[8,   106] loss: 4.510 running loss:  667.851\n",
      "accuracy: 0.100 \n",
      "[8,   116] loss: 7.438 running loss:  733.835\n",
      "accuracy: 0.200 \n",
      "[8,   126] loss: 6.702 running loss:  793.793\n",
      "accuracy: 0.100 \n",
      "[8,   136] loss: 5.952 running loss:  856.666\n",
      "accuracy: 0.000 \n",
      "[8,   146] loss: 6.509 running loss:  916.648\n",
      "accuracy: 0.000 \n",
      "[8,   156] loss: 3.953 running loss:  977.360\n",
      "accuracy: 0.000 \n",
      "[8,   166] loss: 6.107 running loss:  1045.326\n",
      "accuracy: 0.100 \n",
      "[8,   176] loss: 6.395 running loss:  1108.185\n",
      "accuracy: 0.000 \n",
      "[8,   186] loss: 8.222 running loss:  1172.165\n",
      "accuracy: 0.100 \n",
      "[8,   196] loss: 7.084 running loss:  1239.201\n",
      "accuracy: 0.000 \n",
      "[8,   206] loss: 5.898 running loss:  1302.166\n",
      "accuracy: 0.000 \n",
      "[8,   216] loss: 6.625 running loss:  1360.096\n",
      "accuracy: 0.100 \n",
      "[8,   226] loss: 5.599 running loss:  1423.593\n",
      "accuracy: 0.200 \n",
      "[8,   236] loss: 6.449 running loss:  1483.890\n",
      "accuracy: 0.000 \n",
      "[8,   246] loss: 5.451 running loss:  1554.335\n",
      "accuracy: 0.000 \n",
      "[8,   256] loss: 6.499 running loss:  1615.823\n",
      "accuracy: 0.200 \n",
      "[8,   266] loss: 5.675 running loss:  1678.752\n",
      "accuracy: 0.000 \n",
      "[8,   276] loss: 5.956 running loss:  1743.776\n",
      "accuracy: 0.000 \n",
      "[8,   286] loss: 8.141 running loss:  1810.543\n",
      "accuracy: 0.100 \n",
      "[8,   296] loss: 5.241 running loss:  1872.434\n",
      "accuracy: 0.000 \n",
      "[8,   306] loss: 6.115 running loss:  1934.649\n",
      "accuracy: 0.100 \n",
      "[8,   316] loss: 4.865 running loss:  2000.819\n",
      "accuracy: 0.200 \n",
      "[8,   326] loss: 5.219 running loss:  2062.395\n",
      "accuracy: 0.100 \n",
      "[8,   336] loss: 3.984 running loss:  2124.258\n",
      "accuracy: 0.100 \n",
      "[8,   346] loss: 7.854 running loss:  2186.948\n",
      "accuracy: 0.100 \n",
      "[8,   356] loss: 6.419 running loss:  2250.647\n",
      "accuracy: 0.200 \n",
      "[8,   366] loss: 3.579 running loss:  2307.734\n",
      "accuracy: 0.100 \n",
      "[8,   376] loss: 6.734 running loss:  2373.561\n",
      "accuracy: 0.200 \n",
      "[8,   386] loss: 4.790 running loss:  2437.065\n",
      "accuracy: 0.200 \n",
      "[8,   396] loss: 7.242 running loss:  2505.520\n",
      "accuracy: 0.100 \n",
      "[8,   406] loss: 5.956 running loss:  2573.097\n",
      "accuracy: 0.000 \n",
      "[8,   416] loss: 5.978 running loss:  2630.710\n",
      "accuracy: 0.100 \n",
      "[8,   426] loss: 7.017 running loss:  2700.619\n",
      "accuracy: 0.100 \n",
      "[8,   436] loss: 6.257 running loss:  2762.064\n",
      "accuracy: 0.100 \n",
      "[8,   446] loss: 4.167 running loss:  2820.641\n",
      "accuracy: 0.000 \n",
      "[8,   456] loss: 4.909 running loss:  2883.917\n",
      "accuracy: 0.100 \n",
      "[8,   466] loss: 5.487 running loss:  2947.165\n",
      "accuracy: 0.100 \n",
      "[8,   476] loss: 7.117 running loss:  3008.781\n",
      "accuracy: 0.100 \n",
      "[8,   486] loss: 5.522 running loss:  3074.998\n",
      "accuracy: 0.000 \n",
      "[8,   496] loss: 4.933 running loss:  3138.832\n",
      "accuracy: 0.100 \n",
      "[8,   506] loss: 4.204 running loss:  3193.040\n",
      "accuracy: 0.000 \n",
      "[8,   516] loss: 6.532 running loss:  3256.896\n",
      "accuracy: 0.000 \n",
      "[8,   526] loss: 7.127 running loss:  3315.649\n",
      "accuracy: 0.000 \n",
      "[8,   536] loss: 6.915 running loss:  3380.405\n",
      "accuracy: 0.000 \n",
      "[8,   546] loss: 6.910 running loss:  3443.678\n",
      "accuracy: 0.100 \n",
      "[8,   556] loss: 6.080 running loss:  3505.821\n",
      "accuracy: 0.300 \n",
      "[8,   566] loss: 6.854 running loss:  3570.267\n",
      "accuracy: 0.100 \n",
      "[8,   576] loss: 5.335 running loss:  3632.298\n",
      "accuracy: 0.000 \n",
      "[8,   586] loss: 3.847 running loss:  3695.474\n",
      "accuracy: 0.000 \n",
      "[8,   596] loss: 6.901 running loss:  3758.569\n",
      "accuracy: 0.000 \n",
      "[8,   606] loss: 5.534 running loss:  3825.862\n",
      "accuracy: 0.100 \n",
      "[8,   616] loss: 5.706 running loss:  3883.873\n",
      "accuracy: 0.000 \n",
      "[8,   626] loss: 5.718 running loss:  3943.387\n",
      "accuracy: 0.000 \n",
      "[8,   636] loss: 6.950 running loss:  4006.251\n",
      "accuracy: 0.100 \n",
      "[8,   646] loss: 7.404 running loss:  4070.614\n",
      "accuracy: 0.100 \n",
      "[8,   656] loss: 5.937 running loss:  4135.540\n",
      "accuracy: 0.000 \n",
      "[8,   666] loss: 6.866 running loss:  4206.162\n",
      "accuracy: 0.000 \n",
      "[8,   676] loss: 4.443 running loss:  4269.211\n",
      "accuracy: 0.000 \n",
      "[8,   686] loss: 7.969 running loss:  4332.040\n",
      "accuracy: 0.100 \n",
      "[8,   696] loss: 5.179 running loss:  4393.645\n",
      "accuracy: 0.000 \n",
      "[8,   706] loss: 6.238 running loss:  4458.580\n",
      "accuracy: 0.000 \n",
      "[8,   716] loss: 6.896 running loss:  4526.282\n",
      "accuracy: 0.000 \n",
      "[8,   726] loss: 5.912 running loss:  4587.170\n",
      "accuracy: 0.100 \n",
      "[8,   736] loss: 5.827 running loss:  4652.351\n",
      "accuracy: 0.000 \n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a6787454624004a5217152ba4c06bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,     6] loss: 5.680 running loss:  32.347\n",
      "accuracy: 0.100 \n",
      "[9,    16] loss: 8.633 running loss:  98.871\n",
      "accuracy: 0.000 \n",
      "[9,    26] loss: 6.556 running loss:  164.434\n",
      "accuracy: 0.100 \n",
      "[9,    36] loss: 6.761 running loss:  226.156\n",
      "accuracy: 0.000 \n",
      "[9,    46] loss: 7.053 running loss:  291.968\n",
      "accuracy: 0.100 \n",
      "[9,    56] loss: 8.172 running loss:  363.080\n",
      "accuracy: 0.100 \n",
      "[9,    66] loss: 6.968 running loss:  430.712\n",
      "accuracy: 0.000 \n",
      "[9,    76] loss: 6.332 running loss:  495.352\n",
      "accuracy: 0.200 \n",
      "[9,    86] loss: 5.756 running loss:  556.862\n",
      "accuracy: 0.000 \n",
      "[9,    96] loss: 5.478 running loss:  620.845\n",
      "accuracy: 0.000 \n",
      "[9,   106] loss: 6.829 running loss:  678.384\n",
      "accuracy: 0.000 \n",
      "[9,   116] loss: 4.744 running loss:  740.499\n",
      "accuracy: 0.000 \n",
      "[9,   126] loss: 8.397 running loss:  805.893\n",
      "accuracy: 0.000 \n",
      "[9,   136] loss: 7.333 running loss:  858.698\n",
      "accuracy: 0.100 \n",
      "[9,   146] loss: 4.852 running loss:  918.682\n",
      "accuracy: 0.000 \n",
      "[9,   156] loss: 6.753 running loss:  978.943\n",
      "accuracy: 0.000 \n",
      "[9,   166] loss: 6.322 running loss:  1040.989\n",
      "accuracy: 0.000 \n",
      "[9,   176] loss: 5.851 running loss:  1103.172\n",
      "accuracy: 0.200 \n",
      "[9,   186] loss: 5.250 running loss:  1161.627\n",
      "accuracy: 0.000 \n",
      "[9,   196] loss: 5.745 running loss:  1223.389\n",
      "accuracy: 0.100 \n",
      "[9,   206] loss: 8.178 running loss:  1289.999\n",
      "accuracy: 0.000 \n",
      "[9,   216] loss: 5.676 running loss:  1351.314\n",
      "accuracy: 0.000 \n",
      "[9,   226] loss: 5.391 running loss:  1408.909\n",
      "accuracy: 0.000 \n",
      "[9,   236] loss: 5.808 running loss:  1473.276\n",
      "accuracy: 0.100 \n",
      "[9,   246] loss: 7.999 running loss:  1545.425\n",
      "accuracy: 0.000 \n",
      "[9,   256] loss: 6.986 running loss:  1610.358\n",
      "accuracy: 0.200 \n",
      "[9,   266] loss: 4.971 running loss:  1673.403\n",
      "accuracy: 0.200 \n",
      "[9,   276] loss: 6.538 running loss:  1736.916\n",
      "accuracy: 0.100 \n",
      "[9,   286] loss: 5.778 running loss:  1803.831\n",
      "accuracy: 0.000 \n",
      "[9,   296] loss: 6.426 running loss:  1870.946\n",
      "accuracy: 0.000 \n",
      "[9,   306] loss: 6.487 running loss:  1938.988\n",
      "accuracy: 0.000 \n",
      "[9,   316] loss: 6.720 running loss:  2002.700\n",
      "accuracy: 0.100 \n",
      "[9,   326] loss: 6.824 running loss:  2065.125\n",
      "accuracy: 0.200 \n",
      "[9,   336] loss: 6.844 running loss:  2129.167\n",
      "accuracy: 0.000 \n",
      "[9,   346] loss: 6.918 running loss:  2198.660\n",
      "accuracy: 0.100 \n",
      "[9,   356] loss: 5.919 running loss:  2260.427\n",
      "accuracy: 0.000 \n",
      "[9,   366] loss: 8.793 running loss:  2322.163\n",
      "accuracy: 0.100 \n",
      "[9,   376] loss: 7.583 running loss:  2384.452\n",
      "accuracy: 0.000 \n",
      "[9,   386] loss: 3.973 running loss:  2443.793\n",
      "accuracy: 0.100 \n",
      "[9,   396] loss: 6.578 running loss:  2506.255\n",
      "accuracy: 0.000 \n",
      "[9,   406] loss: 5.798 running loss:  2569.464\n",
      "accuracy: 0.000 \n",
      "[9,   416] loss: 6.625 running loss:  2630.584\n",
      "accuracy: 0.100 \n",
      "[9,   426] loss: 7.615 running loss:  2692.887\n",
      "accuracy: 0.100 \n",
      "[9,   436] loss: 5.512 running loss:  2755.192\n",
      "accuracy: 0.000 \n",
      "[9,   446] loss: 5.911 running loss:  2819.465\n",
      "accuracy: 0.200 \n",
      "[9,   456] loss: 6.949 running loss:  2880.150\n",
      "accuracy: 0.000 \n",
      "[9,   466] loss: 6.336 running loss:  2946.574\n",
      "accuracy: 0.000 \n",
      "[9,   476] loss: 7.199 running loss:  3012.394\n",
      "accuracy: 0.000 \n",
      "[9,   486] loss: 7.267 running loss:  3073.626\n",
      "accuracy: 0.000 \n",
      "[9,   496] loss: 5.504 running loss:  3131.937\n",
      "accuracy: 0.200 \n",
      "[9,   506] loss: 7.013 running loss:  3196.531\n",
      "accuracy: 0.100 \n",
      "[9,   516] loss: 7.140 running loss:  3256.265\n",
      "accuracy: 0.100 \n",
      "[9,   526] loss: 6.049 running loss:  3318.376\n",
      "accuracy: 0.300 \n",
      "[9,   536] loss: 7.205 running loss:  3379.381\n",
      "accuracy: 0.100 \n",
      "[9,   546] loss: 7.087 running loss:  3434.509\n",
      "accuracy: 0.200 \n",
      "[9,   556] loss: 7.144 running loss:  3499.812\n",
      "accuracy: 0.100 \n",
      "[9,   566] loss: 5.066 running loss:  3564.724\n",
      "accuracy: 0.200 \n",
      "[9,   576] loss: 5.913 running loss:  3623.651\n",
      "accuracy: 0.100 \n",
      "[9,   586] loss: 5.416 running loss:  3687.891\n",
      "accuracy: 0.000 \n",
      "[9,   596] loss: 6.843 running loss:  3750.365\n",
      "accuracy: 0.200 \n",
      "[9,   606] loss: 5.987 running loss:  3813.060\n",
      "accuracy: 0.200 \n",
      "[9,   616] loss: 6.906 running loss:  3880.320\n",
      "accuracy: 0.100 \n",
      "[9,   626] loss: 5.625 running loss:  3943.567\n",
      "accuracy: 0.200 \n",
      "[9,   636] loss: 8.586 running loss:  4014.306\n",
      "accuracy: 0.100 \n",
      "[9,   646] loss: 5.894 running loss:  4075.961\n",
      "accuracy: 0.200 \n",
      "[9,   656] loss: 6.122 running loss:  4138.729\n",
      "accuracy: 0.000 \n",
      "[9,   666] loss: 5.203 running loss:  4198.592\n",
      "accuracy: 0.100 \n",
      "[9,   676] loss: 6.110 running loss:  4264.621\n",
      "accuracy: 0.000 \n",
      "[9,   686] loss: 6.168 running loss:  4335.743\n",
      "accuracy: 0.000 \n",
      "[9,   696] loss: 5.444 running loss:  4392.756\n",
      "accuracy: 0.100 \n",
      "[9,   706] loss: 5.169 running loss:  4456.293\n",
      "accuracy: 0.100 \n",
      "[9,   716] loss: 6.292 running loss:  4524.165\n",
      "accuracy: 0.000 \n",
      "[9,   726] loss: 8.174 running loss:  4589.307\n",
      "accuracy: 0.000 \n",
      "[9,   736] loss: 6.435 running loss:  4652.295\n",
      "accuracy: 0.000 \n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383b10577f4244ce84e489c5a8b01e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     6] loss: 6.481 running loss:  36.025\n",
      "accuracy: 0.100 \n",
      "[10,    16] loss: 5.369 running loss:  101.089\n",
      "accuracy: 0.000 \n",
      "[10,    26] loss: 6.376 running loss:  165.446\n",
      "accuracy: 0.200 \n",
      "[10,    36] loss: 4.409 running loss:  232.962\n",
      "accuracy: 0.100 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m criterion(y_predicted\u001b[38;5;241m.\u001b[39mfloat(), y_truth\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     67\u001b[0m acc_val\u001b[38;5;241m=\u001b[39m (y_predicted\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y_truth\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 69\u001b[0m errD_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     70\u001b[0m loss\u001b[38;5;241m=\u001b[39merrD_real\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     71\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "if parser.device!='cpu':\n",
    "    vision_transformer.to('cuda')\n",
    "\n",
    "# prepare model fr training\n",
    "vision_transformer.train()\n",
    "\n",
    "\n",
    "for epoch in range(parser.epochs):\n",
    "    x=0\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    acc_val=[]\n",
    "\n",
    "    print('Epoch {}/{}'.format(epoch, parser.epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    #dataloader = utils.get_data_with_labels(512, 512,0.9, boxImagesPath,parser.batch_size,drop_last=True)\n",
    "\n",
    "\n",
    "    for data in tqdm(dataloader):\n",
    "        \n",
    "        if parser.device!='cpu':\n",
    "            images, classes, names, classes_types = data.to('cuda')\n",
    "        else:\n",
    "            images, classes, names, classes_types = data\n",
    "\n",
    "            \n",
    "        a = []\n",
    "        idx=0\n",
    "        \n",
    "        \"\"\"lookup for data corresponding to every image in training batch\"\"\"\n",
    "        for name in names:\n",
    "            series=name.split('_')[-1].split('.')[0]\n",
    "            batch=name.split('_')[4]\n",
    "            for name in glob.glob(DataPath+batch+'\\\\files\\\\'+'/'+parser.metricType+'*'+series+'.csv'): \n",
    "                \n",
    "                #loading the absorption data\n",
    "                train = pd.read_csv(name)\n",
    "                values=np.array(train.values.T)\n",
    "                a.append(values[1])\n",
    "                #print(name)\n",
    "                #print(values[1].shape)\n",
    "        \n",
    "        a=np.array(a)     \n",
    "        #print(a.shape)\n",
    "        conditioningArray=torch.FloatTensor(set_conditioning(classes, names, classes_types))\n",
    "\n",
    "        if conditioningArray.shape[1]==parser.condition_len:\n",
    "            \n",
    "            \n",
    "            opt.zero_grad()\n",
    "\n",
    "            #for conditioning in case required\n",
    "            outmap_min, _ = torch.min(conditioningArray, dim=1, keepdim=True)\n",
    "            outmap_max, _ = torch.max(conditioningArray, dim=1, keepdim=True)\n",
    "            conditioningTensor = (conditioningArray - outmap_min) / (outmap_max - outmap_min)\n",
    "\n",
    "            \n",
    "            # feedforward data\n",
    "\n",
    "            y_predicted = vision_transformer(images,condition=conditioningArray)\n",
    "\n",
    "            y_truth = torch.tensor(a)\n",
    "\n",
    "            errD_real = criterion(y_predicted.float(), y_truth.float())\n",
    "\n",
    "            acc_val= (y_predicted.argmax(dim=-1) == y_truth.argmax(dim=-1)).float().mean()\n",
    "            \n",
    "            errD_real.backward()\n",
    "            loss=errD_real.item()\n",
    "            opt.step()\n",
    "            scale = torch.tensor([10.0])\n",
    "\n",
    "            running_loss +=loss\n",
    "\n",
    "\n",
    "            x += 1\n",
    "            i = i+1\n",
    "\n",
    "\n",
    "            if i % 10 == 5:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss / 10:.3f} running loss:  {running_loss / 10:.3f}')\n",
    "                print(f'accuracy: {acc_val.mean() :.3f} ')\n",
    "            iters += 1\n",
    "        else:\n",
    "        \n",
    "            break\n",
    "    loss_values.append(running_loss)\n",
    "    \n",
    "    acc.append(acc_val.mean())\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114345db-148c-4758-b0cf-77ee42a180d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = './trainedModelTM_abs_ViT_128_V2.pth' \n",
    "\n",
    "torch.save(vision_transformer.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f127cd9a-34c9-4326-aff6-cc280bf7259e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"410.3325pt\" height=\"298.663868pt\" viewBox=\"0 0 410.3325 298.663868\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-02-22T16:15:20.255912</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 298.663868 \n",
       "L 410.3325 298.663868 \n",
       "L 410.3325 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 46.0125 274.785743 \n",
       "L 403.1325 274.785743 \n",
       "L 403.1325 8.673743 \n",
       "L 46.0125 8.673743 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m7d22163821\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"62.245227\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(59.063977 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"102.827045\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(99.645795 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"143.408864\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(140.227614 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"183.990682\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(180.809432 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"224.5725\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(221.39125 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"265.154318\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(261.973068 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"305.736136\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(302.554886 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"346.317955\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(343.136705 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7d22163821\" x=\"386.899773\" y=\"274.785743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(383.718523 289.38418) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m5be7c8ccbd\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"262.54581\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 46650 -->\n",
       "      <g transform=\"translate(7.2 266.345029) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"220.621378\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 46700 -->\n",
       "      <g transform=\"translate(7.2 224.420597) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"178.696946\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 46750 -->\n",
       "      <g transform=\"translate(7.2 182.496165) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"136.772514\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 46800 -->\n",
       "      <g transform=\"translate(7.2 140.571733) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"94.848082\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 46850 -->\n",
       "      <g transform=\"translate(7.2 98.647301) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"52.923651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 46900 -->\n",
       "      <g transform=\"translate(7.2 56.722869) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5be7c8ccbd\" x=\"46.0125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 46950 -->\n",
       "      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 62.245227 20.769743 \n",
       "L 102.827045 176.58944 \n",
       "L 143.408864 156.31228 \n",
       "L 183.990682 205.176627 \n",
       "L 224.5725 231.171073 \n",
       "L 265.154318 257.04382 \n",
       "L 305.736136 256.651719 \n",
       "L 346.317955 262.689743 \n",
       "L 386.899773 252.885435 \n",
       "\" clip-path=\"url(#pa6159538e9)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 46.0125 274.785743 \n",
       "L 46.0125 8.673743 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 403.1325 274.785743 \n",
       "L 403.1325 8.673743 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 46.0125 274.785743 \n",
       "L 403.1325 274.785743 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 46.0125 8.673743 \n",
       "L 403.1325 8.673743 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa6159538e9\">\n",
       "   <rect x=\"46.0125\" y=\"8.673743\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values)\n",
    "np.savetxt('loss_ABS_TM_128_V2.out', loss_values, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402f239f-43f0-4cd4-9b49-4379261d70f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1R5cGUgL0NhdGFsb2cgL1BhZ2VzIDIgMCBSID4+CmVuZG9iago4IDAgb2JqCjw8IC9Gb250IDMgMCBSIC9YT2JqZWN0IDcgMCBSIC9FeHRHU3RhdGUgNCAwIFIgL1BhdHRlcm4gNSAwIFIKL1NoYWRpbmcgNiAwIFIgL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gPj4KZW5kb2JqCjExIDAgb2JqCjw8IC9UeXBlIC9QYWdlIC9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUgovTWVkaWFCb3ggWyAwIDAgNDA3LjE0NSAyOTcuMTgzODc1IF0gL0NvbnRlbnRzIDkgMCBSIC9Bbm5vdHMgMTAgMCBSID4+CmVuZG9iago5IDAgb2JqCjw8IC9MZW5ndGggMTIgMCBSIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nI2WTW/bMAyG7/oVOm6HMRIpkdRx3UeA3toF2GHYqWu7Fc2ArsD690d7SSw1tuACruM3tJ5XEUVx8/H276+b2+vthf/wxW2mp5tnF/2DXfc++Ae7Xnz0W7vuXbCnvUtBIKZsnx9Pn7HYXUklmxjax5/O3bnNexvg2d7ZOpcQFO0dApU4xOwdlQIl1drjpKndj4Md3621AwD/A+7NqtkGNeOGGxSXC4QsgtJAKzUAHbHuwub+4p7sf/Dvgo2Ws72jgcoYWwBFo3m42buLndt8jj4Gv7sbf53dD/fNvwlv/Xe/u3Sfdu7KjU6czYWp5NROu1K7DgpDykWzjLErHMRzBzEFQIzEqbFQy10PkQRCiiw8Bq8wgTMm1O6BomJropL7JkSAkVJJY/AKE3RuwqYLpO1KnLQuHqMChkwDcgU7zbA5QmHWqC2+kvsOsoLYaxTG4BUm8rkJCgg5KRO3m7CS+yYsb4m1JB2DV5jgGROJwKaR8qtKUMldE0RWAnIQljF4hQmZMaEEEhGFWhOV3DdhiZvJFiWPwStM6LmJQ02jbCkgx3oI1EhzcAH0hBAl9moRhDBTjw5MZsvnljlJS0xGSDGUYsWopD4aZ7LvgB5qWjJOmtCTtIQuCJwHdCiqfXRennVEBtaWXWlLcEsIUCMX5WTFp0uX5YnHzFAsTyxfJvqkLdLzuDGMHotwlx47K26pDJFf0SdtkW47gsToIolLn95ZdIwCacgunOiVtkS38wl4SDnBwtSnd9Ydk50e6RV90hbpVpJ0yDrWlKVP76w72tmlQ4bFij5pi3RJEIasGzqF/k7Hdt2f/FmnRdlW2sa0TR7t/ufWf/W/PfpLHyEPDZN9a7mojJLNfeLDn9g3ErIqUsbir7e+7RirXqouY1N/U1eyquVo5KkJqOXjyXzaIqM4nZd1bHWCNeHVodKEV2X+uBBj2fvirvzT+LOEUy/ZTnimh51rTW2k89Z2P9/aWuzKzriKnAZYHvXK/QM5umIqCmVuZHN0cmVhbQplbmRvYmoKMTIgMCBvYmoKNzU0CmVuZG9iagoxMCAwIG9iagpbIF0KZW5kb2JqCjE3IDAgb2JqCjw8IC9MZW5ndGggMzk1IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nD1SS27FQAjb5xRcoNLwm895UlXdvPtva0NSqSq8iTHGMH3KkLnlS10ScYXJt16uWzymfC5bWpl5iLuLjSU+ttyX7iG2XXQusTgdR/ILMp0qRKjNqtGh+EKWhQeQTvChC8J9Of7jL4DB17ANuOE9MkGwJOYpQsZuURmaEkERYeeRFaikUJ9Zwt9R7uv3MgVqb4ylC2Mc9Am0BUJtSMQC6kAAROyUVK2QjmckE78V3WdiHGDn0bIBrhlURJZ77MeIqc6ojLxExD5PTfoolkwtVsZuUxlf/JSM1Hx0BSqpNPKU8tBVs9ALWIl5EvY5/Ej459ZsIYY6btbyieUfM8UyEs5gSzlgoZfjR+DbWXURrh25uM50gR+V1nBMtOt+yPVP/nTbWs11vHIIokDlTUHwuw6uRrHExDI+nY0peqIssBqavEYzwWEQEdb3w8gDGv1yvBA0p2sitFgim7ViRI2KbHM9vQTWTO/FOdbDE8Js753WobIzMyohgtq6hmrrQHazvvNwtp8/M+iibQplbmRzdHJlYW0KZW5kb2JqCjE4IDAgb2JqCjw8IC9MZW5ndGggMjQ5IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nE1RSYoDMAy75xX6QCFek7ynQ5lD5//Xyg6FOQQJr5KTlphYCw8xhB8sPfiRIXM3/Rt+otm7WXqSydn/mOciU1H4UqguYkJdiBvPoRHwPaFrElmxvfE5LKOZc74HH4W4BDOhAWN9STK5qOaVIRNODHUcDlqkwrhrYsPiWtE8jdxu+0ZmZSaEDY9kQtwYgIgg6wKyGCyUNjYTMlnOA+0NyQ1aYNepG1GLgiuU1gl0olbEqszgs+bWdjdDLfLgqH3x+mhWl2CF0Uv1WHhfhT6YqZl27pJCeuFNOyLMHgqkMjstK7V7xOpugfo/y1Lw/cn3+B2vD838XJwKZW5kc3RyZWFtCmVuZG9iagoxOSAwIG9iago8PCAvTGVuZ3RoIDk0IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nEWNwRHAIAgE/1RBCQoK2k8mk4f2/40QMnxg5w7uhAULtnlGHwWVJl4VWAdKY9xQj0C94XItydwFD3Anf9rQVJyW03dpkUlVKdykEnn/DmcmkKh50WOd9wtj+yM8CmVuZHN0cmVhbQplbmRvYmoKMjAgMCBvYmoKPDwgL0xlbmd0aCA4MyAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxFjLsNwDAIRHumYAR+JvY+UZTC3r8NECVuuCfdPVwdCZkpbjPDQwaeDCyGXXGB9JYwC1xHUI6d7KNh1b7qBI31plLz7w+Unuys4obrAQJCGmYKZW5kc3RyZWFtCmVuZG9iagoyMSAwIG9iago8PCAvTGVuZ3RoIDUxIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nDM2tFAwUDA0MAeSRoZAlpGJQoohF0gAxMzlggnmgFkGQBqiOAeuJocrgysNAOG0DZgKZW5kc3RyZWFtCmVuZG9iagoyMiAwIG9iago8PCAvTGVuZ3RoIDcwIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nDMzNlMwULAwAhKmpoYK5kaWCimGXEA+iJXLBRPLAbPMLMyBLCMLkJYcLkMLYzBtYmykYGZiBmRZIDEgujK40gCYmhMDCmVuZHN0cmVhbQplbmRvYmoKMjMgMCBvYmoKPDwgL0xlbmd0aCAzMjAgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVJLbgUxCNvPKbhApfBPzvOqqou++29rE70VTDBg4ykvWdJLvtQl26XD5Fsf9yWxQt6P7ZrMUsX3FrMUzy2vR88Rty0KBFETPViZLxUi1M/06DqocEqfgVcItxQbvINJAINq+AcepTMgUOdAxrtiMlIDgiTYc2lxCIlyJol/pLye3yetpKH0PVmZy9+TS6XQHU1O6AHFysVJoF1J+aCZmEpEkpfrfbFC9IbAkjw+RzHJgOw2iW2iBSbnHqUlzMQUOrDHArxmmtVV6GDCHocpjFcLs6gebPJbE5WkHa3jGdkw3sswU2Kh4bAF1OZiZYLu5eM1r8KI7VGTXcNw7pbNdwjRaP4bFsrgYxWSgEensRINaTjAiMCeXjjFXvMTOQ7AiGOdmiwMY2gmp3qOicDQnrOlYcbHHlr18w9U6XyHCmVuZHN0cmVhbQplbmRvYmoKMjQgMCBvYmoKPDwgL0xlbmd0aCAzNDAgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVI5bgQxDOv9Cn0ggG7b79kgSJH8vw2p2RQDcXRSlDtaVHbLh4VUtex0+bSV2hI35HdlhcQJyasS7VKGSKi8ViHV75kyr7c1ZwTIUqXC5KTkccmCP8OlpwvH+baxr+XIHY8eWBUjoUTAMsXE6BqWzu6wZlt+lmnAj3iEnCvWLcdYBVIb3TjtiveheS2yBoi9mZaKCh1WiRZ+QfGgR4199hhUWCDR7RxJcIyJUJGAdoHaSAw5eyx2UR/0MygxE+jaG0XcQYElkpg5xbp09N/40LGg/tiMN786KulbWllj0j4b7ZTGLDLpelj0dPPWx4MLNO+i/OfVDBI0ZY2Sxget2jmGoplRVni3Q5MNzTHHIfMOnsMZCUr6PBS/jyUTHZTI3w4NoX9fHqOMnDbeAuaiP20VBw7is8NeuYEVShdrkvcBqUzogen/r/G1vtfXHx3tgMYKZW5kc3RyZWFtCmVuZG9iagoyNSAwIG9iago8PCAvTGVuZ3RoIDI1MSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwtUUlyA0EIu88r9IRmp99jlyuH5P/XCMoHBg2LQHRa4qCMnyAsV7zlkatow98zMYLfBYd+K9dtWORAVCBJY1A1oXbxevQe2HGYCcyT1rAMZqwP/Iwp3OjF4TEZZ7fXZdQQ7F2vPZlByaxcxCUTF0zVYSNnDj+ZMi60cz03IOdGWJdhkG5WGjMSjjSFSCGFqpukzgRBEoyuRo02chT7pS+PdIZVjagx7HMtbV/PTThr0OxYrPLklB5dcS4nFy+sHPT1NgMXUWms8kBIwP1uD/VzspPfeEvnzhbT43vNyfLCVGDFm9duQDbV4t+8iOP7jK/n5/n8A19gW4gKZW5kc3RyZWFtCmVuZG9iagoyNiAwIG9iago8PCAvTGVuZ3RoIDIxNSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJw1UTkOAyEM7PcV/kAkjC94T6Iozf6/zYzRVh7BXIa0lCGZ8lKTqCHlUz56mS6cutzXzGo055a0LXOAuLa8L62SwIlmiIPBaZi4AZo8AUPX0ahRQxce0NSlUyiw3AQ+irduD91jtYGXtiHniSBiKBksQc2pRRMWbc8npDW/Xosb3pft3chTpcaWGIEGAVY4HNfo1/CVPU8m0XQVMtSrNcsYCRNFIjz5jqbVE+taNNIyEtTGEaxqA7w7/TBOAAATccsCZJ9KlLPkxG+x9LMGV/r+AZ9HVJYKZW5kc3RyZWFtCmVuZG9iagoxNSAwIG9iago8PCAvVHlwZSAvRm9udCAvQmFzZUZvbnQgL0JNUVFEVitEZWphVnVTYW5zIC9GaXJzdENoYXIgMCAvTGFzdENoYXIgMjU1Ci9Gb250RGVzY3JpcHRvciAxNCAwIFIgL1N1YnR5cGUgL1R5cGUzIC9OYW1lIC9CTVFRRFYrRGVqYVZ1U2FucwovRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250TWF0cml4IFsgMC4wMDEgMCAwIDAuMDAxIDAgMCBdCi9DaGFyUHJvY3MgMTYgMCBSCi9FbmNvZGluZyA8PCAvVHlwZSAvRW5jb2RpbmcKL0RpZmZlcmVuY2VzIFsgNDYgL3BlcmlvZCA0OCAvemVybyAvb25lIC90d28gL3RocmVlIC9mb3VyIC9maXZlIC9zaXggL3NldmVuIC9laWdodCBdCj4+Ci9XaWR0aHMgMTMgMCBSID4+CmVuZG9iagoxNCAwIG9iago8PCAvVHlwZSAvRm9udERlc2NyaXB0b3IgL0ZvbnROYW1lIC9CTVFRRFYrRGVqYVZ1U2FucyAvRmxhZ3MgMzIKL0ZvbnRCQm94IFsgLTEwMjEgLTQ2MyAxNzk0IDEyMzMgXSAvQXNjZW50IDkyOSAvRGVzY2VudCAtMjM2IC9DYXBIZWlnaHQgMAovWEhlaWdodCAwIC9JdGFsaWNBbmdsZSAwIC9TdGVtViAwIC9NYXhXaWR0aCAxMzQyID4+CmVuZG9iagoxMyAwIG9iagpbIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwCjYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgMzE4IDQwMSA0NjAgODM4IDYzNgo5NTAgNzgwIDI3NSAzOTAgMzkwIDUwMCA4MzggMzE4IDM2MSAzMTggMzM3IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYKNjM2IDYzNiAzMzcgMzM3IDgzOCA4MzggODM4IDUzMSAxMDAwIDY4NCA2ODYgNjk4IDc3MCA2MzIgNTc1IDc3NSA3NTIgMjk1CjI5NSA2NTYgNTU3IDg2MyA3NDggNzg3IDYwMyA3ODcgNjk1IDYzNSA2MTEgNzMyIDY4NCA5ODkgNjg1IDYxMSA2ODUgMzkwIDMzNwozOTAgODM4IDUwMCA1MDAgNjEzIDYzNSA1NTAgNjM1IDYxNSAzNTIgNjM1IDYzNCAyNzggMjc4IDU3OSAyNzggOTc0IDYzNCA2MTIKNjM1IDYzNSA0MTEgNTIxIDM5MiA2MzQgNTkyIDgxOCA1OTIgNTkyIDUyNSA2MzYgMzM3IDYzNiA4MzggNjAwIDYzNiA2MDAgMzE4CjM1MiA1MTggMTAwMCA1MDAgNTAwIDUwMCAxMzQyIDYzNSA0MDAgMTA3MCA2MDAgNjg1IDYwMCA2MDAgMzE4IDMxOCA1MTggNTE4CjU5MCA1MDAgMTAwMCA1MDAgMTAwMCA1MjEgNDAwIDEwMjMgNjAwIDUyNSA2MTEgMzE4IDQwMSA2MzYgNjM2IDYzNiA2MzYgMzM3CjUwMCA1MDAgMTAwMCA0NzEgNjEyIDgzOCAzNjEgMTAwMCA1MDAgNTAwIDgzOCA0MDEgNDAxIDUwMCA2MzYgNjM2IDMxOCA1MDAKNDAxIDQ3MSA2MTIgOTY5IDk2OSA5NjkgNTMxIDY4NCA2ODQgNjg0IDY4NCA2ODQgNjg0IDk3NCA2OTggNjMyIDYzMiA2MzIgNjMyCjI5NSAyOTUgMjk1IDI5NSA3NzUgNzQ4IDc4NyA3ODcgNzg3IDc4NyA3ODcgODM4IDc4NyA3MzIgNzMyIDczMiA3MzIgNjExIDYwNQo2MzAgNjEzIDYxMyA2MTMgNjEzIDYxMyA2MTMgOTgyIDU1MCA2MTUgNjE1IDYxNSA2MTUgMjc4IDI3OCAyNzggMjc4IDYxMiA2MzQKNjEyIDYxMiA2MTIgNjEyIDYxMiA4MzggNjEyIDYzNCA2MzQgNjM0IDYzNCA1OTIgNjM1IDU5MiBdCmVuZG9iagoxNiAwIG9iago8PCAvZWlnaHQgMTcgMCBSIC9maXZlIDE4IDAgUiAvZm91ciAxOSAwIFIgL29uZSAyMCAwIFIgL3BlcmlvZCAyMSAwIFIKL3NldmVuIDIyIDAgUiAvc2l4IDIzIDAgUiAvdGhyZWUgMjQgMCBSIC90d28gMjUgMCBSIC96ZXJvIDI2IDAgUiA+PgplbmRvYmoKMyAwIG9iago8PCAvRjEgMTUgMCBSID4+CmVuZG9iago0IDAgb2JqCjw8IC9BMSA8PCAvVHlwZSAvRXh0R1N0YXRlIC9DQSAwIC9jYSAxID4+Ci9BMiA8PCAvVHlwZSAvRXh0R1N0YXRlIC9DQSAxIC9jYSAxID4+ID4+CmVuZG9iago1IDAgb2JqCjw8ID4+CmVuZG9iago2IDAgb2JqCjw8ID4+CmVuZG9iago3IDAgb2JqCjw8ID4+CmVuZG9iagoyIDAgb2JqCjw8IC9UeXBlIC9QYWdlcyAvS2lkcyBbIDExIDAgUiBdIC9Db3VudCAxID4+CmVuZG9iagoyNyAwIG9iago8PCAvQ3JlYXRvciAoTWF0cGxvdGxpYiB2My43LjIsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcpCi9Qcm9kdWNlciAoTWF0cGxvdGxpYiBwZGYgYmFja2VuZCB2My43LjIpCi9DcmVhdGlvbkRhdGUgKEQ6MjAyNDAyMjIxNjE1MjAtMDUnMDAnKSA+PgplbmRvYmoKeHJlZgowIDI4CjAwMDAwMDAwMDAgNjU1MzUgZiAKMDAwMDAwMDAxNiAwMDAwMCBuIAowMDAwMDA1OTg3IDAwMDAwIG4gCjAwMDAwMDU3OTMgMDAwMDAgbiAKMDAwMDAwNTgyNSAwMDAwMCBuIAowMDAwMDA1OTI0IDAwMDAwIG4gCjAwMDAwMDU5NDUgMDAwMDAgbiAKMDAwMDAwNTk2NiAwMDAwMCBuIAowMDAwMDAwMDY1IDAwMDAwIG4gCjAwMDAwMDAzNDEgMDAwMDAgbiAKMDAwMDAwMTE5MCAwMDAwMCBuIAowMDAwMDAwMjA4IDAwMDAwIG4gCjAwMDAwMDExNzAgMDAwMDAgbiAKMDAwMDAwNDU4NiAwMDAwMCBuIAowMDAwMDA0Mzc5IDAwMDAwIG4gCjAwMDAwMDQwMDQgMDAwMDAgbiAKMDAwMDAwNTYzOSAwMDAwMCBuIAowMDAwMDAxMjEwIDAwMDAwIG4gCjAwMDAwMDE2NzggMDAwMDAgbiAKMDAwMDAwMjAwMCAwMDAwMCBuIAowMDAwMDAyMTY2IDAwMDAwIG4gCjAwMDAwMDIzMjEgMDAwMDAgbiAKMDAwMDAwMjQ0NCAwMDAwMCBuIAowMDAwMDAyNTg2IDAwMDAwIG4gCjAwMDAwMDI5NzkgMDAwMDAgbiAKMDAwMDAwMzM5MiAwMDAwMCBuIAowMDAwMDAzNzE2IDAwMDAwIG4gCjAwMDAwMDYwNDcgMDAwMDAgbiAKdHJhaWxlcgo8PCAvU2l6ZSAyOCAvUm9vdCAxIDAgUiAvSW5mbyAyNyAwIFIgPj4Kc3RhcnR4cmVmCjYyMDQKJSVFT0YK",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"407.148125pt\" height=\"297.190125pt\" viewBox=\"0 0 407.148125 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-02-22T16:15:20.750192</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 297.190125 \n",
       "L 407.148125 297.190125 \n",
       "L 407.148125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.828125 273.312 \n",
       "L 399.948125 273.312 \n",
       "L 399.948125 7.2 \n",
       "L 42.828125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"ma6cf106fc6\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"59.060852\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(55.879602 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"99.64267\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(96.46142 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"140.224489\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(137.043239 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"180.806307\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(177.625057 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"221.388125\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(218.206875 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"261.969943\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(258.788693 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"302.551761\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(299.370511 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"343.13358\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(339.95233 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma6cf106fc6\" x=\"383.715398\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(380.534148 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"mf98c455f48\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"261.216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.000 -->\n",
       "      <g transform=\"translate(7.2 265.015219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"230.976\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.025 -->\n",
       "      <g transform=\"translate(7.2 234.775219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"200.736001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.050 -->\n",
       "      <g transform=\"translate(7.2 204.53522) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"170.496001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.075 -->\n",
       "      <g transform=\"translate(7.2 174.29522) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"140.256002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.100 -->\n",
       "      <g transform=\"translate(7.2 144.055221) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"110.016002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.125 -->\n",
       "      <g transform=\"translate(7.2 113.815221) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"79.776003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.150 -->\n",
       "      <g transform=\"translate(7.2 83.575221) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"49.536003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.175 -->\n",
       "      <g transform=\"translate(7.2 53.335222) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf98c455f48\" x=\"42.828125\" y=\"19.296004\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0.200 -->\n",
       "      <g transform=\"translate(7.2 23.095222) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 59.060852 261.216 \n",
       "L 99.64267 261.216 \n",
       "L 140.224489 261.216 \n",
       "L 180.806307 261.216 \n",
       "L 221.388125 140.256 \n",
       "L 261.969943 261.216 \n",
       "L 302.551761 140.256 \n",
       "L 343.13358 261.216 \n",
       "L 383.715398 19.296 \n",
       "\" clip-path=\"url(#pdb0136967a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.828125 273.312 \n",
       "L 42.828125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 399.948125 273.312 \n",
       "L 399.948125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.828125 273.312 \n",
       "L 399.948125 273.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.828125 7.2 \n",
       "L 399.948125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdb0136967a\">\n",
       "   <rect x=\"42.828125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc)\n",
    "np.savetxt('acc_TM_128_V2.out', acc, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f9a563-c3cf-4432-9ec4-dcc264ddbbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "number sections must be larger than 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m chunkSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m601\u001b[39m\n\u001b[0;32m     11\u001b[0m division\u001b[38;5;241m=\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mchunkSize\n\u001b[1;32m---> 12\u001b[0m acc_split\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msplit(acc,division\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m acc_mean\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(acc_split, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(acc_mean)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI\\Lib\\site-packages\\numpy\\lib\\shape_base.py:874\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m%\u001b[39m sections:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    873\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray split does not result in an equal division\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_split(ary, indices_or_sections, axis)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36marray_split\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI\\Lib\\site-packages\\numpy\\lib\\shape_base.py:778\u001b[0m, in \u001b[0;36marray_split\u001b[1;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[0;32m    776\u001b[0m Nsections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(indices_or_sections)\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Nsections \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber sections must be larger than 0.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    779\u001b[0m Neach_section, extras \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(Ntotal, Nsections)\n\u001b[0;32m    780\u001b[0m section_sizes \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    781\u001b[0m                  extras \u001b[38;5;241m*\u001b[39m [Neach_section\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    782\u001b[0m                  (Nsections\u001b[38;5;241m-\u001b[39mextras) \u001b[38;5;241m*\u001b[39m [Neach_section])\n",
      "\u001b[1;31mValueError\u001b[0m: number sections must be larger than 0."
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "with open('acc_TM_128_V2.out', 'r') as arch:\n",
    "    for line in arch:\n",
    "        vectors.append(eval(line.rstrip()))\n",
    "        \n",
    "acc=np.array(vectors).T\n",
    "size= len(acc)\n",
    "print(size)\n",
    "chunkSize=601\n",
    "\n",
    "division=size//chunkSize\n",
    "acc_split=np.split(acc,division-1)\n",
    "\n",
    "acc_mean=np.mean(acc_split, axis=1)\n",
    "\n",
    "plt.plot(acc_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45890f7-a862-45b6-9457-a0389a0b2d68",
   "metadata": {},
   "source": [
    "## References\n",
    "https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/11-vision-transformer.ipynb#scrollTo=d8000482\n",
    "https://github.com/nerminnuraydogan/vision-transformer/blob/main/vision-transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa389f-0786-45ce-bc82-7dff33b32a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
