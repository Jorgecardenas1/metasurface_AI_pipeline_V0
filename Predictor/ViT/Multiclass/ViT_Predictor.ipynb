{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c43fc8-9128-4c0f-a90c-07e4a55fe916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys  \n",
    "sys.path.insert(0, r\"C:\\\\Users\\jorge\\\\Documents\\\\Projects Jorge C\\\\DRUIDA PROJECT\\\\POC\\\\druida_V01\\\\src\\\\\")\n",
    "\n",
    "import os\n",
    "\n",
    "from __future__ import print_function\n",
    "#from Utilities.SaveAnimation import Video\n",
    "\n",
    "\n",
    "\n",
    "from druida import Stack\n",
    "from druida import setup\n",
    "\n",
    "from druida.DataManager import datamanager\n",
    "from druidaHFSS.modules import tools\n",
    "from druida.tools import utils\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.offsetbox import OffsetImage,AnchoredOffsetbox\n",
    "import matplotlib.image as image\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38853f24-6bad-4ca5-b791-c6f47f1bdf14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"run_name\",type=str)\n",
    "parser.add_argument(\"epochs\",type=int)\n",
    "parser.add_argument(\"batch_size\",type=int)\n",
    "parser.add_argument(\"workers\",type=int)\n",
    "parser.add_argument(\"gpu_number\",type=int)\n",
    "parser.add_argument(\"device\",type=str)\n",
    "parser.add_argument(\"learning_rate\",type=float)\n",
    "parser.add_argument(\"condition_len\",type=float) #This defines the length of our conditioning vector\n",
    "parser.add_argument(\"metricType\",type=float) #This defines the length of our conditioning vector\n",
    "parser.add_argument(\"patch_size\",type=int)\n",
    "\n",
    "parser.run_name = \"Predictor Training\"\n",
    "parser.epochs = 1\n",
    "parser.batch_size = 10\n",
    "parser.workers=0\n",
    "parser.gpu_number=0\n",
    "parser.image_size = 128\n",
    "parser.dataset_path = os.path.normpath('/content/drive/MyDrive/Training_Data/Training_lite/')\n",
    "parser.device = \"cpu\"\n",
    "parser.learning_rate = 1e-4\n",
    "parser.condition_len = 10\n",
    "parser.metricType='AbsorbanceTM' #this is to be modified when training for different metrics.\n",
    "parser.patch_size=16\n",
    "\n",
    "metricType=['AbsorbanceTM','AbsorbanceTE' ]\n",
    "\n",
    "categories=[\"box\", \"circle\", \"cross\"]\n",
    "\n",
    "model_kwargs={\n",
    "        \"batch_size\":parser.batch_size,\n",
    "        \"embed_dim\":  3 * (parser.patch_size)**2 ,\n",
    "        \"hidden_dim\":  2*(3 * (parser.patch_size)**2),\n",
    "        \"num_heads\": 16,\n",
    "        \"num_layers\": 8,\n",
    "        \"patch_size\": parser.patch_size,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_patches\": (parser.image_size//parser.patch_size)**2,\n",
    "        \"num_classes\": 601,\n",
    "        \"dropout\": 0.1,\n",
    "        \"image_size\":parser.image_size,\n",
    "        \"conditionalIn\":True,\n",
    "        \"conditionalLen\":10\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801d078-aaea-4170-b40b-d841e78b6e5e",
   "metadata": {},
   "source": [
    "### 1. Image loading : modify all paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9348da90-5629-46ed-85a0-3f1e2430c388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TM = './trainedModelTM_abs_ViT_128_V2.pth'\n",
    "PATH_TE = './trainedModelTM_abs_ViT_128_V2.pth'\n",
    "\n",
    "boxImagesPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\testImages128\\\\\"\n",
    "DataPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Exports\\\\output\\\\\"\n",
    "simulationData=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\DBfiles\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d663ca8-b396-4d5a-b974-6a08d7dcec26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataloader_test = utils.get_data_with_labels(parser.image_size, parser.image_size,1.0, boxImagesPath,parser.batch_size,drop_last=True)\n",
    "\n",
    "print(len(dataloader_test))\n",
    "#Visualizing one item\n",
    "\n",
    "#in this case we can have access to every image name match with \n",
    "#other kind of data \n",
    "#data,target,path = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce34794e-c4de-4821-b7a8-83c9dcb66073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# init vision transformer model\n",
    "net_TM = Stack.VisionTransformer(**model_kwargs )\n",
    "\n",
    "net_TM.eval()\n",
    "net_TM.load_state_dict(torch.load(PATH_TM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ce4f9f-b253-4e4d-821a-c2d46c63f1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# init vision transformer model\n",
    "net_TE = Stack.VisionTransformer(**model_kwargs )\n",
    "\n",
    "net_TE.eval()\n",
    "net_TE.load_state_dict(torch.load(PATH_TE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6eba3a6-45e5-4d56-bdc1-ccfdf054c6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "Substrates={\"Rogers RT/duroid 5880 (tm)\":0}\n",
    "Materials={\"copper\":0,\"pec\":1}\n",
    "Surfacetypes={\"Reflective\":0,\"Transmissive\":1}\n",
    "TargetGeometries={\"circ\":0,\"box\":1, \"cross\":2}\n",
    "           \n",
    "def set_conditioning(target,path,categories):\n",
    "    df = pd.read_csv(\"out.csv\")\n",
    "    arr=[]\n",
    "\n",
    "    for idx,name in enumerate(path):\n",
    "        series=name.split('_')[-1].split('.')[0]\n",
    "        batch=name.split('_')[4]\n",
    "        iteration=series.split('-')[-1]\n",
    "        row=df[(df['sim_id']==batch) & (df['iteration']==int(iteration))  ]\n",
    "        #print(row)\n",
    "        \n",
    "        target_val=target[idx]\n",
    "        category=categories[idx]\n",
    "        geometry=TargetGeometries[category]\n",
    "        \n",
    "        \"\"\"\"\n",
    "        surface type: reflective, transmissive\n",
    "        layers: conductor and conductor material / Substrate information\n",
    "        \"\"\"\n",
    "        surfacetype=row[\"type\"].values[0]\n",
    "        surfacetype=Surfacetypes[surfacetype]\n",
    "        \n",
    "        layers=row[\"layers\"].values[0]\n",
    "        layers= layers.replace(\"'\", '\"')\n",
    "        layer=json.loads(layers)\n",
    "        \n",
    "        materialconductor=Materials[layer['conductor']['material']]\n",
    "        materialsustrato=Substrates[layer['substrate']['material']]\n",
    "        \n",
    "        \n",
    "        if (target_val==2): #is cross. Because an added variable to the desing \n",
    "            \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-2]\n",
    "        else:\n",
    "        \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-1]\n",
    "        \n",
    "        arr.append([geometry,surfacetype,materialconductor,materialsustrato,sustratoHeight,1,1,1,1,1])\n",
    "    \n",
    "    return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e48400-626a-473c-9d45-61d2a861fdcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output_test_TM=[]\n",
    "output_test_TE=[]\n",
    "\n",
    "for item in iter(dataloader_test):\n",
    "    inputs,target,path, categories = item\n",
    "    train=[]\n",
    "    counter=0\n",
    "\n",
    "    #figs\n",
    "    fig, axs = plt.subplots(len(inputs),3,figsize=(15,18))\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    #loop over test images names\n",
    "    for name in path:\n",
    "        series=name.split('_')[-1].split('.')[0]\n",
    "        batch=name.split('_')[4]\n",
    "        metricData=[]\n",
    "        #loop over metric type -TM or TE\n",
    "        for metric in metricType:\n",
    "            \n",
    "            #selecting data files depending on metric\n",
    "            for name in glob(DataPath+batch+'\\\\files\\\\'+'/'+metric+'*'+series+'.csv'): \n",
    "                #loading the absorption data\n",
    "                metricData.append(pd.read_csv(name))\n",
    "        \n",
    "        #every image data in array\n",
    "        train.append(metricData)\n",
    "        \n",
    "\n",
    "    conditioningArray=torch.FloatTensor(set_conditioning(target, path, categories))\n",
    "    \n",
    "    #Normalization\n",
    "    #outmap_min, _ = torch.min(conditioningArray, dim=1, keepdim=True)\n",
    "    #outmap_max, _ = torch.max(conditioningArray, dim=1, keepdim=True)\n",
    "    #conditioningTensor = (conditioningArray - outmap_min) / (outmap_max - outmap_min)\n",
    "    \n",
    "    #batch size\n",
    "    output_test_TM=net_TM(x=inputs,condition=conditioningArray)\n",
    "    output_test_TM= (F.softmax (output_test_TM,dim=1))\n",
    "    \n",
    "    output_test_TE=net_TE(x=inputs,condition=conditioningArray)\n",
    "    #output_test_TE= (F.softmax(output_test_TE,dim=1))\n",
    "\n",
    "    output_to_plot_TM=output_test_TM.cpu().detach().numpy()\n",
    "    output_to_plot_TE=output_test_TE.cpu().detach().numpy()\n",
    "\n",
    "    images_to_plot = inputs.detach().cpu().permute(0, 2, 3,1).numpy()\n",
    "    \n",
    "    cols=['Atom','TM mode', 'TE Mode']\n",
    "    for ax, col in zip(axs[0], cols):\n",
    "        ax.set_title(col)\n",
    "\n",
    "    \n",
    "    #looop over every image data\n",
    "    #[[freq TM][Freq TE]]\n",
    "    for i,data in enumerate(train):\n",
    "        \n",
    "        x=np.array(data[0].values.T)[0] #frequency values\n",
    "        \n",
    "        #predicted TM and TE values \n",
    "        toPlot_TM=output_to_plot_TM[i]\n",
    "        toPlot_TE=output_to_plot_TE[i]\n",
    "        \n",
    "        #toPlot_TM=np.exp(toPlot_TM)/sum(np.exp(toPlot_TM))\n",
    "        toPlot_TM=(toPlot_TM-toPlot_TM.min())/(toPlot_TM.max()-toPlot_TM.min())\n",
    "        #toPlot_TE=(toPlot_TE-toPlot_TE.min())/(toPlot_TE.max()-toPlot_TE.min())\n",
    "        \n",
    "        trueTM=np.array(data[0].values.T)[1]\n",
    "        trueTE=np.array(data[1].values.T)[1]\n",
    "        \n",
    "        \n",
    "        #scaler = MinMaxScaler(feature_range=(0, trueTM.max()))\n",
    "        #toPlot_TM = scaler.fit_transform(toPlot_TM.reshape(-1, 1)).flatten()\n",
    "        #indices = toPlot_TM < 0.1\n",
    "        #toPlot_TM[indices]=0\n",
    "\n",
    "        #scaler = MinMaxScaler(feature_range=(0, trueTE.max()))\n",
    "        #toPlot_TE = scaler.fit_transform(toPlot_TE.reshape(-1, 1)).flatten()\n",
    "        #indices = toPlot_TE < 0.001\n",
    "        #toPlot_TE[indices]=0\n",
    "        \n",
    "        images = (images_to_plot[i] * 255).round().astype(\"uint8\")\n",
    "        pil_images = images\n",
    "\n",
    "        axs[counter][0].imshow(pil_images)\n",
    "        axs[counter][0].set_axis_off()\n",
    "        \n",
    "        axs[counter][1].plot(x,trueTM,label=\"Simulated\",linewidth=3)\n",
    "        axs[counter][1].plot(x,toPlot_TM,'r-',label=\"Predicted\",linewidth=1)\n",
    "        axs[counter][1].legend(loc=\"upper left\")\n",
    "        axs[counter, 1].set(xlabel='Freq', ylabel='Abs')\n",
    "        #axs[counter, 1].set_ylim([0, 1])\n",
    "\n",
    "        axs[counter][2].plot(x,trueTE,label=\"Simulated\",linewidth=2)\n",
    "        axs[counter][2].plot(x,toPlot_TE,'r-',label=\"Predicted\",linewidth=1)\n",
    "        axs[counter][2].legend(loc=\"upper left\")\n",
    "        axs[counter, 2].set(xlabel='Freq', ylabel='Abs')\n",
    "        axs[counter, 2].set_ylim([0, 1])\n",
    "        \n",
    "       \n",
    "        imagebox = OffsetImage(images, zoom=1*0.08)\n",
    "        imagebox2 = OffsetImage(images, zoom=1*0.08)\n",
    "\n",
    "        ab = AnchoredOffsetbox(loc=1, child=imagebox, frameon=False)\n",
    "        ab2 = AnchoredOffsetbox(loc=1, child=imagebox2, frameon=False)\n",
    "\n",
    "        axs[counter][1].add_artist(ab)\n",
    "        axs[counter][2].add_artist(ab2)\n",
    "        \n",
    "        counter=counter+1\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f059d3d-ad53-4e04-96a7-86e0e87f93ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ac92c-eaf9-4972-8bbb-c6b8c8b54f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
