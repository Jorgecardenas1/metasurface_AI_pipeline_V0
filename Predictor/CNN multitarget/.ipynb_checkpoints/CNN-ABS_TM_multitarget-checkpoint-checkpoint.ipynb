{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baa8720-2b4b-42d4-ad9b-9c850cbeea6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys  \n",
    "sys.path.insert(0, r\"C:\\\\Users\\jorge\\\\Documents\\\\Projects Jorge C\\\\DRUIDA PROJECT\\\\POC\\\\druida_V01\\\\src\\\\\")\n",
    "\n",
    "import os\n",
    "\n",
    "from __future__ import print_function\n",
    "#from Utilities.SaveAnimation import Video\n",
    "\n",
    "\n",
    "\n",
    "from druida import Stack\n",
    "from druida import setup\n",
    "\n",
    "from druida.DataManager import datamanager\n",
    "from druidaHFSS.modules import tools\n",
    "from druida.tools import utils\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81271b6e-4895-4900-ac21-3afe6acfedd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x259c7a33ab0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "torch.manual_seed(999)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30767414-f3be-410c-9ecb-46075ce10cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"run_name\",type=str)\n",
    "parser.add_argument(\"epochs\",type=int)\n",
    "parser.add_argument(\"batch_size\",type=int)\n",
    "parser.add_argument(\"workers\",type=int)\n",
    "parser.add_argument(\"gpu_number\",type=int)\n",
    "parser.add_argument(\"device\",type=str)\n",
    "parser.add_argument(\"learning_rate\",type=float)\n",
    "parser.add_argument(\"condition_len\",type=float) #This defines the length of our conditioning vector\n",
    "parser.add_argument(\"metricType\",type=float) #This defines the length of our conditioning vector\n",
    "\n",
    "parser.run_name = \"Predictor Training\"\n",
    "parser.epochs = 15\n",
    "parser.batch_size = 5\n",
    "parser.workers=0\n",
    "parser.gpu_number=0\n",
    "parser.image_size = 512\n",
    "parser.dataset_path = os.path.normpath('/content/drive/MyDrive/Training_Data/Training_lite/')\n",
    "parser.device = \"cpu\"\n",
    "parser.learning_rate = 5e-6\n",
    "parser.condition_len = 10\n",
    "parser.metricType='AbsorbanceTM' #this is to be modified when training for different metrics.\n",
    "\n",
    "categories=[\"box\", \"circle\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd27aba-6330-48f0-bc70-e5480645fab1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c97f58-1dd0-413d-8878-ca0d0a6ebb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagesPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40ed4e3-4b8a-4c70-8d07-d5be55b63535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\09a2f2c2-ad7f-11ee-bb2a-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\0ed3a0e8-a653-11ee-9db6-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\55628454-b316-11ee-82a4-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\56dee422-b317-11ee-bb58-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\7262ded2-a81d-11ee-8e0e-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\b6b02fee-a8bb-11ee-8e20-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\c49e201f-ae96-11ee-88f3-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\d31e1e9f-a766-11ee-8b8e-047c16a08772\\\\', 'C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images\\\\e3744294-aba9-11ee-b4d0-047c16a08772\\\\']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folders=glob(imagesPath+\"/*/\", recursive = True)\n",
    "files=[]\n",
    "\n",
    "print(folders)\n",
    "for folder in folders:\n",
    "    \n",
    "    if folder != imagesPath+\"\\\\\"+ \"processed\\\\\":\n",
    "        files=(files+glob(folder+\"/*\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd9221-451d-45b4-a896-f10278802029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in files:\n",
    "    fileName_absolute = os.path.basename(file) \n",
    "    path=os.path.dirname(file)\n",
    "\n",
    "    #ROI is \n",
    "    image_rgb=tools.cropImage( file,image_path=path,\n",
    "                              image_name=fileName_absolute,\n",
    "                              output_path=imagesPath, \n",
    "                             resize_dim=(512,512))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73520e56-2b92-4458-b376-70379abf3a05",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Load Images\n",
    "<p style=\"font-size: 16px; color: blue;\">Here we create a custom dataset which provides extra information of each image and batch</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24bf01d2-7df8-4cbf-a546-0cbb0ba45207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boxImagesPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Images Jorge Cardenas 512\\\\\"\n",
    "DataPath=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\Exports\\\\output\\\\\"\n",
    "simulationData=\"C:\\\\Users\\\\jorge\\\\Dropbox\\\\Public\\\\MetasufacesData\\\\DBfiles\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73f9625-1f11-483e-81ff-b95522783d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = utils.get_data_with_labels(512, 512,0.95, boxImagesPath,parser.batch_size, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e35e40-541e-42f1-bc9c-976b96719114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('box', 'box', 'circ', 'box', 'box')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Visualizing a sample\n",
    "data,target,path,categories = next(iter(dataloader))\n",
    "print(categories)\n",
    "image = data.detach().cpu().permute(0, 2, 3,1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e0749c-0fec-4da2-ab48-5449ef1dc544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(images[0])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "759a23b0-0027-43a2-baae-bfbf9a027e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Stack.Trainer(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e10998e-1605-4dd2-8a1c-0a7f6d1597de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7b6201-762b-40a4-925f-a084011ef2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a conditioning shape\n",
    "\"\"\"this conditioning must be built during training\"\"\"\n",
    "conditioning=torch.ones(1,parser.batch_size,parser.condition_len)\n",
    "conditioning.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf149b-914d-4694-96e0-00c7f662d62b",
   "metadata": {},
   "source": [
    "## Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da05eb0e-86ef-46c5-b8b7-2f519cfc4eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictor_CNN(\n",
       "  (l1): Linear(in_features=10, out_features=786432, bias=False)\n",
       "  (conv1): Conv2d(6, 8, kernel_size=(6, 6), stride=(2, 2), padding=(4, 4), bias=False)\n",
       "  (conv2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv3): Conv2d(8, 16, kernel_size=(6, 6), stride=(2, 2), padding=(5, 5), bias=False)\n",
       "  (conv4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv6): Conv2d(16, 32, kernel_size=(6, 6), stride=(2, 2), padding=(4, 4), bias=False)\n",
       "  (conv7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv9): Conv2d(32, 64, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (conv10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (conv12): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "  (l2): Linear(in_features=139392, out_features=3000, bias=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (l4): Linear(in_features=3000, out_features=601, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "fwd_test = Stack.Predictor_CNN(cond_input_size=parser.condition_len, \n",
    "                               ngpu=0, image_size=parser.image_size ,\n",
    "                               output_size=8, channels=3,\n",
    "                               features_num=3000,\n",
    "                               dropout=0.2, \n",
    "                               Y_prediction_size=601) #size of the output vector in this case frenquency points\n",
    "\n",
    "fwd_test.apply(weights_init)\n",
    "\n",
    "\n",
    "\"\"\"using weigth decay regularization\"\"\"\n",
    "opt = optimizer.Adam(fwd_test.parameters(), lr=parser.learning_rate, betas=(0.5, 0.999),weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "fwd_test.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde542c9-f766-4538-8625-90c2c6fb8c4c",
   "metadata": {},
   "source": [
    "## Join simulation data\n",
    "<p style=\"font-size: 16px; color: blue;\">We need to have access to other additional simulation data which could be helpful to build our conditioning.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cda5e1a-7844-4fe6-a78b-c3d81178c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_simulationData():\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in glob.glob(simulationData+\"*.csv\"): \n",
    "        df2 = pd.read_csv(file)\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    \n",
    "    df.to_csv('out.csv',index=False)\n",
    "    \n",
    "join_simulationData()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228e2413-3b68-4de3-aacd-a0443efbfdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0.252, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 0.787, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0.252, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 0.252, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 0.508, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "Substrates={\"Rogers RT/duroid 5880 (tm)\":0}\n",
    "Materials={\"copper\":0,\"pec\":1}\n",
    "Surfacetypes={\"Reflective\":0,\"Transmissive\":1}\n",
    "TargetGeometries={\"circ\":0,\"box\":1, \"cross\":2}\n",
    "           \n",
    "def set_conditioning(target,path,categories):\n",
    "    df = pd.read_csv(\"out.csv\")\n",
    "    arr=[]\n",
    "\n",
    "    for idx,name in enumerate(path):\n",
    "        series=name.split('_')[-1].split('.')[0]\n",
    "        batch=name.split('_')[4]\n",
    "        iteration=series.split('-')[-1]\n",
    "        row=df[(df['sim_id']==batch) & (df['iteration']==int(iteration))  ]\n",
    "\n",
    "        \n",
    "        target_val=target[idx]\n",
    "        category=categories[idx]\n",
    "        geometry=TargetGeometries[category]\n",
    "        \n",
    "        \"\"\"\"\n",
    "        surface type: reflective, transmissive\n",
    "        layers: conductor and conductor material / Substrate information\n",
    "        \"\"\"\n",
    "        surfacetype=row[\"type\"].values[0]\n",
    "        surfacetype=Surfacetypes[surfacetype]\n",
    "        \n",
    "        layers=row[\"layers\"].values[0]\n",
    "        layers= layers.replace(\"'\", '\"')\n",
    "        layer=json.loads(layers)\n",
    "        \n",
    "        materialconductor=Materials[layer['conductor']['material']]\n",
    "        materialsustrato=Substrates[layer['substrate']['material']]\n",
    "        \n",
    "        \n",
    "        if (target_val==2): #is cross. Because an added variable to the desing \n",
    "            \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-2]\n",
    "        else:\n",
    "        \n",
    "            sustratoHeight= json.loads(row[\"paramValues\"].values[0])\n",
    "            sustratoHeight= sustratoHeight[-1]\n",
    "        \n",
    "        arr.append([geometry,surfacetype,materialconductor,materialsustrato,sustratoHeight,1,1,1,1,1])\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "conditions=set_conditioning(target, path, categories)\n",
    "conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ae8b0c-80f1-4a17-82c6-d17bde45806d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6385ee3aef4c0fadf4f52f7814fad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     6] loss: 6.646 running loss:  123.276\n",
      "[1,    16] loss: 6.560 running loss:  408.219\n",
      "[1,    26] loss: 4.908 running loss:  712.916\n",
      "[1,    36] loss: 6.005 running loss:  1064.527\n",
      "[1,    46] loss: 5.063 running loss:  1382.399\n",
      "[1,    56] loss: 6.466 running loss:  1685.750\n",
      "[1,    66] loss: 5.160 running loss:  1978.411\n",
      "[1,    76] loss: 4.776 running loss:  2295.985\n",
      "[1,    86] loss: 5.739 running loss:  2593.976\n",
      "[1,    96] loss: 4.570 running loss:  2914.102\n",
      "[1,   106] loss: 6.794 running loss:  3238.493\n",
      "[1,   116] loss: 6.880 running loss:  3524.250\n",
      "[1,   126] loss: 6.669 running loss:  3848.382\n",
      "[1,   136] loss: 6.115 running loss:  4082.373\n",
      "[1,   146] loss: 7.633 running loss:  4471.940\n",
      "[1,   156] loss: 7.054 running loss:  4772.954\n",
      "[1,   166] loss: 4.531 running loss:  5026.223\n",
      "[1,   176] loss: 6.976 running loss:  5335.532\n",
      "[1,   186] loss: 5.713 running loss:  5616.997\n",
      "[1,   196] loss: 4.053 running loss:  5971.290\n",
      "[1,   206] loss: 7.962 running loss:  6284.580\n",
      "[1,   216] loss: 4.177 running loss:  6606.485\n",
      "[1,   226] loss: 3.883 running loss:  6899.766\n",
      "[1,   236] loss: 4.900 running loss:  7155.076\n",
      "[1,   246] loss: 3.177 running loss:  7420.830\n",
      "[1,   256] loss: 6.705 running loss:  7715.964\n",
      "[1,   266] loss: 7.543 running loss:  8038.701\n",
      "[1,   276] loss: 6.066 running loss:  8327.848\n",
      "[1,   286] loss: 5.762 running loss:  8600.096\n",
      "[1,   296] loss: 5.488 running loss:  8900.149\n",
      "[1,   306] loss: 4.506 running loss:  9179.720\n",
      "[1,   316] loss: 5.782 running loss:  9540.121\n",
      "[1,   326] loss: 5.854 running loss:  9881.693\n",
      "[1,   336] loss: 6.278 running loss:  10157.859\n",
      "[1,   346] loss: 4.458 running loss:  10453.170\n",
      "[1,   356] loss: 4.976 running loss:  10760.198\n",
      "[1,   366] loss: 4.765 running loss:  11027.245\n",
      "[1,   376] loss: 6.426 running loss:  11335.674\n",
      "[1,   386] loss: 5.480 running loss:  11675.661\n",
      "[1,   396] loss: 6.435 running loss:  12027.378\n",
      "[1,   406] loss: 4.853 running loss:  12374.400\n",
      "[1,   416] loss: 5.428 running loss:  12700.366\n",
      "[1,   426] loss: 5.655 running loss:  13013.982\n",
      "[1,   436] loss: 9.588 running loss:  13354.649\n",
      "[1,   446] loss: 5.547 running loss:  13693.109\n",
      "[1,   456] loss: 5.157 running loss:  14007.261\n",
      "[1,   466] loss: 5.209 running loss:  14312.657\n",
      "[1,   476] loss: 5.099 running loss:  14632.445\n",
      "[1,   486] loss: 4.665 running loss:  14926.399\n",
      "[1,   496] loss: 6.465 running loss:  15206.900\n",
      "[1,   506] loss: 4.592 running loss:  15538.519\n",
      "[1,   516] loss: 5.887 running loss:  15834.016\n",
      "[1,   526] loss: 4.006 running loss:  16106.159\n",
      "[1,   536] loss: 4.582 running loss:  16346.146\n",
      "[1,   546] loss: 4.378 running loss:  16623.361\n",
      "[1,   556] loss: 8.086 running loss:  16946.034\n",
      "[1,   566] loss: 6.058 running loss:  17243.352\n",
      "[1,   576] loss: 3.872 running loss:  17543.864\n",
      "[1,   586] loss: 4.883 running loss:  17833.520\n",
      "[1,   596] loss: 6.987 running loss:  18129.034\n",
      "[1,   606] loss: 3.939 running loss:  18408.590\n",
      "[1,   616] loss: 9.620 running loss:  18718.511\n",
      "[1,   626] loss: 8.093 running loss:  19020.883\n",
      "[1,   636] loss: 5.189 running loss:  19319.270\n",
      "[1,   646] loss: 4.742 running loss:  19616.051\n",
      "[1,   656] loss: 4.623 running loss:  19892.032\n",
      "[1,   666] loss: 9.282 running loss:  20220.658\n",
      "[1,   676] loss: 7.494 running loss:  20503.400\n",
      "[1,   686] loss: 7.621 running loss:  20866.963\n",
      "[1,   696] loss: 5.501 running loss:  21189.202\n",
      "[1,   706] loss: 9.929 running loss:  21499.761\n",
      "[1,   716] loss: 8.457 running loss:  21818.726\n",
      "[1,   726] loss: 6.883 running loss:  22141.874\n",
      "[1,   736] loss: 9.004 running loss:  22505.188\n",
      "[1,   746] loss: 5.805 running loss:  22829.769\n",
      "[1,   756] loss: 8.365 running loss:  23132.587\n",
      "[1,   766] loss: 4.222 running loss:  23452.417\n",
      "[1,   776] loss: 6.436 running loss:  23771.058\n",
      "[1,   786] loss: 8.425 running loss:  24086.760\n",
      "[1,   796] loss: 6.514 running loss:  24379.262\n",
      "[1,   806] loss: 6.948 running loss:  24679.226\n",
      "[1,   816] loss: 4.620 running loss:  24992.778\n",
      "[1,   826] loss: 5.263 running loss:  25305.143\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54d5413489d4281adb9834a35c2ff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     6] loss: 7.784 running loss:  180.393\n",
      "[2,    16] loss: 7.357 running loss:  494.429\n",
      "[2,    26] loss: 5.727 running loss:  787.158\n",
      "[2,    36] loss: 6.342 running loss:  1076.702\n",
      "[2,    46] loss: 5.880 running loss:  1346.198\n",
      "[2,    56] loss: 4.826 running loss:  1630.099\n",
      "[2,    66] loss: 5.439 running loss:  1911.757\n",
      "[2,    76] loss: 9.044 running loss:  2236.404\n",
      "[2,    86] loss: 6.486 running loss:  2551.497\n",
      "[2,    96] loss: 7.128 running loss:  2839.711\n",
      "[2,   106] loss: 7.592 running loss:  3152.131\n",
      "[2,   116] loss: 5.695 running loss:  3474.170\n",
      "[2,   126] loss: 4.831 running loss:  3769.604\n",
      "[2,   136] loss: 6.325 running loss:  4083.975\n",
      "[2,   146] loss: 7.243 running loss:  4356.015\n",
      "[2,   156] loss: 6.365 running loss:  4703.156\n",
      "[2,   166] loss: 10.067 running loss:  5034.952\n",
      "[2,   176] loss: 4.425 running loss:  5301.870\n",
      "[2,   186] loss: 6.447 running loss:  5644.194\n",
      "[2,   196] loss: 7.143 running loss:  5898.334\n",
      "[2,   206] loss: 7.311 running loss:  6202.587\n",
      "[2,   216] loss: 7.750 running loss:  6511.356\n",
      "[2,   226] loss: 4.019 running loss:  6826.359\n",
      "[2,   236] loss: 6.922 running loss:  7142.161\n",
      "[2,   246] loss: 5.471 running loss:  7419.455\n",
      "[2,   256] loss: 8.712 running loss:  7742.511\n",
      "[2,   266] loss: 7.112 running loss:  8063.349\n",
      "[2,   276] loss: 5.381 running loss:  8396.362\n",
      "[2,   286] loss: 10.322 running loss:  8719.502\n",
      "[2,   296] loss: 5.659 running loss:  9024.130\n",
      "[2,   306] loss: 2.501 running loss:  9322.897\n",
      "[2,   316] loss: 4.637 running loss:  9610.688\n",
      "[2,   326] loss: 6.037 running loss:  9922.340\n",
      "[2,   336] loss: 3.192 running loss:  10214.533\n",
      "[2,   346] loss: 7.453 running loss:  10594.531\n",
      "[2,   356] loss: 6.508 running loss:  10929.208\n",
      "[2,   366] loss: 4.375 running loss:  11236.924\n",
      "[2,   376] loss: 5.624 running loss:  11529.468\n",
      "[2,   386] loss: 4.565 running loss:  11818.129\n",
      "[2,   396] loss: 7.901 running loss:  12152.528\n",
      "[2,   406] loss: 8.966 running loss:  12476.888\n",
      "[2,   416] loss: 7.923 running loss:  12794.294\n",
      "[2,   426] loss: 5.487 running loss:  13120.400\n",
      "[2,   436] loss: 6.484 running loss:  13438.110\n",
      "[2,   446] loss: 8.515 running loss:  13742.816\n",
      "[2,   456] loss: 7.924 running loss:  14037.770\n",
      "[2,   466] loss: 6.321 running loss:  14298.944\n",
      "[2,   476] loss: 8.010 running loss:  14587.130\n",
      "[2,   486] loss: 9.343 running loss:  14957.733\n",
      "[2,   496] loss: 5.089 running loss:  15273.605\n",
      "[2,   506] loss: 8.836 running loss:  15558.659\n",
      "[2,   516] loss: 6.332 running loss:  15885.368\n",
      "[2,   526] loss: 5.378 running loss:  16200.229\n",
      "[2,   536] loss: 7.405 running loss:  16503.410\n",
      "[2,   546] loss: 3.011 running loss:  16788.752\n",
      "[2,   556] loss: 5.501 running loss:  17147.422\n",
      "[2,   566] loss: 3.423 running loss:  17441.914\n",
      "[2,   576] loss: 5.941 running loss:  17731.848\n",
      "[2,   586] loss: 8.155 running loss:  18057.889\n",
      "[2,   596] loss: 6.027 running loss:  18384.974\n",
      "[2,   606] loss: 1.928 running loss:  18669.189\n",
      "[2,   616] loss: 3.760 running loss:  18929.270\n",
      "[2,   626] loss: 5.058 running loss:  19268.883\n",
      "[2,   636] loss: 6.519 running loss:  19572.995\n",
      "[2,   646] loss: 5.690 running loss:  19910.500\n",
      "[2,   656] loss: 4.538 running loss:  20197.537\n",
      "[2,   666] loss: 9.009 running loss:  20508.253\n",
      "[2,   676] loss: 9.009 running loss:  20836.028\n",
      "[2,   686] loss: 4.486 running loss:  21110.092\n",
      "[2,   696] loss: 6.213 running loss:  21430.673\n",
      "[2,   706] loss: 7.698 running loss:  21744.655\n",
      "[2,   716] loss: 6.583 running loss:  21980.458\n",
      "[2,   726] loss: 5.052 running loss:  22279.427\n",
      "[2,   736] loss: 5.103 running loss:  22583.821\n",
      "[2,   746] loss: 7.365 running loss:  22896.902\n",
      "[2,   756] loss: 5.859 running loss:  23159.665\n",
      "[2,   766] loss: 7.369 running loss:  23487.136\n",
      "[2,   776] loss: 7.309 running loss:  23797.177\n",
      "[2,   786] loss: 5.816 running loss:  24124.225\n",
      "[2,   796] loss: 4.845 running loss:  24428.725\n",
      "[2,   806] loss: 6.570 running loss:  24733.217\n",
      "[2,   816] loss: 4.959 running loss:  25014.071\n",
      "[2,   826] loss: 3.885 running loss:  25319.047\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007fb6e796e6449d8d3cb559b2b3dd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     6] loss: 6.139 running loss:  143.796\n",
      "[3,    16] loss: 3.974 running loss:  434.253\n",
      "[3,    26] loss: 5.793 running loss:  728.414\n",
      "[3,    36] loss: 6.470 running loss:  1060.625\n",
      "[3,    46] loss: 4.383 running loss:  1346.085\n",
      "[3,    56] loss: 6.129 running loss:  1674.576\n",
      "[3,    66] loss: 5.891 running loss:  2006.923\n",
      "[3,    76] loss: 6.268 running loss:  2307.462\n",
      "[3,    86] loss: 5.550 running loss:  2607.266\n",
      "[3,    96] loss: 4.411 running loss:  2919.112\n",
      "[3,   106] loss: 7.222 running loss:  3190.261\n",
      "[3,   116] loss: 4.575 running loss:  3516.492\n",
      "[3,   126] loss: 5.950 running loss:  3848.125\n",
      "[3,   136] loss: 5.126 running loss:  4157.389\n",
      "[3,   146] loss: 3.990 running loss:  4423.637\n",
      "[3,   156] loss: 4.281 running loss:  4679.699\n",
      "[3,   166] loss: 6.203 running loss:  4998.245\n",
      "[3,   176] loss: 6.533 running loss:  5342.513\n",
      "[3,   186] loss: 6.185 running loss:  5645.002\n",
      "[3,   196] loss: 5.902 running loss:  5973.554\n",
      "[3,   206] loss: 7.039 running loss:  6266.179\n",
      "[3,   216] loss: 7.380 running loss:  6575.636\n",
      "[3,   226] loss: 4.568 running loss:  6872.473\n",
      "[3,   236] loss: 7.512 running loss:  7168.681\n",
      "[3,   246] loss: 6.009 running loss:  7470.071\n",
      "[3,   256] loss: 6.260 running loss:  7764.337\n",
      "[3,   266] loss: 6.953 running loss:  8054.276\n",
      "[3,   276] loss: 5.716 running loss:  8381.822\n",
      "[3,   286] loss: 6.024 running loss:  8682.170\n",
      "[3,   296] loss: 6.678 running loss:  9025.398\n",
      "[3,   306] loss: 5.789 running loss:  9256.141\n",
      "[3,   316] loss: 6.588 running loss:  9572.806\n",
      "[3,   326] loss: 6.728 running loss:  9914.901\n",
      "[3,   336] loss: 5.868 running loss:  10241.808\n",
      "[3,   346] loss: 6.805 running loss:  10513.065\n",
      "[3,   356] loss: 6.310 running loss:  10798.207\n",
      "[3,   366] loss: 7.113 running loss:  11144.157\n",
      "[3,   376] loss: 5.811 running loss:  11456.587\n",
      "[3,   386] loss: 7.928 running loss:  11756.000\n",
      "[3,   396] loss: 7.475 running loss:  12078.649\n",
      "[3,   406] loss: 7.662 running loss:  12384.628\n",
      "[3,   416] loss: 8.249 running loss:  12676.715\n",
      "[3,   426] loss: 8.051 running loss:  13001.072\n",
      "[3,   436] loss: 3.958 running loss:  13291.733\n",
      "[3,   446] loss: 2.371 running loss:  13566.021\n",
      "[3,   456] loss: 5.035 running loss:  13871.900\n",
      "[3,   466] loss: 9.109 running loss:  14178.169\n",
      "[3,   476] loss: 4.928 running loss:  14464.037\n",
      "[3,   486] loss: 7.243 running loss:  14817.052\n",
      "[3,   496] loss: 5.374 running loss:  15164.175\n",
      "[3,   506] loss: 5.936 running loss:  15474.566\n",
      "[3,   516] loss: 6.810 running loss:  15802.154\n",
      "[3,   526] loss: 4.872 running loss:  16093.110\n",
      "[3,   536] loss: 6.240 running loss:  16413.101\n",
      "[3,   546] loss: 4.969 running loss:  16734.494\n",
      "[3,   556] loss: 6.038 running loss:  17070.296\n",
      "[3,   566] loss: 5.561 running loss:  17347.615\n",
      "[3,   576] loss: 8.037 running loss:  17661.313\n",
      "[3,   586] loss: 7.030 running loss:  17975.253\n",
      "[3,   596] loss: 8.039 running loss:  18265.001\n",
      "[3,   606] loss: 6.195 running loss:  18584.996\n",
      "[3,   616] loss: 3.863 running loss:  18885.647\n",
      "[3,   626] loss: 7.266 running loss:  19208.819\n",
      "[3,   636] loss: 6.439 running loss:  19535.021\n",
      "[3,   646] loss: 7.860 running loss:  19817.291\n",
      "[3,   656] loss: 8.094 running loss:  20107.982\n",
      "[3,   666] loss: 5.250 running loss:  20378.521\n",
      "[3,   676] loss: 6.350 running loss:  20658.722\n",
      "[3,   686] loss: 9.581 running loss:  20994.217\n",
      "[3,   696] loss: 6.632 running loss:  21355.497\n",
      "[3,   706] loss: 4.386 running loss:  21668.708\n",
      "[3,   716] loss: 7.687 running loss:  21985.614\n",
      "[3,   726] loss: 6.275 running loss:  22291.500\n",
      "[3,   736] loss: 6.667 running loss:  22620.245\n",
      "[3,   746] loss: 4.033 running loss:  22916.636\n",
      "[3,   756] loss: 8.540 running loss:  23198.210\n",
      "[3,   766] loss: 5.656 running loss:  23467.596\n",
      "[3,   776] loss: 7.457 running loss:  23792.984\n",
      "[3,   786] loss: 4.484 running loss:  24111.155\n",
      "[3,   796] loss: 5.220 running loss:  24425.921\n",
      "[3,   806] loss: 3.983 running loss:  24709.272\n",
      "[3,   816] loss: 6.213 running loss:  25015.289\n",
      "[3,   826] loss: 4.985 running loss:  25272.506\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c114251548eb4c15ba56c398707a55e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,     6] loss: 7.909 running loss:  139.560\n",
      "[4,    16] loss: 5.916 running loss:  430.330\n",
      "[4,    26] loss: 4.313 running loss:  712.515\n",
      "[4,    36] loss: 8.178 running loss:  1064.695\n",
      "[4,    46] loss: 4.582 running loss:  1393.679\n",
      "[4,    56] loss: 4.644 running loss:  1684.588\n",
      "[4,    66] loss: 6.839 running loss:  2025.203\n",
      "[4,    76] loss: 3.500 running loss:  2286.713\n",
      "[4,    86] loss: 3.344 running loss:  2574.644\n",
      "[4,    96] loss: 9.928 running loss:  2885.396\n",
      "[4,   106] loss: 5.550 running loss:  3153.316\n",
      "[4,   116] loss: 5.848 running loss:  3440.469\n",
      "[4,   126] loss: 4.834 running loss:  3716.907\n",
      "[4,   136] loss: 5.154 running loss:  4021.564\n",
      "[4,   146] loss: 7.027 running loss:  4313.985\n",
      "[4,   156] loss: 3.876 running loss:  4617.021\n",
      "[4,   166] loss: 5.409 running loss:  4941.117\n",
      "[4,   176] loss: 4.192 running loss:  5269.106\n",
      "[4,   186] loss: 5.494 running loss:  5635.346\n",
      "[4,   196] loss: 7.819 running loss:  5949.532\n",
      "[4,   206] loss: 6.810 running loss:  6269.145\n",
      "[4,   216] loss: 5.803 running loss:  6602.643\n",
      "[4,   226] loss: 7.271 running loss:  6895.829\n",
      "[4,   236] loss: 5.661 running loss:  7174.478\n",
      "[4,   246] loss: 7.577 running loss:  7461.221\n",
      "[4,   256] loss: 4.755 running loss:  7752.302\n",
      "[4,   266] loss: 5.230 running loss:  8053.955\n",
      "[4,   276] loss: 5.196 running loss:  8389.961\n",
      "[4,   286] loss: 8.229 running loss:  8731.594\n",
      "[4,   296] loss: 6.646 running loss:  9047.934\n",
      "[4,   306] loss: 7.586 running loss:  9364.108\n",
      "[4,   316] loss: 6.244 running loss:  9695.121\n",
      "[4,   326] loss: 7.261 running loss:  10016.195\n",
      "[4,   336] loss: 6.014 running loss:  10283.209\n",
      "[4,   346] loss: 6.192 running loss:  10569.246\n",
      "[4,   356] loss: 4.903 running loss:  10918.264\n",
      "[4,   366] loss: 5.373 running loss:  11255.621\n",
      "[4,   376] loss: 6.585 running loss:  11574.785\n",
      "[4,   386] loss: 4.608 running loss:  11844.091\n",
      "[4,   396] loss: 8.232 running loss:  12151.755\n",
      "[4,   406] loss: 6.682 running loss:  12453.067\n",
      "[4,   416] loss: 3.431 running loss:  12738.519\n",
      "[4,   426] loss: 8.965 running loss:  13063.361\n",
      "[4,   436] loss: 8.401 running loss:  13338.345\n",
      "[4,   446] loss: 5.780 running loss:  13614.557\n",
      "[4,   456] loss: 5.836 running loss:  13882.652\n",
      "[4,   466] loss: 7.620 running loss:  14198.597\n",
      "[4,   476] loss: 5.088 running loss:  14488.765\n",
      "[4,   486] loss: 6.307 running loss:  14732.215\n",
      "[4,   496] loss: 3.887 running loss:  15067.904\n",
      "[4,   506] loss: 7.551 running loss:  15395.499\n",
      "[4,   516] loss: 5.834 running loss:  15701.313\n",
      "[4,   526] loss: 5.247 running loss:  16000.287\n",
      "[4,   536] loss: 7.228 running loss:  16271.868\n",
      "[4,   546] loss: 6.512 running loss:  16579.724\n",
      "[4,   556] loss: 7.137 running loss:  16896.962\n",
      "[4,   566] loss: 10.306 running loss:  17208.359\n",
      "[4,   576] loss: 5.917 running loss:  17519.003\n",
      "[4,   586] loss: 7.135 running loss:  17840.454\n",
      "[4,   596] loss: 8.895 running loss:  18165.914\n",
      "[4,   606] loss: 3.633 running loss:  18420.097\n",
      "[4,   616] loss: 7.229 running loss:  18738.509\n",
      "[4,   626] loss: 8.171 running loss:  19067.205\n",
      "[4,   636] loss: 9.199 running loss:  19393.064\n",
      "[4,   646] loss: 4.742 running loss:  19675.878\n",
      "[4,   656] loss: 6.859 running loss:  19994.084\n",
      "[4,   666] loss: 6.827 running loss:  20327.228\n",
      "[4,   676] loss: 6.712 running loss:  20625.733\n",
      "[4,   686] loss: 7.076 running loss:  20903.420\n",
      "[4,   696] loss: 8.700 running loss:  21186.027\n",
      "[4,   706] loss: 3.654 running loss:  21509.961\n",
      "[4,   716] loss: 3.322 running loss:  21787.862\n",
      "[4,   726] loss: 7.610 running loss:  22095.668\n",
      "[4,   736] loss: 5.210 running loss:  22411.661\n",
      "[4,   746] loss: 4.818 running loss:  22750.865\n",
      "[4,   756] loss: 7.236 running loss:  23057.705\n",
      "[4,   766] loss: 5.805 running loss:  23370.759\n",
      "[4,   776] loss: 6.085 running loss:  23656.248\n",
      "[4,   786] loss: 9.520 running loss:  23995.186\n",
      "[4,   796] loss: 5.899 running loss:  24300.920\n",
      "[4,   806] loss: 8.676 running loss:  24663.010\n",
      "[4,   816] loss: 8.581 running loss:  24972.759\n",
      "[4,   826] loss: 3.508 running loss:  25239.071\n",
      "Epoch 4/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166b9fa49fa845ebacf98a9f239cb89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,     6] loss: 5.756 running loss:  150.098\n",
      "[5,    16] loss: 5.445 running loss:  481.431\n",
      "[5,    26] loss: 6.705 running loss:  768.326\n",
      "[5,    36] loss: 5.673 running loss:  1032.741\n",
      "[5,    46] loss: 3.487 running loss:  1320.928\n",
      "[5,    56] loss: 5.463 running loss:  1660.002\n",
      "[5,    66] loss: 6.491 running loss:  1988.941\n",
      "[5,    76] loss: 8.185 running loss:  2291.146\n",
      "[5,    86] loss: 6.081 running loss:  2577.688\n",
      "[5,    96] loss: 6.516 running loss:  2901.289\n",
      "[5,   106] loss: 6.284 running loss:  3219.925\n",
      "[5,   116] loss: 7.470 running loss:  3559.858\n",
      "[5,   126] loss: 4.283 running loss:  3842.283\n",
      "[5,   136] loss: 6.287 running loss:  4170.055\n",
      "[5,   146] loss: 5.975 running loss:  4488.808\n",
      "[5,   156] loss: 9.081 running loss:  4789.378\n",
      "[5,   166] loss: 5.520 running loss:  5080.021\n",
      "[5,   176] loss: 3.125 running loss:  5341.342\n",
      "[5,   186] loss: 5.358 running loss:  5664.602\n",
      "[5,   196] loss: 8.345 running loss:  6024.939\n",
      "[5,   206] loss: 9.535 running loss:  6347.275\n",
      "[5,   216] loss: 5.832 running loss:  6634.448\n",
      "[5,   226] loss: 7.071 running loss:  6934.729\n",
      "[5,   236] loss: 8.047 running loss:  7220.270\n",
      "[5,   246] loss: 5.702 running loss:  7504.266\n",
      "[5,   256] loss: 6.365 running loss:  7784.705\n",
      "[5,   266] loss: 5.775 running loss:  8067.125\n",
      "[5,   276] loss: 7.536 running loss:  8387.061\n",
      "[5,   286] loss: 3.791 running loss:  8643.342\n",
      "[5,   296] loss: 5.770 running loss:  8900.631\n",
      "[5,   306] loss: 7.743 running loss:  9184.545\n",
      "[5,   316] loss: 5.765 running loss:  9472.060\n",
      "[5,   326] loss: 6.079 running loss:  9698.462\n",
      "[5,   336] loss: 6.460 running loss:  9978.663\n",
      "[5,   346] loss: 6.763 running loss:  10300.658\n",
      "[5,   356] loss: 6.218 running loss:  10576.947\n",
      "[5,   366] loss: 5.792 running loss:  10874.123\n",
      "[5,   376] loss: 7.355 running loss:  11178.521\n",
      "[5,   386] loss: 8.319 running loss:  11527.670\n",
      "[5,   396] loss: 7.362 running loss:  11888.090\n",
      "[5,   406] loss: 8.862 running loss:  12224.761\n",
      "[5,   416] loss: 5.347 running loss:  12505.944\n",
      "[5,   426] loss: 4.850 running loss:  12810.463\n",
      "[5,   436] loss: 5.992 running loss:  13128.523\n",
      "[5,   446] loss: 7.090 running loss:  13485.215\n",
      "[5,   456] loss: 5.579 running loss:  13823.747\n",
      "[5,   466] loss: 4.650 running loss:  14136.415\n",
      "[5,   476] loss: 5.587 running loss:  14451.182\n",
      "[5,   486] loss: 4.909 running loss:  14748.606\n",
      "[5,   496] loss: 7.361 running loss:  15067.635\n",
      "[5,   506] loss: 6.073 running loss:  15360.303\n",
      "[5,   516] loss: 5.006 running loss:  15679.851\n",
      "[5,   526] loss: 6.838 running loss:  15973.478\n",
      "[5,   536] loss: 4.722 running loss:  16227.688\n",
      "[5,   546] loss: 7.192 running loss:  16519.522\n",
      "[5,   556] loss: 8.558 running loss:  16834.051\n",
      "[5,   566] loss: 5.653 running loss:  17131.448\n",
      "[5,   576] loss: 5.251 running loss:  17388.345\n",
      "[5,   586] loss: 7.331 running loss:  17729.974\n",
      "[5,   596] loss: 4.086 running loss:  17996.116\n",
      "[5,   606] loss: 7.001 running loss:  18309.622\n",
      "[5,   616] loss: 5.316 running loss:  18641.155\n",
      "[5,   626] loss: 4.276 running loss:  18943.444\n",
      "[5,   636] loss: 6.373 running loss:  19282.416\n",
      "[5,   646] loss: 6.557 running loss:  19640.010\n",
      "[5,   656] loss: 6.790 running loss:  19973.890\n",
      "[5,   666] loss: 6.302 running loss:  20301.681\n",
      "[5,   676] loss: 8.054 running loss:  20634.758\n",
      "[5,   686] loss: 4.866 running loss:  20934.865\n",
      "[5,   696] loss: 6.356 running loss:  21253.305\n",
      "[5,   706] loss: 5.045 running loss:  21554.833\n",
      "[5,   716] loss: 4.012 running loss:  21833.843\n",
      "[5,   726] loss: 3.754 running loss:  22125.150\n",
      "[5,   736] loss: 7.514 running loss:  22479.220\n",
      "[5,   746] loss: 4.614 running loss:  22783.251\n",
      "[5,   756] loss: 6.086 running loss:  23090.830\n",
      "[5,   766] loss: 5.898 running loss:  23352.041\n",
      "[5,   776] loss: 8.338 running loss:  23674.740\n",
      "[5,   786] loss: 8.830 running loss:  24010.156\n",
      "[5,   796] loss: 4.271 running loss:  24295.648\n",
      "[5,   806] loss: 5.358 running loss:  24576.093\n",
      "[5,   816] loss: 8.239 running loss:  24898.158\n",
      "[5,   826] loss: 7.934 running loss:  25209.748\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e770a94787645ca99d754473b90b8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,     6] loss: 8.160 running loss:  154.602\n",
      "[6,    16] loss: 6.828 running loss:  453.092\n",
      "[6,    26] loss: 6.550 running loss:  772.986\n",
      "[6,    36] loss: 3.221 running loss:  1053.843\n",
      "[6,    46] loss: 6.939 running loss:  1399.132\n",
      "[6,    56] loss: 7.521 running loss:  1689.513\n",
      "[6,    66] loss: 7.923 running loss:  1956.889\n",
      "[6,    76] loss: 7.675 running loss:  2316.005\n",
      "[6,    86] loss: 6.287 running loss:  2594.611\n",
      "[6,    96] loss: 6.614 running loss:  2892.159\n",
      "[6,   106] loss: 2.945 running loss:  3190.300\n",
      "[6,   116] loss: 3.586 running loss:  3489.215\n",
      "[6,   126] loss: 5.461 running loss:  3769.602\n",
      "[6,   136] loss: 6.112 running loss:  4082.767\n",
      "[6,   146] loss: 7.141 running loss:  4397.716\n",
      "[6,   156] loss: 7.570 running loss:  4713.092\n",
      "[6,   166] loss: 7.791 running loss:  5030.605\n",
      "[6,   176] loss: 6.933 running loss:  5363.242\n",
      "[6,   186] loss: 4.714 running loss:  5664.227\n",
      "[6,   196] loss: 8.000 running loss:  5947.562\n",
      "[6,   206] loss: 7.101 running loss:  6225.918\n",
      "[6,   216] loss: 5.033 running loss:  6525.843\n",
      "[6,   226] loss: 7.059 running loss:  6844.554\n",
      "[6,   236] loss: 5.837 running loss:  7163.311\n",
      "[6,   246] loss: 6.991 running loss:  7465.556\n",
      "[6,   256] loss: 8.315 running loss:  7769.680\n",
      "[6,   266] loss: 6.248 running loss:  8050.483\n",
      "[6,   276] loss: 8.705 running loss:  8398.693\n",
      "[6,   286] loss: 6.532 running loss:  8698.419\n",
      "[6,   296] loss: 6.058 running loss:  9025.275\n",
      "[6,   306] loss: 7.543 running loss:  9345.511\n",
      "[6,   316] loss: 8.278 running loss:  9627.087\n",
      "[6,   326] loss: 6.900 running loss:  9942.896\n",
      "[6,   336] loss: 8.185 running loss:  10242.188\n",
      "[6,   346] loss: 6.495 running loss:  10562.408\n",
      "[6,   356] loss: 6.632 running loss:  10855.997\n",
      "[6,   366] loss: 5.428 running loss:  11198.537\n",
      "[6,   376] loss: 5.278 running loss:  11521.014\n",
      "[6,   386] loss: 4.396 running loss:  11805.124\n",
      "[6,   396] loss: 5.495 running loss:  12119.687\n",
      "[6,   406] loss: 6.519 running loss:  12426.244\n",
      "[6,   416] loss: 3.953 running loss:  12717.315\n",
      "[6,   426] loss: 5.392 running loss:  13027.448\n",
      "[6,   436] loss: 8.539 running loss:  13356.951\n",
      "[6,   446] loss: 5.321 running loss:  13640.202\n",
      "[6,   456] loss: 3.878 running loss:  13914.281\n",
      "[6,   466] loss: 5.642 running loss:  14229.008\n",
      "[6,   476] loss: 6.454 running loss:  14546.589\n",
      "[6,   486] loss: 4.969 running loss:  14876.129\n",
      "[6,   496] loss: 5.696 running loss:  15175.455\n",
      "[6,   506] loss: 7.620 running loss:  15462.121\n",
      "[6,   516] loss: 6.906 running loss:  15789.721\n",
      "[6,   526] loss: 5.666 running loss:  16067.071\n",
      "[6,   536] loss: 3.835 running loss:  16334.744\n",
      "[6,   546] loss: 9.051 running loss:  16650.884\n",
      "[6,   556] loss: 6.408 running loss:  16945.162\n",
      "[6,   566] loss: 4.746 running loss:  17240.664\n",
      "[6,   576] loss: 5.905 running loss:  17585.479\n",
      "[6,   586] loss: 6.117 running loss:  17882.052\n",
      "[6,   596] loss: 7.113 running loss:  18198.707\n",
      "[6,   606] loss: 9.207 running loss:  18534.393\n",
      "[6,   616] loss: 7.378 running loss:  18844.405\n",
      "[6,   626] loss: 5.592 running loss:  19169.635\n",
      "[6,   636] loss: 6.996 running loss:  19436.806\n",
      "[6,   646] loss: 4.286 running loss:  19681.983\n",
      "[6,   656] loss: 6.043 running loss:  19957.903\n",
      "[6,   666] loss: 6.594 running loss:  20240.834\n",
      "[6,   676] loss: 4.053 running loss:  20535.787\n",
      "[6,   686] loss: 5.478 running loss:  20820.731\n",
      "[6,   696] loss: 4.898 running loss:  21102.201\n",
      "[6,   706] loss: 4.676 running loss:  21427.708\n",
      "[6,   716] loss: 4.663 running loss:  21710.345\n",
      "[6,   726] loss: 4.142 running loss:  21973.525\n",
      "[6,   736] loss: 4.602 running loss:  22310.002\n",
      "[6,   746] loss: 3.290 running loss:  22594.034\n",
      "[6,   756] loss: 6.135 running loss:  22934.857\n",
      "[6,   766] loss: 8.695 running loss:  23311.015\n",
      "[6,   776] loss: 3.445 running loss:  23651.745\n",
      "[6,   786] loss: 7.332 running loss:  23957.272\n",
      "[6,   796] loss: 7.663 running loss:  24248.133\n",
      "[6,   806] loss: 6.371 running loss:  24588.990\n",
      "[6,   816] loss: 7.228 running loss:  24896.853\n",
      "[6,   826] loss: 8.184 running loss:  25219.267\n",
      "Epoch 6/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0392c31122d42f4acca4b8d6225077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,     6] loss: 6.373 running loss:  138.812\n",
      "[7,    16] loss: 4.917 running loss:  423.504\n",
      "[7,    26] loss: 5.387 running loss:  735.431\n",
      "[7,    36] loss: 8.026 running loss:  1043.645\n",
      "[7,    46] loss: 6.194 running loss:  1307.825\n",
      "[7,    56] loss: 8.781 running loss:  1638.319\n",
      "[7,    66] loss: 6.611 running loss:  1911.611\n",
      "[7,    76] loss: 6.028 running loss:  2219.434\n",
      "[7,    86] loss: 5.930 running loss:  2541.739\n",
      "[7,    96] loss: 6.515 running loss:  2857.928\n",
      "[7,   106] loss: 5.785 running loss:  3177.642\n",
      "[7,   116] loss: 5.296 running loss:  3488.530\n",
      "[7,   126] loss: 7.805 running loss:  3812.387\n",
      "[7,   136] loss: 3.942 running loss:  4115.823\n",
      "[7,   146] loss: 4.664 running loss:  4381.761\n",
      "[7,   156] loss: 5.947 running loss:  4670.048\n",
      "[7,   166] loss: 5.448 running loss:  4961.087\n",
      "[7,   176] loss: 6.950 running loss:  5292.222\n",
      "[7,   186] loss: 10.923 running loss:  5631.523\n",
      "[7,   196] loss: 6.548 running loss:  5977.332\n",
      "[7,   206] loss: 7.102 running loss:  6308.332\n",
      "[7,   216] loss: 7.512 running loss:  6622.099\n",
      "[7,   226] loss: 9.447 running loss:  6976.867\n",
      "[7,   236] loss: 6.300 running loss:  7300.815\n",
      "[7,   246] loss: 5.757 running loss:  7622.075\n",
      "[7,   256] loss: 8.664 running loss:  7889.767\n",
      "[7,   266] loss: 7.543 running loss:  8178.182\n",
      "[7,   276] loss: 6.054 running loss:  8479.756\n",
      "[7,   286] loss: 6.839 running loss:  8815.159\n",
      "[7,   296] loss: 8.580 running loss:  9137.933\n",
      "[7,   306] loss: 6.091 running loss:  9430.495\n",
      "[7,   316] loss: 7.556 running loss:  9758.644\n",
      "[7,   326] loss: 4.633 running loss:  10038.475\n",
      "[7,   336] loss: 5.215 running loss:  10352.939\n",
      "[7,   346] loss: 7.834 running loss:  10671.458\n",
      "[7,   356] loss: 5.371 running loss:  10990.203\n",
      "[7,   366] loss: 4.714 running loss:  11291.853\n",
      "[7,   376] loss: 8.343 running loss:  11591.060\n",
      "[7,   386] loss: 5.146 running loss:  11907.553\n",
      "[7,   396] loss: 8.335 running loss:  12186.387\n",
      "[7,   406] loss: 5.533 running loss:  12472.563\n",
      "[7,   416] loss: 5.347 running loss:  12793.066\n",
      "[7,   426] loss: 3.873 running loss:  13106.185\n",
      "[7,   436] loss: 4.714 running loss:  13415.427\n",
      "[7,   446] loss: 4.819 running loss:  13679.379\n",
      "[7,   456] loss: 7.554 running loss:  13998.187\n",
      "[7,   466] loss: 5.871 running loss:  14299.357\n",
      "[7,   476] loss: 8.301 running loss:  14616.321\n",
      "[7,   486] loss: 6.304 running loss:  14899.089\n",
      "[7,   496] loss: 6.172 running loss:  15161.950\n",
      "[7,   506] loss: 8.131 running loss:  15491.896\n",
      "[7,   516] loss: 7.987 running loss:  15854.903\n",
      "[7,   526] loss: 6.795 running loss:  16190.297\n",
      "[7,   536] loss: 3.484 running loss:  16454.528\n",
      "[7,   546] loss: 4.544 running loss:  16750.874\n",
      "[7,   556] loss: 5.870 running loss:  17049.062\n",
      "[7,   566] loss: 5.240 running loss:  17363.274\n",
      "[7,   576] loss: 8.903 running loss:  17695.216\n",
      "[7,   586] loss: 7.364 running loss:  18001.016\n",
      "[7,   596] loss: 4.369 running loss:  18302.591\n",
      "[7,   606] loss: 5.976 running loss:  18632.355\n",
      "[7,   616] loss: 7.427 running loss:  18947.392\n",
      "[7,   626] loss: 5.608 running loss:  19266.953\n",
      "[7,   636] loss: 9.845 running loss:  19574.557\n",
      "[7,   646] loss: 1.601 running loss:  19835.013\n",
      "[7,   656] loss: 6.079 running loss:  20158.096\n",
      "[7,   666] loss: 5.235 running loss:  20428.259\n",
      "[7,   676] loss: 6.770 running loss:  20750.382\n",
      "[7,   686] loss: 5.947 running loss:  21050.702\n",
      "[7,   696] loss: 6.211 running loss:  21319.307\n",
      "[7,   706] loss: 4.625 running loss:  21622.563\n",
      "[7,   716] loss: 3.359 running loss:  21912.841\n",
      "[7,   726] loss: 9.584 running loss:  22248.061\n",
      "[7,   736] loss: 6.244 running loss:  22548.682\n",
      "[7,   746] loss: 4.633 running loss:  22843.769\n",
      "[7,   756] loss: 7.562 running loss:  23109.649\n",
      "[7,   766] loss: 4.857 running loss:  23364.985\n",
      "[7,   776] loss: 6.439 running loss:  23696.579\n",
      "[7,   786] loss: 4.885 running loss:  23974.179\n",
      "[7,   796] loss: 4.379 running loss:  24251.513\n",
      "[7,   806] loss: 8.782 running loss:  24564.633\n",
      "[7,   816] loss: 7.633 running loss:  24871.511\n",
      "[7,   826] loss: 3.856 running loss:  25215.689\n",
      "Epoch 7/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de08c4b0dce74ed8a6f793b5a4436a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     6] loss: 5.421 running loss:  159.884\n",
      "[8,    16] loss: 4.583 running loss:  422.582\n",
      "[8,    26] loss: 5.470 running loss:  750.956\n",
      "[8,    36] loss: 3.661 running loss:  1033.294\n",
      "[8,    46] loss: 4.265 running loss:  1339.592\n",
      "[8,    56] loss: 5.817 running loss:  1656.455\n",
      "[8,    66] loss: 6.268 running loss:  1976.437\n",
      "[8,    76] loss: 3.956 running loss:  2309.796\n",
      "[8,    86] loss: 7.121 running loss:  2601.297\n",
      "[8,    96] loss: 4.760 running loss:  2882.008\n",
      "[8,   106] loss: 7.341 running loss:  3167.095\n",
      "[8,   116] loss: 5.541 running loss:  3452.690\n",
      "[8,   126] loss: 6.137 running loss:  3728.387\n",
      "[8,   136] loss: 5.576 running loss:  4024.523\n",
      "[8,   146] loss: 6.309 running loss:  4305.306\n",
      "[8,   156] loss: 6.012 running loss:  4666.045\n",
      "[8,   166] loss: 7.053 running loss:  5012.789\n",
      "[8,   176] loss: 3.128 running loss:  5302.911\n",
      "[8,   186] loss: 9.200 running loss:  5626.782\n",
      "[8,   196] loss: 4.296 running loss:  5879.583\n",
      "[8,   206] loss: 5.554 running loss:  6189.652\n",
      "[8,   216] loss: 5.491 running loss:  6459.139\n",
      "[8,   226] loss: 4.447 running loss:  6787.806\n",
      "[8,   236] loss: 8.924 running loss:  7102.992\n",
      "[8,   246] loss: 4.658 running loss:  7418.548\n",
      "[8,   256] loss: 5.150 running loss:  7708.417\n",
      "[8,   266] loss: 5.164 running loss:  8042.574\n",
      "[8,   276] loss: 8.380 running loss:  8393.190\n",
      "[8,   286] loss: 6.263 running loss:  8676.657\n",
      "[8,   296] loss: 7.023 running loss:  8956.899\n",
      "[8,   306] loss: 5.920 running loss:  9258.767\n",
      "[8,   316] loss: 4.016 running loss:  9571.927\n",
      "[8,   326] loss: 4.695 running loss:  9895.650\n",
      "[8,   336] loss: 7.101 running loss:  10239.772\n",
      "[8,   346] loss: 9.809 running loss:  10577.801\n",
      "[8,   356] loss: 7.297 running loss:  10841.471\n",
      "[8,   366] loss: 4.918 running loss:  11138.451\n",
      "[8,   376] loss: 6.696 running loss:  11472.353\n",
      "[8,   386] loss: 5.123 running loss:  11750.793\n",
      "[8,   396] loss: 5.436 running loss:  12025.767\n",
      "[8,   406] loss: 6.033 running loss:  12314.962\n",
      "[8,   416] loss: 5.883 running loss:  12635.666\n",
      "[8,   426] loss: 8.333 running loss:  12956.123\n",
      "[8,   436] loss: 4.448 running loss:  13239.019\n",
      "[8,   446] loss: 3.031 running loss:  13526.142\n",
      "[8,   456] loss: 5.600 running loss:  13847.618\n",
      "[8,   466] loss: 4.495 running loss:  14181.855\n",
      "[8,   476] loss: 4.275 running loss:  14448.954\n",
      "[8,   486] loss: 6.278 running loss:  14751.458\n",
      "[8,   496] loss: 10.675 running loss:  15114.033\n",
      "[8,   506] loss: 6.126 running loss:  15380.118\n",
      "[8,   516] loss: 8.706 running loss:  15664.463\n",
      "[8,   526] loss: 4.170 running loss:  15955.344\n",
      "[8,   536] loss: 7.805 running loss:  16285.924\n",
      "[8,   546] loss: 5.626 running loss:  16596.141\n",
      "[8,   556] loss: 4.645 running loss:  16877.223\n",
      "[8,   566] loss: 7.055 running loss:  17170.634\n",
      "[8,   576] loss: 4.648 running loss:  17490.784\n",
      "[8,   586] loss: 4.867 running loss:  17813.798\n",
      "[8,   596] loss: 5.078 running loss:  18111.488\n",
      "[8,   606] loss: 3.103 running loss:  18429.112\n",
      "[8,   616] loss: 7.060 running loss:  18759.557\n",
      "[8,   626] loss: 4.458 running loss:  19040.803\n",
      "[8,   636] loss: 4.914 running loss:  19375.457\n",
      "[8,   646] loss: 7.098 running loss:  19683.075\n",
      "[8,   656] loss: 4.707 running loss:  19953.818\n",
      "[8,   666] loss: 6.117 running loss:  20241.197\n",
      "[8,   676] loss: 8.146 running loss:  20588.603\n",
      "[8,   686] loss: 7.547 running loss:  20905.336\n",
      "[8,   696] loss: 5.856 running loss:  21184.090\n",
      "[8,   706] loss: 7.053 running loss:  21464.740\n",
      "[8,   716] loss: 3.154 running loss:  21754.652\n",
      "[8,   726] loss: 7.671 running loss:  22116.691\n",
      "[8,   736] loss: 8.174 running loss:  22378.272\n",
      "[8,   746] loss: 5.285 running loss:  22699.140\n",
      "[8,   756] loss: 6.652 running loss:  23023.455\n",
      "[8,   766] loss: 6.493 running loss:  23381.427\n",
      "[8,   776] loss: 4.315 running loss:  23659.052\n",
      "[8,   786] loss: 6.008 running loss:  24003.069\n",
      "[8,   796] loss: 4.907 running loss:  24297.021\n",
      "[8,   806] loss: 3.784 running loss:  24574.060\n",
      "[8,   816] loss: 7.642 running loss:  24935.558\n",
      "[8,   826] loss: 3.912 running loss:  25213.028\n",
      "Epoch 8/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a7c795934943e08d5379a1c97b58ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,     6] loss: 4.958 running loss:  150.388\n",
      "[9,    16] loss: 4.264 running loss:  442.510\n",
      "[9,    26] loss: 7.546 running loss:  792.165\n",
      "[9,    36] loss: 3.982 running loss:  1112.531\n",
      "[9,    46] loss: 6.143 running loss:  1409.140\n",
      "[9,    56] loss: 6.870 running loss:  1764.735\n",
      "[9,    66] loss: 7.128 running loss:  2068.223\n",
      "[9,    76] loss: 6.931 running loss:  2397.238\n",
      "[9,    86] loss: 6.657 running loss:  2713.853\n",
      "[9,    96] loss: 5.737 running loss:  3015.332\n",
      "[9,   106] loss: 7.468 running loss:  3323.797\n",
      "[9,   116] loss: 3.932 running loss:  3627.451\n",
      "[9,   126] loss: 6.555 running loss:  3893.639\n",
      "[9,   136] loss: 5.243 running loss:  4204.434\n",
      "[9,   146] loss: 5.998 running loss:  4542.785\n",
      "[9,   156] loss: 5.048 running loss:  4839.091\n",
      "[9,   166] loss: 7.889 running loss:  5152.232\n",
      "[9,   176] loss: 4.376 running loss:  5424.054\n",
      "[9,   186] loss: 4.676 running loss:  5704.869\n",
      "[9,   196] loss: 5.303 running loss:  6052.311\n",
      "[9,   206] loss: 5.846 running loss:  6363.423\n",
      "[9,   216] loss: 5.623 running loss:  6661.449\n",
      "[9,   226] loss: 6.215 running loss:  6950.136\n",
      "[9,   236] loss: 7.091 running loss:  7248.590\n",
      "[9,   246] loss: 4.717 running loss:  7597.058\n",
      "[9,   256] loss: 7.231 running loss:  7886.961\n",
      "[9,   266] loss: 6.083 running loss:  8206.243\n",
      "[9,   276] loss: 8.970 running loss:  8573.644\n",
      "[9,   286] loss: 4.437 running loss:  8868.835\n",
      "[9,   296] loss: 4.713 running loss:  9147.565\n",
      "[9,   306] loss: 5.641 running loss:  9446.229\n",
      "[9,   316] loss: 5.267 running loss:  9732.417\n",
      "[9,   326] loss: 5.560 running loss:  10073.915\n",
      "[9,   336] loss: 4.181 running loss:  10411.458\n",
      "[9,   346] loss: 6.833 running loss:  10686.075\n",
      "[9,   356] loss: 5.636 running loss:  11030.813\n",
      "[9,   366] loss: 6.779 running loss:  11367.418\n",
      "[9,   376] loss: 9.807 running loss:  11693.979\n",
      "[9,   386] loss: 6.152 running loss:  12040.000\n",
      "[9,   396] loss: 5.009 running loss:  12315.801\n",
      "[9,   406] loss: 5.501 running loss:  12598.403\n",
      "[9,   416] loss: 6.583 running loss:  12848.458\n",
      "[9,   426] loss: 4.309 running loss:  13133.884\n",
      "[9,   436] loss: 7.028 running loss:  13441.705\n",
      "[9,   446] loss: 7.260 running loss:  13762.926\n",
      "[9,   456] loss: 4.555 running loss:  14009.049\n",
      "[9,   466] loss: 10.923 running loss:  14343.984\n",
      "[9,   476] loss: 6.252 running loss:  14674.544\n",
      "[9,   486] loss: 6.563 running loss:  14977.561\n",
      "[9,   496] loss: 5.264 running loss:  15257.135\n",
      "[9,   506] loss: 4.363 running loss:  15571.869\n",
      "[9,   516] loss: 2.543 running loss:  15858.290\n",
      "[9,   526] loss: 5.555 running loss:  16154.424\n",
      "[9,   536] loss: 4.194 running loss:  16418.358\n",
      "[9,   546] loss: 7.622 running loss:  16716.856\n",
      "[9,   556] loss: 9.447 running loss:  17018.657\n",
      "[9,   566] loss: 4.129 running loss:  17288.863\n",
      "[9,   576] loss: 8.522 running loss:  17651.863\n",
      "[9,   586] loss: 4.831 running loss:  17935.851\n",
      "[9,   596] loss: 8.326 running loss:  18256.471\n",
      "[9,   606] loss: 6.955 running loss:  18569.362\n",
      "[9,   616] loss: 6.713 running loss:  18869.541\n",
      "[9,   626] loss: 4.726 running loss:  19153.561\n",
      "[9,   636] loss: 4.533 running loss:  19451.718\n",
      "[9,   646] loss: 3.531 running loss:  19759.418\n",
      "[9,   656] loss: 2.235 running loss:  20060.021\n",
      "[9,   666] loss: 7.247 running loss:  20353.494\n",
      "[9,   676] loss: 6.362 running loss:  20675.568\n",
      "[9,   686] loss: 3.715 running loss:  21000.959\n",
      "[9,   696] loss: 5.479 running loss:  21293.817\n",
      "[9,   706] loss: 6.693 running loss:  21640.069\n",
      "[9,   716] loss: 8.579 running loss:  21919.580\n",
      "[9,   726] loss: 5.902 running loss:  22190.213\n",
      "[9,   736] loss: 5.726 running loss:  22520.914\n",
      "[9,   746] loss: 4.869 running loss:  22836.634\n",
      "[9,   756] loss: 4.144 running loss:  23167.063\n",
      "[9,   766] loss: 6.155 running loss:  23468.768\n",
      "[9,   776] loss: 4.203 running loss:  23778.633\n",
      "[9,   786] loss: 7.455 running loss:  24066.406\n",
      "[9,   796] loss: 6.335 running loss:  24363.502\n",
      "[9,   806] loss: 5.596 running loss:  24657.602\n",
      "[9,   816] loss: 4.313 running loss:  24921.954\n",
      "[9,   826] loss: 5.702 running loss:  25188.101\n",
      "Epoch 9/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3148ecb55e48eeb665aee1f7f1c448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     6] loss: 4.265 running loss:  152.750\n",
      "[10,    16] loss: 6.248 running loss:  446.689\n",
      "[10,    26] loss: 6.596 running loss:  822.397\n",
      "[10,    36] loss: 9.766 running loss:  1202.966\n",
      "[10,    46] loss: 5.007 running loss:  1528.155\n",
      "[10,    56] loss: 7.874 running loss:  1877.462\n",
      "[10,    66] loss: 3.381 running loss:  2195.336\n",
      "[10,    76] loss: 6.582 running loss:  2481.590\n",
      "[10,    86] loss: 4.796 running loss:  2821.472\n",
      "[10,    96] loss: 8.887 running loss:  3115.813\n",
      "[10,   106] loss: 5.707 running loss:  3418.342\n",
      "[10,   116] loss: 6.202 running loss:  3705.815\n",
      "[10,   126] loss: 7.300 running loss:  3993.029\n",
      "[10,   136] loss: 7.514 running loss:  4303.848\n",
      "[10,   146] loss: 6.803 running loss:  4612.874\n",
      "[10,   156] loss: 4.923 running loss:  4879.269\n",
      "[10,   166] loss: 4.598 running loss:  5190.092\n",
      "[10,   176] loss: 5.945 running loss:  5504.203\n",
      "[10,   186] loss: 7.570 running loss:  5824.394\n",
      "[10,   196] loss: 4.692 running loss:  6147.098\n",
      "[10,   206] loss: 4.749 running loss:  6435.794\n",
      "[10,   216] loss: 5.550 running loss:  6745.207\n",
      "[10,   226] loss: 4.022 running loss:  7069.027\n",
      "[10,   236] loss: 8.396 running loss:  7329.781\n",
      "[10,   246] loss: 5.849 running loss:  7627.248\n",
      "[10,   256] loss: 5.422 running loss:  7917.648\n",
      "[10,   266] loss: 3.471 running loss:  8228.809\n",
      "[10,   276] loss: 3.796 running loss:  8524.315\n",
      "[10,   286] loss: 4.119 running loss:  8820.594\n",
      "[10,   296] loss: 4.299 running loss:  9137.897\n",
      "[10,   306] loss: 3.698 running loss:  9448.023\n",
      "[10,   316] loss: 10.862 running loss:  9747.840\n",
      "[10,   326] loss: 5.505 running loss:  10061.611\n",
      "[10,   336] loss: 5.821 running loss:  10365.752\n",
      "[10,   346] loss: 6.565 running loss:  10697.337\n",
      "[10,   356] loss: 5.589 running loss:  11026.010\n",
      "[10,   366] loss: 5.688 running loss:  11307.843\n",
      "[10,   376] loss: 6.383 running loss:  11569.880\n",
      "[10,   386] loss: 4.499 running loss:  11881.184\n",
      "[10,   396] loss: 4.704 running loss:  12159.896\n",
      "[10,   406] loss: 6.430 running loss:  12523.974\n",
      "[10,   416] loss: 6.115 running loss:  12819.638\n",
      "[10,   426] loss: 6.540 running loss:  13132.280\n",
      "[10,   436] loss: 7.053 running loss:  13457.030\n",
      "[10,   446] loss: 5.385 running loss:  13746.827\n",
      "[10,   456] loss: 4.114 running loss:  14057.334\n",
      "[10,   466] loss: 4.922 running loss:  14313.568\n",
      "[10,   476] loss: 4.184 running loss:  14604.126\n",
      "[10,   486] loss: 3.230 running loss:  14864.419\n",
      "[10,   496] loss: 4.513 running loss:  15170.180\n",
      "[10,   506] loss: 3.832 running loss:  15430.689\n",
      "[10,   516] loss: 6.152 running loss:  15777.377\n",
      "[10,   526] loss: 6.945 running loss:  16074.394\n",
      "[10,   536] loss: 6.559 running loss:  16384.686\n",
      "[10,   546] loss: 5.447 running loss:  16667.337\n",
      "[10,   556] loss: 3.758 running loss:  16975.117\n",
      "[10,   566] loss: 6.670 running loss:  17247.066\n",
      "[10,   576] loss: 4.861 running loss:  17585.463\n",
      "[10,   586] loss: 6.920 running loss:  17882.834\n",
      "[10,   596] loss: 6.603 running loss:  18230.351\n",
      "[10,   606] loss: 6.206 running loss:  18549.226\n",
      "[10,   616] loss: 8.358 running loss:  18850.102\n",
      "[10,   626] loss: 6.095 running loss:  19161.853\n",
      "[10,   636] loss: 3.045 running loss:  19421.185\n",
      "[10,   646] loss: 7.998 running loss:  19717.541\n",
      "[10,   656] loss: 6.374 running loss:  20004.566\n",
      "[10,   666] loss: 5.510 running loss:  20290.555\n",
      "[10,   676] loss: 8.085 running loss:  20595.352\n",
      "[10,   686] loss: 5.364 running loss:  20903.285\n",
      "[10,   696] loss: 6.831 running loss:  21190.463\n",
      "[10,   706] loss: 5.159 running loss:  21490.066\n",
      "[10,   716] loss: 7.548 running loss:  21803.483\n",
      "[10,   726] loss: 4.221 running loss:  22102.278\n",
      "[10,   736] loss: 5.278 running loss:  22382.434\n",
      "[10,   746] loss: 5.497 running loss:  22703.281\n",
      "[10,   756] loss: 6.544 running loss:  23016.648\n",
      "[10,   766] loss: 5.625 running loss:  23345.045\n",
      "[10,   776] loss: 5.131 running loss:  23645.639\n",
      "[10,   786] loss: 3.961 running loss:  23942.701\n",
      "[10,   796] loss: 8.426 running loss:  24250.635\n",
      "[10,   806] loss: 5.082 running loss:  24536.228\n",
      "[10,   816] loss: 6.175 running loss:  24890.358\n",
      "[10,   826] loss: 8.376 running loss:  25256.239\n",
      "Epoch 10/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7793f1151c054b80b21f3b2c6d60ac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11,     6] loss: 7.447 running loss:  169.730\n",
      "[11,    16] loss: 2.552 running loss:  491.256\n",
      "[11,    26] loss: 2.826 running loss:  820.505\n",
      "[11,    36] loss: 5.238 running loss:  1146.750\n",
      "[11,    46] loss: 9.033 running loss:  1450.562\n",
      "[11,    56] loss: 6.895 running loss:  1723.797\n",
      "[11,    66] loss: 7.314 running loss:  2050.101\n",
      "[11,    76] loss: 5.038 running loss:  2371.431\n",
      "[11,    86] loss: 6.399 running loss:  2663.571\n",
      "[11,    96] loss: 3.599 running loss:  2958.830\n",
      "[11,   106] loss: 5.401 running loss:  3244.356\n",
      "[11,   116] loss: 9.865 running loss:  3557.510\n",
      "[11,   126] loss: 5.246 running loss:  3853.590\n",
      "[11,   136] loss: 7.698 running loss:  4163.688\n",
      "[11,   146] loss: 4.882 running loss:  4494.394\n",
      "[11,   156] loss: 9.289 running loss:  4827.827\n",
      "[11,   166] loss: 6.262 running loss:  5142.771\n",
      "[11,   176] loss: 3.973 running loss:  5419.963\n",
      "[11,   186] loss: 6.683 running loss:  5723.303\n",
      "[11,   196] loss: 4.358 running loss:  6041.565\n",
      "[11,   206] loss: 6.033 running loss:  6376.451\n",
      "[11,   216] loss: 5.989 running loss:  6687.032\n",
      "[11,   226] loss: 5.684 running loss:  6995.097\n",
      "[11,   236] loss: 7.307 running loss:  7300.855\n",
      "[11,   246] loss: 5.959 running loss:  7603.425\n",
      "[11,   256] loss: 6.136 running loss:  7903.808\n",
      "[11,   266] loss: 4.696 running loss:  8233.754\n",
      "[11,   276] loss: 4.317 running loss:  8518.634\n",
      "[11,   286] loss: 7.556 running loss:  8803.552\n",
      "[11,   296] loss: 4.410 running loss:  9128.382\n",
      "[11,   306] loss: 4.386 running loss:  9430.286\n",
      "[11,   316] loss: 5.323 running loss:  9743.524\n",
      "[11,   326] loss: 8.951 running loss:  10055.843\n",
      "[11,   336] loss: 5.214 running loss:  10364.692\n",
      "[11,   346] loss: 6.836 running loss:  10663.167\n",
      "[11,   356] loss: 5.889 running loss:  10998.713\n",
      "[11,   366] loss: 4.560 running loss:  11296.581\n",
      "[11,   376] loss: 4.946 running loss:  11603.128\n",
      "[11,   386] loss: 6.284 running loss:  11942.303\n",
      "[11,   396] loss: 7.088 running loss:  12273.674\n",
      "[11,   406] loss: 5.828 running loss:  12627.322\n",
      "[11,   416] loss: 5.386 running loss:  12921.387\n",
      "[11,   426] loss: 5.564 running loss:  13229.870\n",
      "[11,   436] loss: 5.891 running loss:  13525.008\n",
      "[11,   446] loss: 5.018 running loss:  13844.684\n",
      "[11,   456] loss: 4.438 running loss:  14156.878\n",
      "[11,   466] loss: 6.805 running loss:  14450.946\n",
      "[11,   476] loss: 4.317 running loss:  14720.600\n",
      "[11,   486] loss: 4.466 running loss:  15036.537\n",
      "[11,   496] loss: 4.587 running loss:  15325.395\n",
      "[11,   506] loss: 7.566 running loss:  15628.101\n",
      "[11,   516] loss: 6.363 running loss:  15947.520\n",
      "[11,   526] loss: 6.074 running loss:  16289.566\n",
      "[11,   536] loss: 6.973 running loss:  16587.717\n",
      "[11,   546] loss: 3.673 running loss:  16881.450\n",
      "[11,   556] loss: 4.641 running loss:  17202.013\n",
      "[11,   566] loss: 4.594 running loss:  17483.821\n",
      "[11,   576] loss: 6.745 running loss:  17752.393\n",
      "[11,   586] loss: 6.445 running loss:  18093.418\n",
      "[11,   596] loss: 6.138 running loss:  18407.459\n",
      "[11,   606] loss: 7.383 running loss:  18684.248\n",
      "[11,   616] loss: 4.681 running loss:  18939.792\n",
      "[11,   626] loss: 6.546 running loss:  19266.876\n",
      "[11,   636] loss: 7.626 running loss:  19550.755\n",
      "[11,   646] loss: 7.437 running loss:  19912.108\n",
      "[11,   656] loss: 5.519 running loss:  20205.859\n",
      "[11,   666] loss: 5.254 running loss:  20502.353\n",
      "[11,   676] loss: 2.799 running loss:  20757.644\n",
      "[11,   686] loss: 7.139 running loss:  21081.841\n",
      "[11,   696] loss: 7.950 running loss:  21357.253\n",
      "[11,   706] loss: 3.802 running loss:  21633.807\n",
      "[11,   716] loss: 5.166 running loss:  21966.267\n",
      "[11,   726] loss: 8.198 running loss:  22247.525\n",
      "[11,   736] loss: 4.859 running loss:  22537.615\n",
      "[11,   746] loss: 6.614 running loss:  22851.024\n",
      "[11,   756] loss: 3.797 running loss:  23138.337\n",
      "[11,   766] loss: 5.617 running loss:  23449.989\n",
      "[11,   776] loss: 8.982 running loss:  23733.316\n",
      "[11,   786] loss: 4.039 running loss:  23994.381\n",
      "[11,   796] loss: 4.838 running loss:  24303.375\n",
      "[11,   806] loss: 3.866 running loss:  24608.786\n",
      "[11,   816] loss: 7.046 running loss:  24898.628\n",
      "[11,   826] loss: 2.432 running loss:  25199.247\n",
      "Epoch 11/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706cd53240964a3e908e9e82b769fd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,     6] loss: 4.893 running loss:  174.990\n",
      "[12,    16] loss: 9.126 running loss:  532.187\n",
      "[12,    26] loss: 8.742 running loss:  842.392\n",
      "[12,    36] loss: 6.937 running loss:  1168.365\n",
      "[12,    46] loss: 6.526 running loss:  1456.762\n",
      "[12,    56] loss: 5.634 running loss:  1724.266\n",
      "[12,    66] loss: 4.910 running loss:  2030.647\n",
      "[12,    76] loss: 8.522 running loss:  2401.552\n",
      "[12,    86] loss: 5.267 running loss:  2691.308\n",
      "[12,    96] loss: 4.494 running loss:  2989.930\n",
      "[12,   106] loss: 7.936 running loss:  3315.182\n",
      "[12,   116] loss: 5.380 running loss:  3645.215\n",
      "[12,   126] loss: 7.832 running loss:  3947.107\n",
      "[12,   136] loss: 7.448 running loss:  4263.274\n",
      "[12,   146] loss: 4.630 running loss:  4518.632\n",
      "[12,   156] loss: 8.380 running loss:  4855.610\n",
      "[12,   166] loss: 6.432 running loss:  5181.111\n",
      "[12,   176] loss: 5.556 running loss:  5470.387\n",
      "[12,   186] loss: 5.288 running loss:  5780.894\n",
      "[12,   196] loss: 5.407 running loss:  6086.610\n",
      "[12,   206] loss: 7.996 running loss:  6399.737\n",
      "[12,   216] loss: 9.406 running loss:  6713.401\n",
      "[12,   226] loss: 5.284 running loss:  7029.056\n",
      "[12,   236] loss: 3.505 running loss:  7316.060\n",
      "[12,   246] loss: 3.458 running loss:  7577.580\n",
      "[12,   256] loss: 7.106 running loss:  7870.263\n",
      "[12,   266] loss: 6.327 running loss:  8136.516\n",
      "[12,   276] loss: 5.411 running loss:  8426.378\n",
      "[12,   286] loss: 8.631 running loss:  8763.500\n",
      "[12,   296] loss: 5.680 running loss:  9060.546\n",
      "[12,   306] loss: 6.498 running loss:  9406.969\n",
      "[12,   316] loss: 6.804 running loss:  9714.057\n",
      "[12,   326] loss: 7.624 running loss:  10019.343\n",
      "[12,   336] loss: 8.116 running loss:  10296.569\n",
      "[12,   346] loss: 8.846 running loss:  10610.643\n",
      "[12,   356] loss: 5.855 running loss:  10929.194\n",
      "[12,   366] loss: 6.178 running loss:  11235.527\n",
      "[12,   376] loss: 5.522 running loss:  11523.489\n",
      "[12,   386] loss: 5.276 running loss:  11789.236\n",
      "[12,   396] loss: 3.966 running loss:  12088.083\n",
      "[12,   406] loss: 3.796 running loss:  12364.551\n",
      "[12,   416] loss: 5.790 running loss:  12714.856\n",
      "[12,   426] loss: 2.849 running loss:  13017.520\n",
      "[12,   436] loss: 3.618 running loss:  13330.638\n",
      "[12,   446] loss: 7.804 running loss:  13608.542\n",
      "[12,   456] loss: 5.393 running loss:  13933.196\n",
      "[12,   466] loss: 9.721 running loss:  14259.347\n",
      "[12,   476] loss: 7.286 running loss:  14522.430\n",
      "[12,   486] loss: 5.037 running loss:  14827.988\n",
      "[12,   496] loss: 6.111 running loss:  15212.432\n",
      "[12,   506] loss: 5.879 running loss:  15531.175\n",
      "[12,   516] loss: 4.348 running loss:  15818.907\n",
      "[12,   526] loss: 4.211 running loss:  16092.088\n",
      "[12,   536] loss: 6.633 running loss:  16423.406\n",
      "[12,   546] loss: 5.398 running loss:  16718.037\n",
      "[12,   556] loss: 3.829 running loss:  17017.607\n",
      "[12,   566] loss: 5.839 running loss:  17334.148\n",
      "[12,   576] loss: 3.508 running loss:  17627.713\n",
      "[12,   586] loss: 4.998 running loss:  17939.842\n",
      "[12,   596] loss: 7.499 running loss:  18238.922\n",
      "[12,   606] loss: 4.863 running loss:  18531.951\n",
      "[12,   616] loss: 6.026 running loss:  18822.941\n",
      "[12,   626] loss: 5.734 running loss:  19102.251\n",
      "[12,   636] loss: 3.989 running loss:  19399.191\n",
      "[12,   646] loss: 5.403 running loss:  19710.848\n",
      "[12,   656] loss: 5.384 running loss:  20034.795\n",
      "[12,   666] loss: 7.002 running loss:  20338.906\n",
      "[12,   676] loss: 5.829 running loss:  20632.781\n",
      "[12,   686] loss: 5.760 running loss:  20974.832\n",
      "[12,   696] loss: 3.975 running loss:  21277.650\n",
      "[12,   706] loss: 6.475 running loss:  21577.877\n",
      "[12,   716] loss: 6.410 running loss:  21861.720\n",
      "[12,   726] loss: 5.469 running loss:  22170.832\n",
      "[12,   736] loss: 5.001 running loss:  22488.517\n",
      "[12,   746] loss: 7.720 running loss:  22825.004\n",
      "[12,   756] loss: 8.545 running loss:  23163.487\n",
      "[12,   766] loss: 5.049 running loss:  23492.360\n",
      "[12,   776] loss: 5.671 running loss:  23769.813\n",
      "[12,   786] loss: 7.838 running loss:  24040.027\n",
      "[12,   796] loss: 5.617 running loss:  24376.202\n",
      "[12,   806] loss: 4.573 running loss:  24638.346\n",
      "[12,   816] loss: 5.377 running loss:  24952.260\n",
      "[12,   826] loss: 4.281 running loss:  25223.010\n",
      "Epoch 12/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d091c181b745709a0c95f37098ac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13,     6] loss: 4.432 running loss:  135.491\n",
      "[13,    16] loss: 7.735 running loss:  422.964\n",
      "[13,    26] loss: 3.452 running loss:  714.551\n",
      "[13,    36] loss: 6.967 running loss:  1029.298\n",
      "[13,    46] loss: 4.231 running loss:  1308.647\n",
      "[13,    56] loss: 7.105 running loss:  1596.585\n",
      "[13,    66] loss: 6.979 running loss:  1915.194\n",
      "[13,    76] loss: 7.012 running loss:  2252.569\n",
      "[13,    86] loss: 6.948 running loss:  2573.665\n",
      "[13,    96] loss: 7.347 running loss:  2901.935\n",
      "[13,   106] loss: 5.970 running loss:  3237.193\n",
      "[13,   116] loss: 5.705 running loss:  3540.041\n",
      "[13,   126] loss: 5.149 running loss:  3841.284\n",
      "[13,   136] loss: 7.384 running loss:  4159.916\n",
      "[13,   146] loss: 4.208 running loss:  4465.292\n",
      "[13,   156] loss: 4.493 running loss:  4781.366\n",
      "[13,   166] loss: 6.011 running loss:  5094.479\n",
      "[13,   176] loss: 3.651 running loss:  5389.128\n",
      "[13,   186] loss: 7.708 running loss:  5716.438\n",
      "[13,   196] loss: 5.926 running loss:  6000.462\n",
      "[13,   206] loss: 4.017 running loss:  6262.056\n",
      "[13,   216] loss: 6.937 running loss:  6591.605\n",
      "[13,   226] loss: 8.059 running loss:  6883.501\n",
      "[13,   236] loss: 3.086 running loss:  7164.493\n",
      "[13,   246] loss: 6.428 running loss:  7453.208\n",
      "[13,   256] loss: 5.294 running loss:  7742.606\n",
      "[13,   266] loss: 5.261 running loss:  8022.513\n",
      "[13,   276] loss: 7.504 running loss:  8335.583\n",
      "[13,   286] loss: 5.669 running loss:  8611.443\n",
      "[13,   296] loss: 3.170 running loss:  8905.065\n",
      "[13,   306] loss: 5.266 running loss:  9230.532\n",
      "[13,   316] loss: 4.203 running loss:  9516.057\n",
      "[13,   326] loss: 7.472 running loss:  9782.345\n",
      "[13,   336] loss: 5.631 running loss:  10067.665\n",
      "[13,   346] loss: 7.238 running loss:  10379.944\n",
      "[13,   356] loss: 8.252 running loss:  10712.788\n",
      "[13,   366] loss: 4.797 running loss:  11001.818\n",
      "[13,   376] loss: 4.262 running loss:  11299.337\n",
      "[13,   386] loss: 6.718 running loss:  11617.155\n",
      "[13,   396] loss: 7.960 running loss:  11927.680\n",
      "[13,   406] loss: 5.444 running loss:  12270.724\n",
      "[13,   416] loss: 4.687 running loss:  12575.943\n",
      "[13,   426] loss: 6.998 running loss:  12902.819\n",
      "[13,   436] loss: 7.013 running loss:  13258.945\n",
      "[13,   446] loss: 8.712 running loss:  13568.713\n",
      "[13,   456] loss: 5.917 running loss:  13848.386\n",
      "[13,   466] loss: 5.839 running loss:  14154.334\n",
      "[13,   476] loss: 5.850 running loss:  14475.205\n",
      "[13,   486] loss: 7.014 running loss:  14796.081\n",
      "[13,   496] loss: 6.066 running loss:  15120.155\n",
      "[13,   506] loss: 5.234 running loss:  15408.322\n",
      "[13,   516] loss: 8.813 running loss:  15734.840\n",
      "[13,   526] loss: 8.280 running loss:  16013.861\n",
      "[13,   536] loss: 7.777 running loss:  16306.008\n",
      "[13,   546] loss: 7.368 running loss:  16619.371\n",
      "[13,   556] loss: 7.252 running loss:  16962.830\n",
      "[13,   566] loss: 7.419 running loss:  17302.387\n",
      "[13,   576] loss: 9.769 running loss:  17624.337\n",
      "[13,   586] loss: 7.305 running loss:  17982.421\n",
      "[13,   596] loss: 5.642 running loss:  18291.553\n",
      "[13,   606] loss: 6.328 running loss:  18597.797\n",
      "[13,   616] loss: 10.755 running loss:  18936.228\n",
      "[13,   626] loss: 5.313 running loss:  19207.524\n",
      "[13,   636] loss: 2.762 running loss:  19489.789\n",
      "[13,   646] loss: 6.802 running loss:  19789.519\n",
      "[13,   656] loss: 5.790 running loss:  20083.520\n",
      "[13,   666] loss: 4.331 running loss:  20388.934\n",
      "[13,   676] loss: 7.959 running loss:  20744.563\n",
      "[13,   686] loss: 5.857 running loss:  21019.345\n",
      "[13,   696] loss: 8.047 running loss:  21339.763\n",
      "[13,   706] loss: 4.724 running loss:  21673.043\n",
      "[13,   716] loss: 5.790 running loss:  21946.782\n",
      "[13,   726] loss: 7.146 running loss:  22262.018\n",
      "[13,   736] loss: 6.422 running loss:  22534.416\n",
      "[13,   746] loss: 5.950 running loss:  22820.226\n",
      "[13,   756] loss: 4.369 running loss:  23120.434\n",
      "[13,   766] loss: 3.669 running loss:  23396.226\n",
      "[13,   776] loss: 3.944 running loss:  23703.186\n",
      "[13,   786] loss: 6.524 running loss:  24008.865\n",
      "[13,   796] loss: 8.032 running loss:  24325.364\n",
      "[13,   806] loss: 3.841 running loss:  24603.007\n",
      "[13,   816] loss: 6.086 running loss:  24892.159\n",
      "[13,   826] loss: 10.198 running loss:  25205.700\n",
      "Epoch 13/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eadfb71e3494ef787765b97bf9b7530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14,     6] loss: 5.756 running loss:  144.850\n",
      "[14,    16] loss: 5.606 running loss:  442.738\n",
      "[14,    26] loss: 4.145 running loss:  722.807\n",
      "[14,    36] loss: 7.215 running loss:  1043.720\n",
      "[14,    46] loss: 5.384 running loss:  1362.165\n",
      "[14,    56] loss: 7.394 running loss:  1647.216\n",
      "[14,    66] loss: 7.800 running loss:  1953.772\n",
      "[14,    76] loss: 4.648 running loss:  2233.205\n",
      "[14,    86] loss: 5.642 running loss:  2539.585\n",
      "[14,    96] loss: 7.130 running loss:  2815.297\n",
      "[14,   106] loss: 9.031 running loss:  3186.307\n",
      "[14,   116] loss: 4.905 running loss:  3470.507\n",
      "[14,   126] loss: 6.406 running loss:  3761.555\n",
      "[14,   136] loss: 6.109 running loss:  4045.821\n",
      "[14,   146] loss: 7.779 running loss:  4350.044\n",
      "[14,   156] loss: 5.902 running loss:  4642.584\n",
      "[14,   166] loss: 4.554 running loss:  4948.205\n",
      "[14,   176] loss: 6.669 running loss:  5241.584\n",
      "[14,   186] loss: 7.213 running loss:  5551.716\n",
      "[14,   196] loss: 8.941 running loss:  5865.838\n",
      "[14,   206] loss: 6.955 running loss:  6197.109\n",
      "[14,   216] loss: 5.115 running loss:  6482.176\n",
      "[14,   226] loss: 8.984 running loss:  6824.013\n",
      "[14,   236] loss: 6.472 running loss:  7142.431\n",
      "[14,   246] loss: 6.979 running loss:  7468.006\n",
      "[14,   256] loss: 5.295 running loss:  7786.112\n",
      "[14,   266] loss: 7.563 running loss:  8096.400\n",
      "[14,   276] loss: 5.528 running loss:  8368.809\n",
      "[14,   286] loss: 6.258 running loss:  8665.448\n",
      "[14,   296] loss: 7.311 running loss:  8985.106\n",
      "[14,   306] loss: 5.233 running loss:  9301.181\n",
      "[14,   316] loss: 5.748 running loss:  9603.097\n",
      "[14,   326] loss: 5.131 running loss:  9888.361\n",
      "[14,   336] loss: 7.132 running loss:  10175.830\n",
      "[14,   346] loss: 5.274 running loss:  10514.932\n",
      "[14,   356] loss: 7.535 running loss:  10839.753\n",
      "[14,   366] loss: 3.159 running loss:  11127.116\n",
      "[14,   376] loss: 8.791 running loss:  11465.132\n",
      "[14,   386] loss: 5.879 running loss:  11738.267\n",
      "[14,   396] loss: 3.244 running loss:  12022.214\n",
      "[14,   406] loss: 8.324 running loss:  12317.538\n",
      "[14,   416] loss: 7.503 running loss:  12631.925\n",
      "[14,   426] loss: 6.670 running loss:  12922.727\n",
      "[14,   436] loss: 6.677 running loss:  13241.565\n",
      "[14,   446] loss: 8.201 running loss:  13552.984\n",
      "[14,   456] loss: 5.263 running loss:  13811.024\n",
      "[14,   466] loss: 5.179 running loss:  14075.117\n",
      "[14,   476] loss: 4.601 running loss:  14398.760\n",
      "[14,   486] loss: 7.338 running loss:  14719.049\n",
      "[14,   496] loss: 7.085 running loss:  15091.116\n",
      "[14,   506] loss: 6.821 running loss:  15410.732\n",
      "[14,   516] loss: 6.041 running loss:  15721.558\n",
      "[14,   526] loss: 5.028 running loss:  16075.342\n",
      "[14,   536] loss: 4.957 running loss:  16354.220\n",
      "[14,   546] loss: 5.014 running loss:  16654.861\n",
      "[14,   556] loss: 6.834 running loss:  16972.282\n",
      "[14,   566] loss: 8.628 running loss:  17286.517\n",
      "[14,   576] loss: 5.343 running loss:  17603.292\n",
      "[14,   586] loss: 8.415 running loss:  17855.373\n",
      "[14,   596] loss: 4.916 running loss:  18168.173\n",
      "[14,   606] loss: 4.828 running loss:  18464.405\n",
      "[14,   616] loss: 4.045 running loss:  18743.594\n",
      "[14,   626] loss: 6.496 running loss:  19067.135\n",
      "[14,   636] loss: 5.303 running loss:  19385.227\n",
      "[14,   646] loss: 8.113 running loss:  19723.201\n",
      "[14,   656] loss: 3.442 running loss:  19990.057\n",
      "[14,   666] loss: 5.872 running loss:  20292.470\n",
      "[14,   676] loss: 3.711 running loss:  20596.328\n",
      "[14,   686] loss: 7.460 running loss:  20872.653\n",
      "[14,   696] loss: 5.610 running loss:  21193.774\n",
      "[14,   706] loss: 7.486 running loss:  21501.423\n",
      "[14,   716] loss: 6.066 running loss:  21823.519\n",
      "[14,   726] loss: 7.622 running loss:  22115.240\n",
      "[14,   736] loss: 7.308 running loss:  22374.116\n",
      "[14,   746] loss: 4.993 running loss:  22655.165\n",
      "[14,   756] loss: 4.388 running loss:  23009.876\n",
      "[14,   766] loss: 4.110 running loss:  23320.738\n",
      "[14,   776] loss: 5.660 running loss:  23610.223\n",
      "[14,   786] loss: 8.887 running loss:  23970.182\n",
      "[14,   796] loss: 4.800 running loss:  24300.857\n",
      "[14,   806] loss: 7.098 running loss:  24574.302\n",
      "[14,   816] loss: 7.464 running loss:  24898.560\n",
      "[14,   826] loss: 8.462 running loss:  25200.923\n",
      "Epoch 14/14\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d1683ba74f4064a524ac058e96e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,     6] loss: 5.219 running loss:  160.222\n",
      "[15,    16] loss: 6.885 running loss:  467.124\n",
      "[15,    26] loss: 5.191 running loss:  801.582\n",
      "[15,    36] loss: 7.726 running loss:  1095.334\n",
      "[15,    46] loss: 8.748 running loss:  1439.602\n",
      "[15,    56] loss: 6.508 running loss:  1754.361\n",
      "[15,    66] loss: 8.149 running loss:  2064.331\n",
      "[15,    76] loss: 5.359 running loss:  2380.183\n",
      "[15,    86] loss: 5.035 running loss:  2706.898\n",
      "[15,    96] loss: 6.759 running loss:  3016.691\n",
      "[15,   106] loss: 7.633 running loss:  3349.763\n",
      "[15,   116] loss: 6.041 running loss:  3623.064\n",
      "[15,   126] loss: 5.537 running loss:  3918.159\n",
      "[15,   136] loss: 8.202 running loss:  4235.599\n",
      "[15,   146] loss: 5.383 running loss:  4553.838\n",
      "[15,   156] loss: 7.783 running loss:  4876.268\n",
      "[15,   166] loss: 4.507 running loss:  5185.325\n",
      "[15,   176] loss: 5.665 running loss:  5459.046\n",
      "[15,   186] loss: 7.574 running loss:  5721.419\n",
      "[15,   196] loss: 5.262 running loss:  6040.242\n",
      "[15,   206] loss: 7.722 running loss:  6359.339\n",
      "[15,   216] loss: 5.294 running loss:  6646.123\n",
      "[15,   226] loss: 6.061 running loss:  6970.258\n",
      "[15,   236] loss: 7.869 running loss:  7289.003\n",
      "[15,   246] loss: 8.361 running loss:  7610.642\n",
      "[15,   256] loss: 6.314 running loss:  7913.120\n",
      "[15,   266] loss: 5.891 running loss:  8238.236\n",
      "[15,   276] loss: 5.685 running loss:  8546.005\n",
      "[15,   286] loss: 4.461 running loss:  8837.589\n",
      "[15,   296] loss: 6.287 running loss:  9119.732\n",
      "[15,   306] loss: 4.766 running loss:  9395.067\n",
      "[15,   316] loss: 6.620 running loss:  9666.442\n",
      "[15,   326] loss: 6.216 running loss:  9981.741\n",
      "[15,   336] loss: 5.897 running loss:  10271.095\n",
      "[15,   346] loss: 3.941 running loss:  10583.309\n",
      "[15,   356] loss: 5.058 running loss:  10843.338\n",
      "[15,   366] loss: 6.700 running loss:  11094.495\n",
      "[15,   376] loss: 6.504 running loss:  11390.658\n",
      "[15,   386] loss: 6.882 running loss:  11672.229\n",
      "[15,   396] loss: 6.842 running loss:  11990.145\n",
      "[15,   406] loss: 7.140 running loss:  12350.429\n",
      "[15,   416] loss: 8.179 running loss:  12678.232\n",
      "[15,   426] loss: 4.217 running loss:  12974.846\n",
      "[15,   436] loss: 6.865 running loss:  13301.288\n",
      "[15,   446] loss: 6.008 running loss:  13585.308\n",
      "[15,   456] loss: 9.976 running loss:  13908.095\n",
      "[15,   466] loss: 5.023 running loss:  14220.245\n",
      "[15,   476] loss: 5.504 running loss:  14534.920\n",
      "[15,   486] loss: 6.548 running loss:  14844.281\n",
      "[15,   496] loss: 4.642 running loss:  15180.544\n",
      "[15,   506] loss: 5.836 running loss:  15486.769\n",
      "[15,   516] loss: 3.974 running loss:  15768.342\n",
      "[15,   526] loss: 7.089 running loss:  16046.284\n",
      "[15,   536] loss: 6.515 running loss:  16328.514\n",
      "[15,   546] loss: 6.592 running loss:  16650.329\n",
      "[15,   556] loss: 5.168 running loss:  16928.848\n",
      "[15,   566] loss: 6.444 running loss:  17289.461\n",
      "[15,   576] loss: 4.337 running loss:  17574.883\n",
      "[15,   586] loss: 5.546 running loss:  17916.133\n",
      "[15,   596] loss: 5.116 running loss:  18214.383\n",
      "[15,   606] loss: 3.808 running loss:  18523.026\n",
      "[15,   616] loss: 5.327 running loss:  18825.612\n",
      "[15,   626] loss: 3.945 running loss:  19103.312\n",
      "[15,   636] loss: 5.212 running loss:  19388.412\n",
      "[15,   646] loss: 5.886 running loss:  19682.530\n",
      "[15,   656] loss: 5.161 running loss:  19956.502\n",
      "[15,   666] loss: 5.975 running loss:  20251.378\n",
      "[15,   676] loss: 6.201 running loss:  20529.669\n",
      "[15,   686] loss: 5.008 running loss:  20817.841\n",
      "[15,   696] loss: 7.134 running loss:  21186.861\n",
      "[15,   706] loss: 5.760 running loss:  21492.536\n",
      "[15,   716] loss: 5.628 running loss:  21781.800\n",
      "[15,   726] loss: 5.506 running loss:  22078.017\n",
      "[15,   736] loss: 8.821 running loss:  22398.240\n",
      "[15,   746] loss: 7.864 running loss:  22744.606\n",
      "[15,   756] loss: 4.752 running loss:  23051.272\n",
      "[15,   766] loss: 4.813 running loss:  23352.510\n",
      "[15,   776] loss: 7.833 running loss:  23726.884\n",
      "[15,   786] loss: 5.227 running loss:  24003.587\n",
      "[15,   796] loss: 6.404 running loss:  24324.817\n",
      "[15,   806] loss: 6.747 running loss:  24639.169\n",
      "[15,   816] loss: 5.031 running loss:  24927.643\n",
      "[15,   826] loss: 6.700 running loss:  25217.213\n"
     ]
    }
   ],
   "source": [
    "#### #File reading conf\n",
    "a = []\n",
    "idx=0\n",
    "iters=0\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(parser.epochs):\n",
    "    x=0\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    print('Epoch {}/{}'.format(epoch, parser.epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    \n",
    "    dataloader = utils.get_data_with_labels(512, 512,0.9, boxImagesPath,parser.batch_size,drop_last=True)\n",
    "\n",
    "\n",
    "    for data in tqdm(dataloader):\n",
    "        \n",
    "        inputs, classes, names, classes_types = data\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        #Loading data\n",
    "        a = []\n",
    "        idx=0\n",
    "        \n",
    "        \"\"\"lookup for data corresponding to every image in training batch\"\"\"\n",
    "        for name in names:\n",
    "            series=name.split('_')[-1].split('.')[0]\n",
    "            batch=name.split('_')[4]\n",
    "            for name in glob.glob(DataPath+batch+'\\\\files\\\\'+'/'+parser.metricType+'*'+series+'.csv'): \n",
    "                \n",
    "                #loading the absorption data\n",
    "                train = pd.read_csv(name)\n",
    "                values=np.array(train.values.T)\n",
    "                a.append(values[1])\n",
    "                \n",
    "                \n",
    "        a=np.array(a)     \n",
    "        \n",
    "        conditioningArray=torch.FloatTensor(set_conditioning(target, path, categories))\n",
    "\n",
    "        if conditioningArray.shape[1]==parser.condition_len:\n",
    "            \n",
    "            outmap_min, _ = torch.min(conditioningArray, dim=1, keepdim=True)\n",
    "            outmap_max, _ = torch.max(conditioningArray, dim=1, keepdim=True)\n",
    "            conditioningTensor = (conditioningArray - outmap_min) / (outmap_max - outmap_min)\n",
    "\n",
    "            y_predicted=fwd_test(input_=inputs, conditioning=conditioningTensor, b_size=inputs.shape[0])\n",
    "            y_predicted=torch.nn.functional.normalize(y_predicted, p=2.0, dim = 1)\n",
    "\n",
    "            y_truth = torch.tensor(a)\n",
    "\n",
    "            #error\n",
    "\n",
    "            errD_real = criterion(y_predicted.float(), y_truth.float())\n",
    "            errD_real.backward()\n",
    "            loss=errD_real.item()\n",
    "            opt.step()\n",
    "            scale = torch.tensor([10.0])\n",
    "\n",
    "            running_loss +=loss*inputs.size(0)\n",
    "\n",
    "\n",
    "            x += 1\n",
    "            i = i+1\n",
    "\n",
    "\n",
    "            if i % 10 == 5:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss / 10:.3f} running loss:  {running_loss / 10:.3f}')\n",
    "\n",
    "            iters += 1\n",
    "        else:\n",
    "        \n",
    "            break\n",
    "    \n",
    "    loss_values.append(running_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363ae30c-4682-4178-b292-fe93a49dcfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = './trainedModelTM_abs_Multitarget.pth'\n",
    "torch.save(fwd_test.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecde8d83-4f47-4090-a986-f0b29d6401d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values)\n",
    "np.savetxt('loss_ABS_TM.out', loss_values, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae19ef-a951-43cd-b3bc-101875142fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c03be-cdb9-4550-ab06-47e62d770cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
